#+STARTUP: showall
* question/answer:
- 因为中断的处理函数在内核态运行，又因为中断可以嵌套的，如果一连续有行验中断产生，那么会不
  会把内核栈给爆了？应该不会,因为产生某个中断之后系统会禁止相应的中断.
- 内核线程用谁的栈呢？应该是当前进程的,再次回到用户态的时候线程已经执行完了,所以内核栈不用
  因为再次回到用户态的时候把内核栈保存下来.但像ksoftirqd()这样的内核线程会调用schedule(),在
  内核空间用schdule()会切换进程,那原来的硬件上下文保存在那里了呢?什么时候会再用呢?
- 进程0最后执行cpu_idle()
- linux2.6有exit_group()和_exit()系统调用来终止一个用户程序。前者用do_group_exit()实现，C库
  的exit()调用，后者用do_exit()实现，如C库的pthread_exit()调用。
- TGID，PGID，SID这几个hash表的主要目的是让你容易通过一个领头进程找到它的线程组中的其它线程，
  进程组中的其它进程，对话中的其它进程。
- 一个中断号一个irq_desc_t结构体。
- irq_desc_t里的depth表示这个中断被禁止了多少次，但是Linux经常是禁止所有中断，为了效率不能
  在这种情况一个一个中断来禁止。
- arm的用arch/arm/kernel/irq.c的，里面有disable_irq()之类的，i386用kernel/irq/manage.c也有
  disable_irq()之类的，但是disable_irq()的实现是一样的。request_irq()也有类似的情况.
- 中断丢失是发生要多处理器中的。CPU A在应答一个中断线前被CPU B禁止了相同的中断，但是CPU A在
  应答之后执行do_irq()，但在do_irq()里会检查IRQ_DISABLE,所以do_irq()不会往下执行了。
- SA_INTERRUPT 这个标志指的是在禁止本地上所有的中断，而不是自已本身。
- IRQ_PER_CPU:该IRQ只能发生在一个cpu上
  IRQ_LEVEL:该中断由电平触发
  IRQ_MASKED:屏蔽了其他中断
- irq_desc->handler和irqaction->handler老是混淆.
- set_current_state这个函数不就只是改变进程的状态吗?并没有对运行队列做出相应的操作.这有什么
  用呢?好像在schedule()里会检测这个标志.
- ULK有句这样的话被我误解了:Right after switching from User Mode to Kernel Mode, the
  kernel stack of a process is always empty, and therefore the esp register points to the
  byte immediately following the stack.我以为从内核态切回用户态后就会把内核栈给清掉了,原来
  不是,只是说把esp给改变了,当这个进程再切回到内核态时就会用回原来在内核态的数据,如调用read
  函数时可能会休眠以致切换到其它进程.
- ret_from_intr()和ret_from_exception()时若有可能会进行调度。
- ULK:a preemptive kernel differs from a nonpreemptive kernel on the way a process running
  in Kernel Mode reacts to asynchronous events that could induce a process switchfor
  instance, an interrupt handler that awakes a higher priority process. We will call this
  kind of process switch a forced process switch。linux不能在中断里进行进程切换。那么不在定
  时器中断里进行切换那么它是怎么做到分时的呢？
- ULK：Both in preemptive and nonpreemptive kernels, a process running in Kernel Mode can
  voluntarily relinquish the CPU, for instance because it has to sleep waiting for some
  resource. We will call this kind of process switch a planned process switch.非抢占不是说
  不能在内核态进行进程切换。
- schedule()可能因为preempt_count设了PREEMPT_ACTIVE而会不调度，是不是说明调用schedule()有可
  能不会切换进程的可能呢？
- 对于多核处理机，使用PREEMPT_ACTIVE实现禁止抢占是没有什么作用的，不能防止其它CPU防问临界区，
  仅仅是不能进行进程切换而已，要用自旋锁之类的.不是这样的，还是有用的，不禁止抢占会
- ULK：kernel preemption may happen either when a kernel control path (usually, an
  interrupt handler) is terminated, or when an exception handler reenables kernel
  preemption by means of preempt_enable( )，kernel preemption may also happen when
  deferrable functions are enabled.所以有三个地方是内核固定会调度的。
- 感觉PREEMPT_ACTIVE实现的抢占在单核上就像一个LBK，在多核中又没什么作用。
- ULK:spin locks are usually convenient, because many kernel resources are locked for a
  fraction of a millisecond only;therefore, it would be far more time-consuming to release
  the CPU and reacquire it later.
- ULK:In the case of a uniprocessor system, the locks themselves are useless, and the spin
  lock primitives just disable or enable the kernel preemption.
- 可以确定设置PREEMPT_ACTIVE是不能/会抢占的意思。
- 通过一个虚拟地址访问物理地址时，是不是先判断这时的权限是什么，是内核权限呢还是用户权限？
  如果是内核权限那么就减去PAGE_OFFSET就是内核地址，如果是用户权限那么就用页表找到物理地址？
  是这样子的吗？
- 每个进程都有自已的页表，是吗？应该是的，若只有一个页表，那么不同的进程使用不同的线性地址
  时会访问到相同的物理地址。
- 内核访问内存时是不通过MMU的吗？MMU是怎么被使用的呢？使用MMU是要有一个寄存器指向全局页表的
  物理地址，当进程访问内存时（读写内存），MMU就会用虚拟地址和那个寄存器找到具体的物理地址。
  所以在打开MMU之前要把页表准备好，之后再把全局页表的地址给那个寄存器。
  www.cnblogs.com/leaven/archive/2011/04/18/2019696.html
- http://daimajishu.iteye.com/blog/1080667 这里有说到进程切换时会把那个存放进程全局页表地址
  的寄存器修改，若不使用MMU就不修改。当系统访问一个32位虚拟地址时，假设cache未命中且TLB（页
  地址快表）中未存有这个地址页的情况下，MMU将全局页目录存放PGD的地址读出来进行寻址。ARM存储
  体系支持的页的大小有几种——1M,64KB,4KB,1KB，支持的二级页表大小有两种：粗粒度和细粒度。
- 虽然内核的线性地址与物理地址是线性关系，差了PAGE_OFFSET,但是这不表明访问内核内存空间时不
  使用MMU。也因为是线性关系，所以内核在它自已的空间里如果线性地址是连续的，那么对应的物理地
  址也是连续的。
- 内核映像并不是可直接运行的目标代码，而是一个压缩过的zImage（小内核）。但是，也并非是
  zImage映像中的一切均被压缩了，映像中包含未被压缩的部分，这部分中包含解压缩程序，解压缩程
  序会解压缩映像中被压缩的部分。zImage使用gzip压缩的，它不仅仅是一个压缩文件，而且在这个文
  件的开头部分内嵌有gzip解压缩代码。http://blog.chinaunix.net/uid-12567959-id-160966.html
- 如何保证进程不能访问内核数据？http://daimajishu.iteye.com/blog/1080667 有回答。虚拟内存到
  物理内存的映射真的能保证系统安全稳定吗？也有回答
- x86在进程切换*前*确实会有类似 movl 新的页表地址，cr3 的处理，新的页表地址即是新的进程的页
  表基地址。这一点不会有问题，因为此时CPU仍然在系统空间运行，而所有进程的页面目录中与系统空
  间相对应的目录项都指向相同的页面表，所以，不管换上哪一个进程的页面目录都一样，受影响的只
  是用户空间，系统空间的映射则永远不变。http://bbs.csdn.net/topics/100049760
- 所有进程的pgd里的768-1023都是一样，都是拷贝swapper_pg_dir的，且大小刚好是
  1G.http://bbs.csdn.net/topics/100049760 这个帖子讨论了内核空间使用不使用MMU，和
  PAGE_OFFSET的关系。我觉得访问内核空间也要用MMU的,为什么要PAGE_OFFSET呢？我觉得这个可以使
  得在系统启动初始化页表时更方便且在一些时候也方便，如可以在内核DUMP时打印的信息可以判断出
  错的内存空间是否在与内核相关。
- 若是内存小于896的话，虽然这些内存都可以映射到内核空间，但肯定不可能全映射到内核空间去的，
  如果可以确定一个地址是内核空间的线性地址，那么可以用它减去PAGE_OFFSET来获得物理地址，但若
  是用户空间的地址，那么就算是小于896的内存也不能用它减去PAGE_OFFSET获得物理地址。
- 页描述符有一个成员叫_mapcount： Number of Page Table entries that refer to the page
  frame (-1 if none).可以看出一个页框可以被多个Page Table页表项映射。

** struct pid * fastcall find_pid(enum pid_type type, int nr)
- 没办法，还是要用遍历
** int fastcall attach_pid(task_t *task, enum pid_type type, int nr)
- 注意所有的成员都要设置好。
** static fastcall int __detach_pid(task_t *task, enum pid_type type)
- 返回值被detach_pid使用得有点巧妙。
** void fastcall detach_pid(task_t *task, enum pid_type type)
- 比__detach_pid ()多一个功能就是把从pid位图里把nr删掉。
** task_t *find_task_by_pid_type(int type, int nr)
** void switch_exec_pids(task_t *leader, task_t *thread)
- 一个非领头线程调用sys_execve()时就调用它。
** void __init pidhash_init(void)
** void __init pidmap_init(void)
- 主要是做0号进程的工作。只分配一页。
** int alloc_pidmap(void)
- 在这里也分配页给page
- alloc_pidmap里的求max_scan的方法为什么要减!offset呢?因为若不在一页的起始位置就要减去0而不是1是因为想多循环一次当前页，所以max_scan指的是
  将要经过多少次页头（页尾）.
** int default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
- 仅仅是调用try_to_wake_up
- key参数没有用
- 所以自定义唤醒函数时,里面可以一开始调用default_wake_function,在最后做一些想做的事情,而不
  能在一开始做想做的事情因为进程还没有切换.不是这样的,无论在default_wake_function之前还是之
  后所做的事都是在被切换的进程的上下文中进行的.
** int autoremove_wake_function(wait_queue_t *wait, unsigned mode, int sync, void *key)
- 调用上一个函数后从链表中删除,删除用list_del_init,与__remove_wait_queue所用的list_del不一
  样,为什么是在调用上一个函数之前而不是之后呢?被try_to_wake_up()之后没有从等待队列里删除会
  不会又再次唤醒呢?
** #define DEFINE_WAIT(name)
- 用了上一个函数作为唤醒函数。
- 若用这个定义一个WAIT,因为用上一个函数作为唤醒调用函数,所以同时会把它从队列删除.唤醒之后不
  用再把它从队列删除.
** static inline void init_waitqueue_func_entry(wait_queue_t *q, wait_queue_func_t func) 
*** include/linux/wait.h:
- 可以自定义唤醒函数。仅此而已,没有赋值给task
** static inline void init_waitqueue_entry(wait_queue_t *q, struct task_struct *p)
*** include/linux/wait.h:
- 与上一个比多了初始化进程。但唤醒函数用default_wake_function, flags都是0
** #define DECLARE_WAITQUEUE(name, tsk)
*** include/linux/wait.h:
- 注意与DEFINE_WAIT的不同，用tsk,default_wake_function,NULL和NULL初始task_list,而不是
  current,autoremove_wake_function,LIST_HEAD_INIT
- 那么用DECLARE_WAITQUEUE定义的要不要在删除的时候把它从链表删除呢？要的,用
  remove_wait_queue，在ulk里也有说的:unless DEFINE_WAIT or finish_wait( ) are used, the
  kernel must remove the wait queue element from the list after the waiting process has
  been awakened.
** #define DECLARE_WAIT_QUEUE_HEAD(name)
*** include/linux/wait.h:
- 用自已来初始化链表.
** static inline void init_waitqueue_head(wait_queue_head_t *q)
*** include/linux/wait.h:
- 结果和DECLARE_WAIT_QUEUE_HEAD(name)一样.
** static inline int waitqueue_active(wait_queue_head_t *q)
*** include/linux/wait.h:
- 看队列是否为空
** static inline void __add_wait_queue(wait_queue_head_t *head, wait_queue_t *new)
*** include/linux/wait.h:
- 这个是加在队列前面的
** static inline void __add_wait_queue_tail(wait_queue_head_t *head, wait_queue_t *new)
*** include/linux/wait.h:
- 这个是加在队列尾的
** void fastcall __sched sleep_on(wait_queue_head_t *q)
*** kernel/sched.c:
- 就是改状态,加入队列,schedule,删除队列. 要注意加锁.
- sleep_on系列的函数是与等待队列相关的.
- 时间窗口出现在改状态和schedule之间可能会被唤醒.
- the sleep_on( )-like functions cannot be used in the common situation where one has to
  test a condition and atomically put the process to sleep when the condition is not
  verified; therefore, because they are a well-known source of race conditions, their use
  is discouraged.
** long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
** long fastcall __sched interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
** void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
** void fastcall prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)
*** include/linux/wait.h:
- 这个用于把current加入等待队列的。
- 注释有说为什么把设置进程状态放在加入队列的后面
- 要先判断wait->task_list为空的时候才把wait加入队列。为什么在sleep_on里不用呢?因为
  prepare_to_wait的应用场合不同，prepare_to_wait会放在一个循环里重复调用，但是finish_wait不会被放到循环里，看看__wait_event就知道了。
- 虽然在is_sync_wait里会检查wait是否为空，但进入prepare_to_wait是肯定不会为空的，所以is_sync_wait做了多余的事情。
** #define is_sync_wait(wait)	(!(wait) || ((wait)->task))
*** include/linux/wait.h:
- 有一段注释：Used to distinguish between sync and async io wait context: sync i/o typically specifies a NULL wait queue entry or a wait
  queue entry bound to a task (current task) to wake up. aio specifies a wait queue entry with an async notification
  callback routine, not associated with any task.为什么同步io可以指定一个NULL 的wait呢？
** void fastcall prepare_to_wait_exclusive(wait_queue_head_t *q, wait_queue_t *wait, int state)
*** include/linux/wait.h:
- 不同的是设置了exclusive标志。
** void fastcall finish_wait(wait_queue_head_t *q, wait_queue_t *wait)
*** kernel/wait.c:
- 用了list_empty_careful，为什么呢？只能用于调用list_del_init的情况，因为list_del_init里调用了INIT_LIST_HEAD
- sleep_on是状态->插入队列->schedule->删除队列;插入队列(prepare)（检测是否已插入）->状态
  (prepare)（检查同步）->schedule->状态(finish)->删除队列(finish)(先list_empty_careful)
- 有一个例子：
#+BEGIN_EXAMPLE
    DEFINE_WAIT(wait);
    prepare_to_wait_exclusive(&wq, &wait, TASK_INTERRUPTIBLE);
                                /* wq is the head of the wait queue */
    ...
    if (!condition)
        schedule();
    finish_wait(&wq, &wait);
#+END_EXAMPLE
** #define wake_up(x)			__wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, 1, NULL)
*** kernel/sched.c:
- 要知道linux是不能指定下一个切换到某个进程。
- wake_up也不能指定唤醒某个进程（把某个进程状态改成运行），注意只有一个参数x，但是找到一个
  被唤醒的进程后就会马上调用它的func，因为大部分的func是default_wake_function，会调用
  try_to_wake_up
- 等待队列是从第一个开始唤醒的，一个wait可以加入到队列头add_wait_queue也可以加到队列尾
  add_wait_queue_tail，同时还有互斥和非互斥的wait，所以可以用这些东西组合成一个有优先级的队
  列。

** #define wake_up_nr(x, nr)		__wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, nr, NULL)
** #define wake_up_all(x)			__wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, 0, NULL)
** #define wake_up_interruptible(x)	__wake_up(x, TASK_INTERRUPTIBLE, 1, NULL)
** #define wake_up_interruptible_nr(x, nr)	__wake_up(x, TASK_INTERRUPTIBLE, nr, NULL)
** #define wake_up_interruptible_all(x)	__wake_up(x, TASK_INTERRUPTIBLE, 0, NULL)
** #define wake_up_locked(x)		__wake_up_locked((x), TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE)
- 已经把队列给lock住了
** #define wake_up_interruptible_sync(x)   __wake_up_sync((x),TASK_INTERRUPTIBLE, 1)
** void fastcall __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
*** kernel/sched.c:
- 这个函数目前为止只是用于上一个宏，所以nr_exclusive一直是1，但是在实现的时候nr_exclusive为0的时候就不同步了，为什么呢？
** static void __wake_up_common(wait_queue_head_t *q, unsigned int mode, int nr_exclusive, int sync, void *key)
- sync这个参数是只是传给func而已。
- 如果想唤醒所有的进程而不管它是否互斥，那么nr_exclusive就是0，实现的方法是!--nr_exclusive
** clone
- 这个是C的库函数，它有多个参数但是它调用的sys_clone只有一个参数，转而调用的do_fork有多个参
  数。但是ARM又是不一样的，它的包含了很多参数。
- 关于fn和arg参数在ULK有：the wrapper function saves the pointer fn into the child's stack
  position corresponding to the return address of the wrapper function itself; the pointer
  arg is saved on the child's stack right below fn.
** fork
- 也是一个C库函数。
- ULK:The traditional fork( ) system call is implemented by Linux as a clone( ) system
  call whose flags parameter specifies both a SIGCHLD signal and all the clone flags
  cleared, and whose child_stack parameter is the current parent stack pointer. Therefore,
  the parent and child temporarily share the same User Mode stack.总之比clone就多了一个
  SIGCHLD和与父进程共用一个堆栈.
** vfork
- 也是一个C库函数。
- ULK:The vfork( ) system call, introduced in the previous section, is implemented by
  Linux as a clone( ) system call whose flags parameter specifies both a SIGCHLD signal
  and the flags CLONE_VM and CLONE_VFORK, and whose child_stack parameter is equal to the
  current parent stack pointer.总之比fork就多了CLONE_VM和CLONE_VFORK
** long do_fork(unsigned long clone_flags, unsigned long stack_start, struct pt_regs *regs, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr)
*** kernel/fork.c:
- 注意参数的意思
- 如果clone_flags和current->ptrace的符合某些条件时，就算clone_flags不设置CLONE_PTRACE也给它加上。主要是看current_ptrace的设置。
- 关于调用ptrace_notify在ULK有这样说：If the parent process is being traced, it stores the
  PID of the child in the ptrace_message field of current and invokes ptrace_notify( ),
  which essentially stops the current process and sends a SIGCHLD signal to its
  parent. The "grandparent" of the child is the debugger that is tracing the parent; the
  SIGCHLD signal notifies the debugger that current has forked a child, whose PID can be
  retrieved by looking into the current->ptrace_message field.
- 在调用ptrace_notify时的参数在里面被赋给了task_struct->exit_code，为什么要这样呢？也赋给了
  si_code,这还可以理解
- 调的ptrace_notify的原因：因为父进程被跟踪而且要求被创建的子进程也要被跟踪，所以就调用了。
  调用完这个函数的过程中因调用schedule(do_notify_parent_cldstop())所以会停下。
- 这个函数是在哪里真正创建一个进程并开始以两个执行路径运行的呢？好像整个进程都是以current来
  运行的。那创建的子进程在什么时候运行呢？可能是在copy_process函数里把它插入到了某个运行队列里了。
- 若CLONE_VFORK设置了要在ptrace_notify之后才可以等待，子进程运行完。
** static inline int fork_traceflag (unsigned clone_flags)
*** kernel/fork.c:
- 在clone_flags里的最低8位是指定退出时所要发送的信号。
- 若系统调用是由vfork发起的且想跟踪vfork发起的创建的子进程就返回PTRACE_EVENT_VFORK;若子进程
  退出时所发的信号不是SIGCHLD(为什么要这个条件呢？)且想跟踪clone创建的子进程就返回PTRACE_EVENT_CLONE；若想跟踪由
  fork创建的子进程就返回PTRACE_EVENT_FORK.
- 为什么是CLONE_VFORK是要使用completion原语呢？因为vfork的man手册有一段这样的话：vfork()
       is a special case of clone(2).  It is used to create new processes without copying
       the page tables of the parent process.  It may be useful in performance-sensitive
       applica‐ tions where a child is created which then immediately issues an
       execve(2)vfork() differs from fork(2) in that the parent is suspended until the
       child terminates (either normally, by calling _exit(2), or abnormally, after
       delivery of a fatal signal), or it makes a call to execve(2).  Until that point,
       the child shares all memory with its parent, including the stack.  The child must
       not return from the current function or call exit(3), but may call _exit(2).
       Signal handlers are inherited, but not shared.  Signals to the parent arrive after
       the child releases the parent's memory (i.e., after the child terminates or calls
       execve(2)).
- CLONE_STOPPED:Forces the child to start in the TASK_STOPPED state.
- 若设置了CLONE_STOPPED,为什么还要设置PT_PTRACE才可以添加SIGSTOP的信号呢?
** void fastcall wake_up_new_task(task_t * p, unsigned long clone_flags)
*** kernel/sched.c:
- 再次说一下task_t->array是指向CPU运行队列里的某一个active或expire成员.
- 如何通过一个task_t来获得一个运行队列:从task_t里的thread_inof里的CPU来找到task是在哪一个
  CPU上,知道哪个CPU就可以找出相应的运行队列了.task_t里的run_list就是task_t->array链表里的一
  个结点.
- 会根据是否共用相同的VM和是否在同一个CPU来插入进程和父进程的相对位置。
- __activate_task会使用enqueue_task来把进程插入到相应的运行队列尾，而不是头。
- 在这个函数里current->array会有空的时候,是什么时候呢？
- 为什么不共享VM就要子进程运行先呢？注释有说明是因为可能运行exec，那么是不是子进程运行
  exec后会把所有的原来的VM删掉呢？
- 好像在cpu==this且CLONE_VM清除且current->array不为空时的情况下没有设置array->bitmap,这个是
  一个bug吗？在这里为什么要把子进程的prio设置成父进程的prio呢？难道仅是为了想在父进程之前运
  行而把它放在程父进程相同优先级的运行队列中？
- 它的this_rq为什么不是通过task_rq_lock来获取的呢？而是根据cpu==this_cpu来判断的呢？
- 为什么在不是同一个CPU时要重新计算timestamp呢？计算的方法是减去父进程所在运行队列的
  timestamp_last_tick再加上子进程所在运行队列的timestamp_last_tick
- __activate_task这个函数里会把进程加入到运行队列里的，虽然名子看起来不是这样子的，但结合参
  数一起还是可以看来的。
- 把一个进程加入到另外一个CPU之后还要看那个CPU需不需要重新调度。实现很简单，用被加入的进程
  的优先级（动态优先级）与CPU上的运行队列里的curr->prio比较即可。
- 为什么不实现CONFIG_SMP版和非CONFIG_SMP版的呢？像resched_task那样。
- set_need_resched()和resched_task()不一样的，前者只是设置了current的标志，而后者会让其它
  CPU的进程重新调度。
- 为什么要把current的运行队列锁住呢？
- 这个函数只有do_fork调用而已
** void ptrace_notify(int exit_code)
*** kernel/signal.c:
- ULK有解释：ptrace_notify( ), which essentially stops the current process and sends a
  SIGCHLD signal to its parent.
- si_signo的是SIGTRAP，且调的的ptrace_stop函数里的do_notify_parent_cldstop是用相应的
  CLD_TRAPPED
- 在这个函数里建立的siginfo_t是在ptrace_stop函数被放到last_siginfo里，在
  do_notify_parent_cldstop里建立的siginfo_t是发给跟踪进程的.
- 调的这个函数因ptrace_stop的schedule，所以可能会被调度.是在do_notify_parent_cldstop里唤醒
  父进程,在ptrace_stop调度。
- current重新可以运行后是马上看有没有挂起的进程。为什么这之前要把last_sigpending清掉呢？
** static void ptrace_stop(int exit_code, int nostop_code, siginfo_t *info)
*** kernel/signal.c:
- 这个函数的作用应该是在current被跟踪时用来停止current的,并通知跟踪进程
- 不知道为什么要自减group_stop_count
- task_t->last_siginfo是给跟踪用的,保存的东西有什么用呢?
- 为什么要设置current->exit_code呢?
- 把current的状态改成TASK_TRACED之后没有把它从运行队列中删除吗？
- 函数里是先解siglock的锁再加上siglock的锁
- 为什么那个PT_ATTACHED要非呢?好像不对的吧
- current->parent->signal不等于current->signal是不是说明current与current->parent不在同一个
  线程组呢?
- 为什么要各种条件不成立的时候再次设置进程为TASK_RUNNING状态呢？有注释说是跟踪进程已不在了。
** static void do_notify_parent_cldstop(struct task_struct *tsk, struct task_struct *parent, int why)
*** kernel/signal.c:
- 重新认识一下struct siginfo_t,结构体里的联合体用得有技巧，因为_kill->_pid和_sigchld->_pid
  的地址相同，_kill->_uid和_sigchld->_uid地址相同，所以只需提供访问_kill或_sigchld里其
  中_pid和_uid即可，所以只提供了访问了_kill->_uid和_kill->_pid的宏而没有_sigchld.si_errno是
  The error code of the instruction that caused the signal to be raised, or 0 if there was
  no error。si_code是A code identifying who raised the signal 。si_status是exit code.不知道
  si_utime和si_stime有什么用，保存什么的，但是它们分别被赋tsk->utime和tsk->stime,原来ULK里
  列出的si_code只是一部分，还有一些SIGILL类、SIGFPE类、SIGSEGV类、SIGBUF类的、SIGTRAP类的、
  SIGCHLD类的等。每次发信号时都要填这个结构体，在这个函数里因为要给发一个信号所以要填这个结
  构体,又因为发的是SIGCHLD信号，所以要填结构体中的联合体的SIGCHLD结构体。
- CLD_TRAPPED这个表示被跟踪进程被捕获，可能运行到了breakpoint,所以运行要停下来，这时候应该
  给跟踪进程发一个信号，所以在这个函数里要检查CLD_TRAPPED是否被置。但是为什么也要检查
  CLD_CONTINUED.
- 为什么是CLD_STOPPED时要这样设置si_status呢？是CLD_TRAPPED时要这样设置si_status呢?
- 既然要在两个判断之后才使用info为什么花那么多时间先设置info呢？
- 因为一个子进程只有一个父进程，所以__wake_up_parent函里使用的wait_chldexit等待队列最多只有
  一个进程，这样解释对吗？
- wait_chldexit是给wait4()系统调用用的，但是子进程只给父进程发一个SIGCHLD信号而已，父进程是
  怎么是子进程退出了呢？
- 这个函数的主要功能是什么呢？是给父进程发一个SIGCHLD信号，而且还必须与CLD_CONTINUED、
  CLD_STOPPED、CLD_TRAPPED相关的。
** static task_t *copy_process(unsigned long clone_flags, unsigned long stack_start, struct pt_regs *regs, long stack_size, int __user *parent_tidptr,int __user *child_tidptr, int pid)
*** kernel/fork.c:
- p->user这个成员是一个指针，就是说还是和父进程共用的。这是对的，因为创建子进程时的用户是与
  父进程的用户相同的
- nr_threads在这个函数里增加，在__unhash_process减，__unhash_process被release_task和
  unhash_process调用
- max_threads表示什么意思呢？它在fork_init里被初始化
- 我觉得nr_threads>=max_threads不应该放在nr_threads++之前，如果root调用fork足够多次且每次都
  在nr_threads>=max_threads这个比较之后且在nr_threads++之前那么nr_threads就会大于
  max_threads
- 在这里把tgid设置了pid,就是说创建一个进程时把子进程作一个新线程组的领头进程,是这样子吗?为
  什么呢?但是设置了CLONE_THREAD又不同了.
- 为什么要设置set_child_tid和clear_child_tid呢？clear_child_tid是什么来的，有一句主释：
  Clear TID on mm_release()?
- 创建的子进程是不能跟踪它运行后的系统调用的.
- 把父进程的执行域设置成了自已的执行域,为什么呢?
- 为什么要这样设置exit_signal呢?若没设置CLONE_THREAD,那么用clone_flags里的
- 要设置group_leader为子进程,好像不对的吧.是不对,在后面会因为CLONE_THREAD而做修改的
- 把cpus_allowed设置成与current的一样,设置子进程的cpu与current的一样.
- 若current有一个SIGKILL信号,那么它就不能创建子进程.
- 若设置了CLONE_PARENT或CLONE_THREAD的时候要把子进程的real_parent设置了current的
  reald_parent,为什么有CLONE_THREAD的要这样设置,正真创建子进程的current竟然不是current,
- 刚创建完的子进程的parent与real_parent是一样的.
- 若CLONE_THREAD设置了同是SIGNAL_GROUP_EXIT也设置了,那么是不能创建子进程的.
- 若CLONE_THREAD设置了那么把子进程的group_leader设置成current的group_leader
- SIGNAL_GROUP_EXIT和group_stop_count没有关联的吗?可以不设置SIGNAL_GROUP_EXIT但
  group_stop_count可以大于0?
- 若group_stop_count大于0说明一个全组的停止正在进行,那么就要把正创建的进程加入到停止组中.
- 有一个跟踪进程的被跟踪进程链表(task_struct->ptrace_list, task_struct->ptrace_childen)
- 子进程一定会被插入到PID hash表和TGID hash（tgid在上面被设置成正确的值了）表，但是为什么要
  在子进程为线程组领头进程时才会把子进程插入PGID hash表和SID hash表呢？
- 在这里居然还会检查p->pid是否为0，是否多此一举？
- 每个CPU都有一个进程个数计数器process_counts
- ULK有一句这样的话：If the child is a thread group leader (flag CLONE_THREAD cleared)。就
  是说CLONE_THREAD若不设置那么子进程就是线程组领头进程，若进程是一个线程而不是一个线程组领
  头进程那么它就不是一个进程组的成员，若进程是一个线程组领头进程那么它就是一个进程组的成员，
  那么一个进程怎样才可以成为进程组领头进程呢？
- 关于进程归属的问题总结ULK:若子进程是一个thread group leader(清CLONE_THREAD),就设tgid为
  pid(就是自已),设group_leader为自已(这个有点想不明白,源码的确的这样的.)那么除了把子进程插
  入到TGID,PGID之外还要插入到SID中.若子进程不是一个thread group leader(置CLONE_THREAD),那么
  tgid设为current->tgid(注意不是current,所以线程组不能嵌套),设group_leader为
  current->group_leader并把它插入到TGID中去,但是为什么子进程不是thread group leader了还要插
  入到TGID中呢？哦看错了，源码是这样的attach_pid(p, PIDTYPE_TGID, p->tgid);不是插p而是
  p->tgid那么每创建一个线程时领头线程不是都要被插一次，这点在ULK上表述有错。不是，我错了，
  的确的把子进程插入TGID中。要重新认识一下那4个链表。
** static struct task_struct *dup_task_struct(struct task_struct *orig)
*** kernel/fork.c:
- prepare_to_copy()在i386里用来关闭fpu，在arm里什么也不做。
- 就是分配了task_struct和thread_info并拷贝和设置之间的指针；再设置tsk->usage.
** static inline void copy_flags(unsigned long clone_flags, struct task_struct *p)
*** kernel/fork.c:
- 这个函数是被copy_process调用的，为什么要把PF_SUPERPRIV清掉呢？不能继承父进程的
  PF_SUPERPRIV吗？为什么把PF_FORKNOEXEC给置了？
- 由copy_process传入的clone_flags在do_fork可能对CLONE_PTRACE动了手脚.task_struct->ptrace是
  在这里设置的,在do_fork里用到,为什么不在do_fork做修改呢?隔太远了.
** static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
*** kernel/fork.c:
- 一个后台程序可能不包含任何文件.
- 若设置了CLONE_FILES那么就用current的files,不用改.
- 
** static int count_open_files(struct files_struct *files, int size)
*** kernel/fork.c:
- 这种的计算方法是不是有的粗略了?
** static inline int copy_fs(unsigned long clone_flags, struct task_struct * tsk)
*** kernel/fork.c:
- CLONE_FS:ulk:Shares the table that identifies the root directory and the current working
  directory, as well as the value of the bitmask used to mask the initial file permissions
  of a new file (the so-called file umask ).
** static inline struct fs_struct *__copy_fs_struct(struct fs_struct *old)
*** kernel/fork.c:
- 主要的步骤的分配一个fs_struct再把old里的值拷贝过去
** static inline int copy_sighand(unsigned long clone_flags, struct task_struct * tsk)
*** kernel/fork.c:
- 若是CLONE_sighand或CLONE_THREAD其中一个设置就与父进程共享，即增加计数器就可以了。
- 若不共享，但还要把action的内容拷贝过来，为什么呢？所以无论共享与否都会把共享action
** static inline int copy_signal(unsigned long clone_flags, struct task_struct * tsk)
*** kernel/fork.c:
- 若CLONE_THREAD设置那么就共享进程的，增加计数器即可。
** static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
*** kernel/fork.c:
- 若current->mm为空的时候就不用拷贝也不分配了，为什么不分配了呢？
- 若设置了CLONE_VM就共享.若不共享,还会把父进程的mm给拷贝过来.再后来又修改了部分成员
** int copy_thread(int nr, unsigned long clone_flags, unsigned long esp, unsigned long unused, struct task_struct * p, struct pt_regs * regs)
*** arch/i386/kernel/process.c:
- 这个函数初始化了task_struct->thread_info结构体,其中有reg成员、栈、ip寄存器，使子进程被调
  度运行时在正确的栈和ip指针下运行。
- ULK:The value returned by the system call is contained in eax: the value is 0 for the
  child and equal to the PID for the child's parent. To understand how this is done, look
  back at what copy_thread() does on the eax register of the child's process.
** void fastcall sched_fork(task_t *p)
*** kernel/sched.c:
- 函数一开始把进程状态设置为运行,但到目前为止还没有把进程插入到运行队列,所以还不会被调度.但
  可以不可以通过把它插入到一个等待队列来获得运行呢?
- 给子进程分时间片时为什么current的时间片要先加1呢?current->time_slice不可能是0;若是1的话,
  如果不加1,那么子进程的时间就是0,又因为current的时间片又会用移位的方式重新调整,所以若是1,
  最后父和子进程的时间片都是0.用这种计算方式无论如何都不会增加和减少原来current的时间片.
- 调整之后的current时间片若为0那么会马上开始定时器的调度.
** NORET_TYPE void do_group_exit(int exit_code)
*** kernel/exit.c:
- SIGNAL_GROUP_EXIT是该线程组中所有的线程都要设置的吗？还是只是调用这个函数的线程才会设置。
  不是这样的，task_struct->signal是线程共享的。
- 这代码实现了退出代码的传递性，就是第一个调用该函数的线程所使用的exit_code参数才会被一值采
  用。
** void zap_other_threads(struct task_struct *p)
*** kernel/signal.c:
- SIGNAL_GROUP_EXIT不是在do_group_exit里设置了吗？为什么又要在这里设置SIGNAL_GROUP_EXIT呢？
  多此一举吗？
- 要把group_stop_count清零。所以group_stop_count是从0开始计数的。
- thread_group_exit不是在do_goup_exit里查过了吗？这里是不是多此一举呢？
- 在copy_process里可以看出线程的group_leader与领头线程的一样。但这里为什么有不相等的情况呢？
  一个线程可以不与领头线程在相同的进程组吗？可以从注释上看出若线程执行execve或类似的东西的
  时候不在同一进程组
- 不在同一进程组时要设exit_signal为-1呢？就算不在同一线程组也要发SIGKILL信号给它。
- 会把被杀线程的SIGSTOP,SIGSTP,SIGTTIN,SIGTTOU的信号删掉。
** void signal_wake_up(struct task_struct *t, int resume)
*** kernel/signal.c:
- ULK:to notify the process about the new pending signal
- 若想恢复执行，那么就不管进程的状态是TASK_INTERRUPTIBLE，TASK_STOPPED还是TASK_TRACED.
- 会调的wake_up_state,转而调用try_to_wake_up来唤配进程。若在其它CPU会发一个CPU间中断让它调
  度。
** fastcall NORET_TYPE void do_exit(long code)
*** kernel/exit.c:
- 在能在中断上下文调的，不能对进程0进程1调用
- 按照字面意思，PT_TRACE_EXIT应该是指跟踪进程退出，就是在退出时通知跟踪进程。
** static void exit_notify(struct task_struct *tsk)
*** kernel/exit.c:
- 有一个调用exit同时该进程被选中来执行一个线程组信号。这时它要找其它线程来处理。
- 有注释：Check to see if any process groups have become orphaned as a result of our
  exiting, and if they have any stopped jobs, send them a SIGHUP and then a SIGCONT.
  (POSIX 3.2.2.2)
- 如果被杀进程的exit_signal不等于SIGCHLD且exit_signal不为空，安全域或执行域被修改且没有
  KILL权限那么改exit_signla为SIGCHLD。为了是让父进程知道子进程已死。
- 进入这个函数之后，退出进程有可能在EXIT_ZOMBIE状态，也有可能在EXIT_DEAD状态，如果进入
  EXIT_DEAD了，那么之后会调用release_task.若没在出退信号且不被跟踪或在进行组退出就是
  EXIT_DEAD,否则是EXIT_ZOMBIE.如果在这里被设置成EXIT_ZOMBIE,那么会在那被设成EXIT_DEAD呢？好
  像是如果在EXIT_ZOMBIE时，就表明等待父进程调用wait类函数。
- 在这里会把forget_original_parent收集到的子进程给release_task掉。
- 在这个函数一定会把tsk->flags设为PF_DEAD，说明无论是EXIT_DEAD还是EXIT_ZOMBIE,那是PF_DEAD.
- 想到一个问题：release_task之后应该是所有的内存空都被收回了，但是为什么被退出的进程还可以
  运行呢？因为内存空间被收回后并没有把内存空间的内容清掉且这些内存没有被分配，因为已把抢占
  给禁止了，所以进程的代码代和数据都没被破坏。好像上面那个解释是错的，因为ULK：The
  release_task( ) function detaches the last data structures from the descriptor of a
  zombie process; it is applied on a zombie process in two possible ways: by the
  do_exit()function if the parent is not interested in receiving signals from the child,
  or by the wait4( ) or waitpid( ) system calls after a signal has been sent to the
  parent. In the latter case, the function also will reclaim the memory used by the
  process descriptor,while in the former case the memory reclaiming will be done by the
  scheduler (see Chapter7).
** static inline void forget_original_parent(struct task_struct * father, struct list_head *to_release)
*** kernel/exit.c:
- ULK：All child processes created by the terminating process become children of another
  process in the same thread group, if any is running, or otherwise of the init process.
- 里面有一个child_reaper是被初始化为init_task的。
- 要处理这个进程的两个进程链表：子进程链表，跟踪进程链表。在task_struct->children里这两种的
  进程那包含了。
- 有注释：If something other than our normal parent is ptracing us, then send it a SIGCHLD
  instead of honoring exit_signal.  exit_signal only has special meaning to our real
  parent.被杀进程要是被跟踪，就发SIGCHLD，否则且exit_signal不为空且所在的线程组没有其它线程
  (thread_group_empty(tsk)是指tsk作为一个线程所在的线程组没有其它线程，不是指以tsk作为线
  程组领头线程的线程组没有其它线程)（为什么要这个条件呢？）
** static inline void reparent_thread(task_t *p, task_t *father, int traced)
*** kernel/exit.c:
- exit_signal等于-1是什么意思,表示没有任何信号，就是初始值，不能用0，这个好像用来测试的。
- 若子进程有退出信号，那么把它改成SIGCHLD,为什么要这样子做呢？
- pdeath_singal是在ULK：The signal sent when the parent dies时候发的。但是发给谁呢？从代码
  看好像是发给自已,从forget_original_parent来看好像发给父进程的.
- 在task_struct里关于跟踪的链表也有两个：ptrace_list和ptrace_children.
- 在这里father是正在退出的进程,在进入这个函数之前real_parent被改了，无论是有没跟踪的。
- 参数traced说明的是父进程有没有被跟踪。
- 如果父进程是被跟踪的且p的parent和real_parent不相同，那么就把p插入到real_parent的
  ptrace_children中,但什么时候父进程是被跟踪的且parent与修改之后的real_parent是相同的呢？好
  像这样是不可能的吧，因为在以参数trace=1调用之前p是在father->ptrace_children链表里的，所以
  p->parent一定是father，又因为在调用reparent_thread () 之前调用了choose_new_parent把
  p->real_parent改成了不可能为father的reaper,所以在reparent_thread里是不可能有
  p->parent==p->real_parent的,如果有可能的话，那么可能是p虽在father->ptrace_children里但是
  p->parent不等于father.
- ptrace不为0是什么意思，为0又是什么意思。
- 如果进程A在进程B的children中，那么进程A的real_parent一定是进程B吗？
- 如果进程A在进程B的ptrace_children中，那么进程A的parent一定是进程B吗？
- 不知道为什么在traced假的时候要设p->ptrace为0而且还把p->parent改为p->real_parent，如果是这
  样的话，那么有可能有这样一种情况：p->parent和p->real_parent不相同(p被p->parent跟踪)且它的
  real_parent正被杀成为father参数且又先以traced=0调用reparent_thread ()，这时会在reparent里
  把p->ptrace设为0，把p->parent设为p->real_parent,但一直没有从p->parent的ptrace_children把
  p删掉，这合理吗？
- 在这个函数里如果发现有僵死进程且有退出信号那么就通知父进程，为什么还要加多一个判断线程组是否为空呢？
- 关于p->state==TASK_TRACED 有注释:If it was at a trace stop, turn it into a normal stop
  since it's no longer being traced.
- 又有不明白了，本来p和father是父子关系，按理说应该在同一个进程组，但是为什么还要判断是否在
  同一个进程组呢？可以这样的。孤儿进程组： 一个进程组中的所有进程的父进程要么是该进程组的一
  个进程，要么不是该进程组所在的会话中的进程。 一个进程组不是孤儿进程组的条件是，该组中有一
  个进程其父进程在属于同一个会话的另一个组中。函数里有一个判断孤立进程组的代码？又有一个问
  题：在同一个进程组里的任意一个进程都与同组中其它至少一个进程有父子或兄弟关系吗？
- 在forget_original_parent里以traced为0调用reparent_thread时的father是p的real_parent,在这种
  情况下，在reparent_thread里会把对p的跟踪去掉，换句话说就是如果一个进程A的父进程
  real_parent被杀掉，那么进程A就不能再被跟踪和去跟踪了，因为p->ptrace被清和p->parent设为
  p->real_parent。
- 如果进程A的父进程被杀掉，且进程A被跟踪且是EXIT_ZOMBIE状态，且没有退出信号，这种情况为什么
  要收集这些进程呢？
- 在forget_original_parent里以traced为1调用reparent_thread时的father是p的parent（可能与
  real_parent一样），且p->parent不等于p->real_parent,在这种情况下，reparent_thread里会把
  p->ptrace_list插入到p->real_parent->ptrace_children中，但p->real_parent可能不是一个跟踪函
  数，为什么要这样做呢？有这样的注释：Preserve ptrace links if someone else is tracing
  this child.
** void ptrace_untrace(task_t *child)
*** kernel/ptrace.c:
- 也不是一定是切回TASK_STOPPED状态，还要看看是不是有停止信号。
** void release_task(struct task_struct * p)
*** kernel/exit.c:
- 在这里p->ptrace还有可能不为0，这是会调用__ptrace_unlink把p->trace改为0
- 为什么如果这个函数被跟踪就要脱离跟踪呢?而且是在release_task里做
- ULK关于task_struct->parent的说明：this is the process that must be signaled whn the
  child process terminates。
- 看了ULK的一段话:If the process is not a thread group leader, the leader is a zombie, and
  the process is the last member of the thread group, the function sends a signal to the
  parent of the leader to notify it of the death of the process.和看了源码,有一个结
  论:task_struct->group_leader是线程组的领头进程的task_struct.为什么要这样的需求呢?
- 通知已在EXIT_ZOMBIE状态的进程还有用吗?它会做出响应.
- 为什么thread_group_empty(leader)为true时表示被杀进程是最后一个线程,因为被杀进程
  在_unhash_process里被从PIDTYPE_TGID中删除了.
- 在这里调用了put_task_struct回收task_struct了
- 有注释:If we were the last child thread and the leader has exited already, and the
  leader's parent ignores SIGCHLD, then we are the one who should release the leader.所以在
  最后p又会回到函数的开始来把leader删除掉.
** void __ptrace_unlink(task_t *child)
*** kernel/ptrace.c:
- 有一个问题：silbing是一定与real_parent有关系的吗？如果parent与real_parent不相等就和
  parent没有任何关系吗？如果是这样，那为什么还要在这个函数里调同REMOVE_LINKS(child)呢？因为
  REMOVE_LINKS是与real_parent有关的.
- 
** void __exit_signal(struct task_struct *tsk)
*** kernel/signal.c:
- atomic_dec_and_test(v):Subtract 1 from *v and return 1 if the result is zero; 0
  otherwise
- 为什么没有其它进程用signal之后还可以找到next_thread呢？
- 关于group_exit_task有注释： notify group_exit_task when ->count is equal to notify_count；
  everyone except group_exit_task is stopped during signal delivery of fatal signals,
  group_exit_task processes the signal.
- notify_count是通知group_exit_task的阀值，有这种需求吗？
- flush_sigqueue(tsk->pending)是一定的，但是共享的要在signal_struct使用计数为0的时候才flush.
- 如果没有人用signal_struct会在最后回收signal_struct结构体。
** void __exit_sighand(struct task_struct *tsk)
*** kernel/signal.c:
- 这个函数比较简单，没人使用就直接回收。
** static void __unhash_process(struct task_struct *p)
*** kernel/exit.c:
- 减nr_threads, 从PIDTYPE_PID, PIDTYPE_TGID,中删除， 若是线程组领头进程就从PIDTYPE_PGID和
  PIDTYPE_SID中删除，从进程链表中删除。
- 要为线程组领头进程才可以减process_counts,为什么会这样呢?可能创建一个非领头线程时不会增加
  process_coun
- 在这里有可能p->pid为空吗?
** void fastcall sched_exit(task_t * p)
*** kernel/sched.c:
- 用来修改父进程的时间片和平均睡眠时间.注意父进程是parent而不是real_parent
- 有注释:Potentially available exiting-child timeslices are retrieved here - this way the
  parent does not get penalized for creating too many threads.
- 若进程还没有执行完一次，那么把退出的进程的时间片加回到父进程，以防父进程因创建太多的子进
  程后马上退出而使得父进程的时间减少。
- 若进程的平均睡眠时间比父进程的平均睡眠小那么就更新父进程平均睡眠时间
** void disable_irq(unsigned int irq)
*** kernel/irq/manage.c:
- 这个函数要同步的，看是否正在执行这个中断函数。所以进入中断函数之后不能禁止本中断，这可能是
  为什么要在进入该中断函数之前系统会自已禁止中断。
** void disable_irq_nosync(unsigned int irq)
*** kernel/irq/manage.c:
- depth是先判断再增加的
- 这个函数可以多次被调用，仅是在depth上有变化。
** void synchronize_irq(unsigned int irq)
*** kernel/irq/manage.c:
- 这里的这个是在多处理器的情况下实现的，在非多处理器的情况下的实现在
  include/linux/hardirq.h下，就是一个barrier而己。
- 在进行处理中就relax cpu.
** void enable_irq(unsigned int irq)
*** kernel/irq/manage.c:
- 在case 1时不用break,我还以为depth没有减少
- 虽然在case 1时会挽回丢失的中断，但是已经晚了，因为中断不是在被应答之后马上处理的，这种情
  况有点意思，CPU A在接收到中断后接着CPU B才禁止中断，但是因为中断丢失所以要在CPU B禁止之后
  的不确定时间后才执行中断。
- 有IRQ_PENDING和IRQ_DISABLE就表明有中断丢失了
** fastcall unsigned int __do_IRQ(unsigned int irq, struct pt_regs *regs)
*** kernel/irq/handle.c:
- 在这里会增加kstat->irqs[irq],kstat是per_cpu变量。
- 为什么在这个函数里还要检查IRQ_DISABLE和IRQ_INPROGRESS呢？
- 在这个函数里会把IRQ_PENDING清掉，若IRQ_DISABLE或IRQ_INPROGRESS设置了也有可能会把
  IRQ_PENDING设置了。
- 相同的中断处理函数有可能正在别的CPU上执行，为什么不推到接收的那CPU上运行该中断处理
  函数呢？ULK：This leads to a simpler kernel architecture because device drivers'
  interrupt service routines need not to be reentrant (their execution is
  serialized). Moreover, the freed CPU can quickly return to what it was doing, without
  dirtying its hardware cache; this is beneficial to system performance.
- 那个死循环里处理特点是可以在执行这个函数的过程中处理别的CPU刚刚接收到的相同的中断。但是假
  如有多个相同的中断发生，那么只有一个未决的中断,这个未决的中断在handle_IRQ_event里发生（设
  置IRQ_PENDING）有注释：* This applies to any hw interrupts that allow a second instance
  of the same irq to arrive while we are in do_IRQ or in the handler. But the code here
  only handles the _second_ instance of the irq, not the third or fourth. So it is mostly
  useful for irq hardware that does not mask cleanly in an SMP environment.
- 如果IRQ_PER_CPU被设置了,那么就不用加锁了,也不用处理IRQ_PENDING, IRQ_INPROGRESS所有这些标志.
** int request_irq(unsigned int irq, irqreturn_t (*handler)(int, void *, struct pt_regs *), unsigned long irqflags, const char * devname, void *dev_id)
*** kernel/irq/manage.c:
- 是SA_SHIRQ时那么dev_id就不能为空.
- 基本是用参数初始化一个irqaction之后调用setup_irq()
** int setup_irq(unsigned int irq, struct irqaction * new)
*** kernel/irq/manage.c:
- 不能
** void open_softirq(int nr, void (*action)(struct softirq_action*), void *data)
*** kernel/softirq.c:
- 就是在softirq_vec的第nr个元素下设置data和action
** void fastcall raise_softirq(unsigned int nr)
*** kernel/softirq.c:
- 仅是调用了raise_softirq_irqoff
- 执行软中断是有检测点的(local_bh_enable,do_IRQ等),所以raise_softirq之后不会马上进入软中断.
** inline fastcall void raise_softirq_irqoff(unsigned int nr)
*** kernel/softirq.c:
- 调用了__raise_softirq_irqoff.
- 若不在中断上下文且没禁止软中断就马上调用wakeup_softirqd用线程来执行软中断
- ULK:If we're in an interrupt or softirq, we're done (this also catches softirq-disabled
  code). We will actually run the softirq once we return from the irq or softirq.
** #define __raise_softirq_irqoff(nr)
*** include/linux/interrupt.h:
- 主要是用local_softirq_pending获取__softirq_pending来将相应的位置1.
- 所以在pending了某个软中断之后多次raise_softirq()它是没有效果的.
** #define in_interrupt()		(irq_count())
*** include/linux/hardirq.h:
- 这个为1表明是在中断也可能是禁止了中断.
** asmlinkage void do_softirq(void)
*** kernel/softirq.c:
- 若抢占禁止那么一开始就要退出去,禁止枪占时不处理软中断.
** asmlinkage void do_softirq(void)
*** kernel/softirq.c:
- 在raise的时候会把__softirq_pending的位给置了,那在哪里清呢?是在这里.
- 在执行软中断函数的时候是打开软中断的,没有禁止.
- 在这个函数里会把所有pending的软中断都处理掉.
- 轮询完一次后发现又有新的软中断就又重新开始,但重新开始的次数是有限的,当次数到限后还有
  pending的软中断那么就用线程来处理了.
- 因为是轮询整个softirq_vec,所以优先级的区别就没那么大了.
** static int ksoftirqd(void * __bind_cpu)
*** kernel/softirq.c:
- 这个函数只有在被通知退出的时候才会退出,退出的结果是线程也终止.
- 在执行do_softirq()前要禁止抢占,将进程状态设为运行,执行do_softirq()后要使能抢占,将进程状态
  设为可中断,
** int kthread_should_stop(void)
*** kernel/kthread.c:
- 这个函数什么意思呢?
- 这个函数会在内核线程里用循环使用这个函数来判断是否应该退出线程
- 这个函数的实现比较简单，就是把线程停止信息中(struct kthread_stop_info)的停止进程与
  current比较
- 但是只有一个struct kthread_stop_info的全局变量，所有的进程都用这个变量，但多个进程都用这
  个变量的话若是多个进程都要禁止线程执行不是会出问题吗？
- 会不会在多个进程中都会执行相同的线程呢？如在进程A进入内核态时执行了线程C，在线程C还没有被
  退出时切换到了进程B，在进程B进入内核态时又执行了线程C，这时就会有两个进程执行线程C了。关
  键要看kthread_stop()什么时候调用了。
** int kthread_stop(struct task_struct *k)
*** kernel/kthread.c:
- 因为thread_stop_info引用了stask_struct结构体，所以就要调用get_task_struct()
- 在这个函数里用了init_completion()来初始化了kthread_stop_info.done,且调用了
  wait_for_completion(),但是在ksoftirqd这个线程函数里没有相应地调用complete(),也没有设置相
  应的kthread_stop_info.err
** struct task_struct *kthread_create(int (*threadfn)(void *data), void *data, const char namefmt[], ...)
*** kernel/kthread.c:
- 这个函数用来创建一个线程，是用工作队列来运行线程函数的，用completion来同步创建完成而不是
  同步创建开始，其实给工作队列执行的函数还不是线程函数，还是一个中间过程而已。执行的函数是
  keventd_create_kthread,被执行的函数在它的参数create里。
- 为什么要先判断helper_wq呢?因为工作队列的创建也是用到这个函数的,执行工作队列里的那些工作是
  在通过创建一个线程来执行的.但是又因为创建内核线程也是用工作队列来完成的
  (keventd_create_kthread()),所以这个先有鸡还是先有蛋的问题就出来了.
** static void keventd_create_kthread(void *_create)
*** kernel/kthread.c:
- 在这里没有调用线程的执行函数，而是以线程的执行函数来创建一个内核线程，是调用
  kernel_thread()创建内核线程的。
- 创建完成之后要等待创建开始的completion.
- 最后要complete创建远成以通知kthread_create().
** int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
*** arch/i386/kernel/process.c:
- 这个函数是与架构相关的，与sys_clone的类似，里面调用了do_fork来创建一个线程，被创建的这个
  线程所执行的函数还不是我们要执行的函数，还是一个系统函数kthread函数。
- 就i386的架构来看，执行的函数是先是kernel_thread_helper,再在里面调用kthread(),返回后而调用
  do_exit(),所以线程函数执行完返回到kthread()，kthread()再返回到
  kernel_thread_helper,kernel_thread_helper再调用do_exit();
** static int kthread(void *_create)
*** kernel/kthread.c:
- 在这个函数里会调用我们所要执行的函数
- 这个内核线程会屏蔽所有的信号。
- 为什么在执行线程函数之前要以TASK_INTERRUPTIBLE把current给切换出去呢？所以也可以看出创建完
  线程还有一点延时才执行线程函数。
- 为什么要在执行schedule()之前complete创建开始完成。
- 因为先调用kthread_should_stop再调用线程函数，所以可能一次都没执行线程函数就终止了线程。
- 所以创建内核线程的函数执行顺序是kthread_create创建一个工作队列的工作执行
  keventd_create_kthread(),keventd_create_kthread()调用与架构相关的
  kernel_thread(),kernel_thread()调用do_fork()来执行kthread(),kthread()里会调用要执行的线程
  函数，kthread()函数会等待线程的终止。
- 执行软中断函数的方式也是以一个独立的软中断线程为载体执行的，以kksoftirqd函数为参数调用
  kthread_create,ksoftirqd()这个函数是被kthread()这个函数调用的，有意思的一点是kthread()使
  用了kthread_should_stop()判断是否退出循环，ksoftirqd()也使用了，但从代码的设计看这是必要的。
** __init int spawn_ksoftirqd(void)
*** kernel/softirq.c:
- 创建当前CPU的软中断线程,要创建多个CPU的ksoftirqd线程时就要调用多次了。
- 先用CPU_UP_PREPARE调用cpu_callback()，再用CPU_ONLINE调用cpu_callback()
- 这些线程是在哪个进程的内核栈中执行的呢?这个进程会不会退出呢?这个线程应该是在init进程的内
  核栈里执行的.好像这些线程又不是用内核栈的?是的,内核线程也一个进程,只不过它在内核运行,一个
  进程的执行怎么会用到其它进程的栈呢?使用内核栈的都是系统调用和中断执行之类的.
- 那么线程是怎样被调度的呢?与用户进程一样吗?
** void kthread_bind(struct task_struct *k, unsigned int cpu)
*** kernel/kthread.c:
- 原来把一个进程绑定到特定的CPU执行是那么容易就实现了的。
- 这个函数的作用就是把线程k绑定到第cpu个cpu上执行.
- 绑定之前要先说线程先停下来.
- 绑定的方法就是设置线程k的cpu再设置cpumask.
** static int __devinit cpu_callback(struct notifier_block *nfb, unsigned long action, void *hcpu)
*** kernel/softirq.c:
- 这个函数只是用来给软中断用的.
- 根据kthread_should_pending()来决定是否退出。
- 为CPU_UP_PREPARE时就创建线程,再绑定到一个CPU上,为CPU_ONLINE时就唤醒一个线程.
- 这个函数创建的线程是调用ksoftirqd这个函数的.软中断只需一个线程就可以了,ksoftirqd里调用
  do_softrirq(),所以它会轮询所有等级的软中断.执行do_softirq()会禁止抢占。
- 每个CPU都有一个执行本地CPU的所有等级软中断的内核线程(ksoftirqd()),而执行HI_SOFTIRQ和
  TASKLET_SOFTIRQ这些等级的软中断时会执行完tasklet函数链表中函数.
** void __init softirq_init(void)
*** kernel/softirq.c:
- 就是打开了TASKLET_SOFTIRQ和HI_SOFTIRQ而已,与其它的软中断无关了,也就这两个软中断是提供给内
  核开发者用的.
- 被添加到这两个软中断函数是用链表的形式关联的.
- struct tasklet_head是struct tasklet_struct里next的头结点.
** void fastcall __tasklet_schedule(struct tasklet_struct *t)
*** kernel/softirq.c:
- 新加入的tasklet是放在链表头的,但执行的时候也是从链表头开始执行的,就是先进后出(堆栈)
- 插入后会raise,就像向一个工作队列里添加一个工作之后马上唤醒处理该工作队列的线程。
** static inline int tasklet_trylock(struct tasklet_struct *t)
*** include/linux/interrupt.h:
- 看tasklet是否在TASKLET_STATE_RUN的状态,主要用来防止一个tasklet在其它CPU被执行,本地CPU不会
  嵌套tasklet吗?执行tasklet函数之前没有禁止抢占.
- 禁止抢占就是禁止进程切换禁止调度，不能禁止中断，在中断里是不能进行调度的。
** static inline void tasklet_unlock(struct tasklet_struct *t)
- 清掉TASKLET_STATE_RUN状态,可以看出tasklet用来防止本地CPU嵌套执行相同的tasklet函数(不是同
  一等级的tasklet)是用tasklet_struct->state是否在TASKLET_STATE_RUN决定的,所以在运行tasklet
  函数时是可以被中断的.
** static inline void tasklet_schedule(struct tasklet_struct *t)
*** include/linux/interrupt.h:
- 要先看这个tasklet是否正在被调度,若是就不能再把这个tasklet插到tasklet的运行链表里了(就是调
  用__tasklet_schedule()),所以正在schedule的函数不能在插入到运行链表中.
** static void tasklet_action(struct softirq_action *a)
*** kernel/softirq.c:
- 这个函数就是TASKLET_SOFTIRQ这个等级所要的执行的函数,这个函数会轮询tasklet_vec这个链表中所
  有的tasklet函数.
- 每执行完这个函数就会把所有的tasklet函数执行完,再等到下一次执行TASKLET_SOFTIRQ这个等级的软
  中断时就又会执行这个函数.
- tasklet_trylock(),tasklet_unlock(),tasklet_unlock_wait()这些函数是与
  TASKLET_STATE_SCHED/RUN有关的,tasklet_disable_nosync(),tasklet_disable(),tasklet_enable()这
  些是与tasklet_struct->count有关的,为什么要两个标志呢?前者是用来防止同一个tasklet在多个
  CPU执行的,而count可以防止同一个tasklet在同一个CPU嵌套执行,因为执行一个tasklet时是没有禁止
  中断的,所以不用count的话可能会同一个tasklet在同一个CPU嵌套执行(这个说法错误,同一个
  tasklet是不可能被嵌套执行的),使用count是可以使得即使某个tasklet被插入了也可以使它不被执
  行.两者的区别可能是另一种情况：TASKLET_STATE_SCHED/RUN表明的是tasklet有没有运行，所以
  tasklet_trylock(),tasklet_unlock(),tasklet_unlock_wait()这些只能用于执行tasklet的前后，而
  tasklet_struct->count是用来显示禁止tasklet执行的,不管在TASKLET_STATE_SCHED还是在
  TASKLET_STATE_RUN，同时tasklet_disable()会调用tasklet_disable_nosync()和
  tasklet_unlock_wait()这说明这个函数会被阻塞，要在为TASKLET_STATE_SCHED时才可以。
- 从代码可以看出如果一个tasklet在其它的CPU上正在被执行或被禁止时会把这个tasklet重新插到链表
  头,等着下一次执行tasklet_action时再执行,因为tasklet_action()下一次被调用的时候可能是在
  do_softirq()函数里的下一次循环中所以会很快被调用,也有可以是下一次进入从ksoftirqd()进入
  do_softirq()的时候.
- ULK:using two kinds of non-urgent interruptible kernel functions: the so-called
  deferrable functions, and those executed by means of some work queues.这可以看出软中断函
  数是可以被中断的.但其实不然,因为在调用__do_softirq()之前已经把中断给禁止了(这种说法是错误
  的,禁止的是抢占而不是中断)
- ULK:interrupt context : it specifies that the kernel is currently executing either an
  interrupt handler or a deferrable function.禁止抢占了就是在中断上下文吗?,禁止中断了也是在
  中断上下文吗?可能叫临界区,中断上下文与临界区是不一样的.临界区应该包含中断上下文的意思,都不能调度不能休眠.
- Softirqs are statically allocated, while tasklets can also be allocated and initialized
  at runtime.
- 因为在执行__do_softirq()时已经禁止抢占但没有禁止中断,所以执行可延迟函数时是不能做调度和休
  眠的,但是可以被中断,所以软中断是执行在中断上下文的.
** static void tasklet_hi_action(struct softirq_action *a)
*** kernel/softirq.c:
- 与上一个类似
** #define in_interrupt()		(irq_count())
*** include/linux/hardirq.h:
- in_interrupte()判断的是Softirq counter域和Hardirq counter是否为正,不检查Preemption
  counter域,而preempt_disable()增加的是Preemption counter域,所以in_interrupte()和
  preempt_disable()是不相干的.
** struct workqueue_struct *__create_workqueue(const char *name, int singlethread)
*** kernel/workqueue.c:
- 与工作队列相关的结构体和变量:struct workqueue_struct,struct cpu_workqueue_struct(这个结构
  体包含在struct workqueue_struct中,wq成员指回包含它的struct workqueue_struct),workqueues工
  作队列链表
- 为什么当创建非单个CPU的工作队列时会把要创建的工作队列插入到workqueues里而创建单个CPU的工
  作队列时就不用呢?又在什么地方删除呢?
- 调用了create_workqueue_thread()创建工作队列.创建之后马上唤醒它。所以主要就两个任务：创建后唤醒。
** static struct task_struct *create_workqueue_thread(struct workqueue_struct *wq,
*** kernel/workqueue.c:
- 初始化完cpu_workqueue_thread结构体后调用kthread_create()创建一个线程调用worker_thread(),
  传以初始化完的cpu_workqueue_thread.kthread_create()创建线程的方式用了工作队列.
** static int worker_thread(void *__cwq)
*** kernel/workqueue.c:
- 进入这个函数时已经是在新创建的内核线程里执行了
- 这里要把线程状态加多一个PF_NOFREEZE状态,这个状态有什么用呢?用来指明这个线程在休眠时不能冻
  结,/linux/Documentation/power/freezing-of-tasks.txt有说明.
- 在这个函数里就是一直循环执行已插入工作队列的工作,直到这个线程被要求停止
  (kthread_should_stop())
- 把一个没有执行函数的wait_queue_t插入到struct cpu_workqueue_struct->more_work是什么意思呢?应
  该是这样的:这个wait_queue_t的task被设为current了,就是这个执行工作队列的内核线程,所以要想
  执行这个工作队列的工作可以wake_up这个more_work里的进程,是这个意思吗?ULK: more_work:Wait
  queue where the worker thread waiting for more work to be done sleeps.调用schedule()把当
  前线程加入到more_work()当schdule()返回后就从more_work删掉当前进程.虽然more_work是一个等待
  队列,但是并没有用到等待队列比较关键的东西(用wake_up类函数唤醒进程),仅仅是把一个有current
  的waitqueue插入到more_work而没有任何作用.不是这样的,在__queue_work()函数里有调用
  wake_up()把more_work的进程全唤醒.
- 因为这个函数是在一直循环执行的, 但是它是在一开始就用current定义了wait这个waitqueue,而
  current就是新创建的内核进程，所以current进程运行就会执行工作，在__queue_work里会在把一个
  工作插入工作队列之后马上唤醒current.但是为什么要在while循环里把wait加到more_work里又把它
  删除呢？就是在调度之前，在current状态为TASK_INTERRUPTIBLE的时候会把wait加到more_work里，
  而调度完之后会把wait从more_work里删除而把状态改为TASK_RUNNING。
- 好像只有current在more_work里.不会有其它进程了。
- 好像有一个BUG：在把wait从more_work里删除之后more_work可能为空了，但是queue_work这个函数不
  管有没有空都会从more_work里唤醒进程。这个BUG是不存在的，因为queue_work就是会把工作插入到
  当前的CPU的，所以执行这个函数的current（B）一定不是处理工作队列那个current（A），所以A已
  经是在调用schudule中，所以A一定是被插入到了more_work中，所以queue_work里调用wake_up是没有
  问题的,在remove之后再到add之前内核线程是不会切换出去的,即使产生了中断,在内核态时如果不调
  用schudle自动放弃，否则是不会被切到其它进程的。所以在run_workqueue()里执行的工作是在内核
  线程的上下文执行的.
** int fastcall queue_work(struct workqueue_struct *wq, struct work_struct *work)
*** kernel/workqueue.c:
- 一个工作被插入工作队列之后执行之前是不能再被插入的,这点和tasklet是一样的.
- 被插入的工作只会被插入到当前的CPU,不是所有的CPU.
** static void __queue_work(struct cpu_workqueue_struct *cwq,
*** kernel/workqueue.c:
- 在queue_work里找到CPU后在这个函数里设置wq_data,这个要找到CPU后才可以设置.
- insert_sequence在这里自加
- 会把more_work里的进程给唤醒以执行工作函数.
** static inline int is_single_threaded(struct workqueue_struct *wq)
*** kernel/workqueue.c:
- 若一个工作队列不是单CPU的，那么会把workqueue_struct.list插入到workqueues这个全局变量,难道
  workqueues仅仅是把所有非单CPU工作队列链在一起的作用吗？从代码看好像这样链也没什么作用。现
  在发现是有用的,因为struct workqueue_struct这个结构体里有一个cpu_wq成员是一个数组,就是说分
  配一个workqueue_struct结构体的时候就会分配整个数组,但是为了分辨一个workqueue_struct变量是
  一个CPU用的还是多个CPU用的,所以把所有的非单CPU的工作队列全放到一个链表里,以便对一个工作队
  列操作时可以根据该workqueue_struct是否被链到workqueues链表而作出不同的操作.
** static inline void run_workqueue(struct cpu_workqueue_struct *cwq)
*** kernel/workqueue.c:
- 注释说run_depth是防止run_workqueue()函数嵌套的,但是从代码看没有嵌套的可能啊.就算在工作函
  数中进行了休眠也不可能被嵌套的啊,因为CPU与cpu_workqueue_struct与内核线程是绑定的,而且
  run_workqueue是一个静态的函数所以不能被外部调用,在可能用的范围内也出现不了嵌套啊.出现了就是一个BUG.
- 执行这个函数一次会把worklist里的所有工作都处理掉。
** static void flush_cpu_workqueue(struct cpu_workqueue_struct *cwq)
*** kernel/workqueue.c:
- 如果是参数cpu_workqueue_struct是当前CPU的,那么就调用run_workqueue()这个是不会出现
  run_workqueue()嵌套的.
- 这里把当前的insert_sequence保存下来与remove_sequeuece比较来判断是否某个cpu_workqueue已经
  没有工作可以执行,但是为什么要用这种方式呢?直接用一个原子变量不行吗?或用一个加锁的计数器不
  行吗?但因为访问insert_sequence和remove_sequeuece都获得了自旋锁，所以也不会出问题。
- 如果调这个函数所在的CPU不与cpu_workqueue_struct所属的CPU不是同一个就不会调用
  run_workqueue()仅仅是调用了schdule()因为所以工作一定要在指定的CPU执行，这样也可以防止
  run_workqueue()嵌套执行。
- 在这里会使用work_done这个等待队列来实现同步，在run_workqueue()会wake_up这个工作队列。
  work_done就只是这个作用而已。
** int fastcall queue_delayed_work(struct workqueue_struct *wq, struct work_struct *work, unsigned long delay)
*** kernel/workqueue.c:
- 实现延时执行的方法就是定时器插入工作。在时间到来的时候调用delayed_work_timer_fn()来把工作
  插入工作队列。虽然把工作插入之后会马上调用wake_up把more_work唤醒，但是这个是有比较大的延
  迟的。
- 在没有把工作插入工作队列之前已经把work.pending给设置了，这个合适吗？合适的，因为不这样可
  能有多个定时器同时在等着把工作插入工作队列。
** #define get_cpu()		({ preempt_disable(); smp_processor_id(); })
** #define put_cpu()		preempt_enable()
*** include/linux/smp.h:
- 先调用preempt_enable_no_resched()递减抢占计数器然后再调用preempt_check_resched()判断是否
  要切换该进程来调用preempt_schedule().
** #define put_cpu_no_resched()	preempt_enable_no_resched()
** asmlinkage void __sched preempt_schedule(void)
*** kernel/sched.c:
- 
** #define DEFINE_PER_CPU(type, name)
*** include/asm-generic/percpu.h:
- 为什么在多核处理器下多单核处理器下多加了一个指定段的编译属性就可以实现给每个CPU分配变量了？
- 就像声明一个全局变量一样，因为这些全局变量都有一个per_cpu作为开头，所以以后声明变量时要注意
  了。
** #define per_cpu(var, cpu) (*RELOC_HIDE(&per_cpu__##var, __per_cpu_offset[cpu]))
*** include/asm-generic/percpu.h:
- 获取第cpu个的var变量。
** #define __get_cpu_var(var) per_cpu(var, smp_processor_id())
*** include/asm-generic/percpu.h:
- 用per_cpu实现的
** #define get_cpu_var(var) (*({ preempt_disable(); &__get_cpu_var(var); }))
-　比上一个多了一个禁止抢占
** #define put_cpu_var(var) preempt_enable()
*** include/linux/percpu.h:
- 打开抢占而已。name不使用
** #define alloc_percpu(type)
*** include/linux/percpu.h:
- 这个是动态分配的，直接调用__alloc_percpu
** static inline void *__alloc_percpu(size_t size, size_t align)
*** include/linux/percpu.h:
*** mm/slab.c:
- 是单核的话就直接用kmalloc分配，是多核也用kmalloc分配，但分配的是struct percpu_data结构，
  再用kmalloc_node来根据每个CPU各自所用的内存结点来分配。
** #define spin_lock_init(lock)	do { (void)(lock); } while(0)
*** include/linux/spinlock.h:
- 这是单核的定义.
** #define spin_lock_init(x)	do { *(x) = SPIN_LOCK_UNLOCKED; } while(0)
*** include/asm-i386/spinlock.h:
- 这是多核的的定义.
** #define spin_lock(lock)		_spin_lock(lock)
*** include/linux/spinlock.h:
- 这个的定义没有分单多核.这个函数与ULK说的是不一样的,竟然调用preempt_enable()的地方不一样,
  应该在spin_unlock里调用的把.看错ULK了,在spin_lock里调用preempt_enable()是在可抢占内核里实
  现的.如果在不可抢占的内核中会一直自旋,不放弃CPU.
- pause指令相当于rep;nop.
** #define _spin_lock(lock)
*** include/linux/spinlock.h:
- 这是单核的定义,三步:禁止抢占,调_raw_spin_lock,调__acquire.
- 为什么要禁止抢占呢?这样可以防止自已自动放弃CPU.
** void __lockfunc _spin_lock(spinlock_t *lock) __acquires(spinlock_t);
- 也上单核的操作方式
** #define _raw_spin_lock(lock)	do { (void)(lock); } while(0)
*** include/linux/spinlock.h:
- 这是单核的方式
** static inline void _raw_spin_lock(spinlock_t *lock)
*** include/asm-i386/spinlock.h:
- 多核的与架构有关,用汇编实现的.
** #define spin_unlock(lock)	_spin_unlock(lock)
*** include/linux/spinlock.h:
- 这个不分单多核.
** #define _spin_unlock(lock)
*** include/linux/spinlock.h:
- 这是单核的实现,就三步:调用_raw_spin_unlock,打开抢占,调用__release
** void __lockfunc _spin_unlock(spinlock_t *lock)
*** kernel/spinlock.c:
- 与单核的一样.
** #define spin_unlock_wait(lock)	(void)(lock)
*** include/linux/spinlock.h:
- 这是单核的实现
** #define spin_unlock_wait(x)	do { barrier(); } while(spin_is_locked(x))
*** include/linux/spinlock.h:
- 这是多核的实现
- 死循环调用spin_is_locked
** #define spin_is_locked(lock)	((void)(lock), 0)
*** include/linux/spinlock.h:
- 这是单核的实现
** #define spin_is_locked(x)	(*(volatile signed char *)(&(x)->slock) <= 0)
*** include/asm-i386/spinlock.h:
- 这个是多核的实现,就只是一个判断而已.
** #define rwlock_init(lock)	do { (void)(lock); } while(0)
*** include/linux/spinlock.h:
- 这是单核的实现。
** #define rwlock_init(x)	do { *(x) = RW_LOCK_UNLOCKED; } while(0)
*** include/asm-i386/spinlock.h:
- 这是多核的实现。
- 0x01000000为可读可写，当想读时就将它减1，为0x00ffffff，为0x00000000时是已加上了写锁。
- 像这些自旋锁的实现，要根据是否是单多核，是否配置为可抢占。
** void __lockfunc _spin_lock(spinlock_t *lock)
*** kernel/spinlock.c:
- 在这个头文件里实现了_read_lock(), _read_lock(), _spin_lock_irqsave(), _spin_lock_irq(),
  _spin_lock_irq(), _spin_lock_bh(), _read_lock_irqsave(), _read_lock_irq(),
  _read_lock_bh(), _write_lock_irqsave(), _write_lock_irq(), _write_lock_bh(),
  _spin_lock(), _write_lock()这些函数的可抢占版和非抢占版。而这些函数里面调用的前面有_raw字
  样的就是被实现了单核版和多核版，单核版在include/linux/spinlock.h里实现的是单核版，多核版
  在特定的架构的spinlock.h文件里实现。
- 抢占版的都会调用cpu_relax()和使用break_lock成员。抢占版的会打开抢占，允许其它进程抢占，自
  旋锁不再自旋。
- 这类函数的调用关系层是以spin_lock为例：spin_lock() -> _spin_lock()(加一个下划线前缀) ->
  _raw_spin_lock()
- 关于_spin_lock()类函数的可抢占版和非抢占版的所在的文件的定义：要使用自旇锁就要包含文件
  include/linux/spinlock.h(A文件),在文件A定义了单核版的_spin_lock()宏和多核版的_spin_lock()函
  数声明，单核版的_spin_lock()没有可和非可抢占版之分,多核版的可抢占版和非抢占版
  的_spin_lock()的实现都在kernel/spinlock.c里(注意:实现的函数的用宏的方式,所以cscope找不到
  的).单核版的_spin_lock()里调用的_raw_spin_lock()在文件A里定义,单核版的_spin_lock()调用
  的_raw_spin_lock()是有调试版和非调试版的;多核版的_spin_lock()的可抢占版调
  用_raw_spin_lock(),这个_raw_spin_lock()不是单核版那个,而是在include/asm-i386/spinlock.h里
  定义的那个,这个_raw_spin_lock()会用到pause指令(rep;nop),而多核版的_spin_lock()的非可抢占
  版(!CONFIG_PREEMPT)没调用_raw_spin_lock(),而是调用了cpu_relax()而且在调用cpu_relax()之前还使
  能抢占,cpu_relax()也是用rep;nop指令的.
- 单核版的_spin_lock() -> 禁止抢占后调用有调试版的_raw_spin_lock(); 多核版的可抢占版
  的_spin_lock() -> _raw_spin_lock()(架构相关), 多核版的非可抢占版的_spin_lock() -> 使能抢
  占再调用cpu_relax().
** #define seqlock_init(x)	do { *(x) = (seqlock_t) SEQLOCK_UNLOCKED; } while (0)
*** include/linux/seqlock.h:
- #define SEQLOCK_UNLOCKED { 0, SPIN_LOCK_UNLOCKED }
** static inline void write_seqlock(seqlock_t *sl)
*** include/linux/seqlock.h:
- seqlock_t这个锁只是防止多个写操作,与读无关.
- 把sequence自加。
** static inline void write_sequnlock(seqlock_t *sl) 
*** include/linux/seqlock.h:
- 又把sequence自加。
** static inline int write_tryseqlock(seqlock_t *sl)
*** include/linux/seqlock.h:
- 用了spin_trylock(),写是用锁的
** static inline unsigned read_seqbegin(const seqlock_t *sl)
*** include/linux/seqlock.h:
- 返回sequence，读是不用锁的
** static inline int read_seqretry(const seqlock_t *sl, unsigned iv)
*** include/linux/seqlock.h:
- iv为奇数表示正在写，写的锁没有解，sl不等于iv表示已被修改过。这两种情况都要重新执行。
** #define rcu_read_lock()		preempt_disable()
*** include/linux/rcupdate.h:
- 就只是禁止抢占而已
** #define rcu_read_unlock()	preempt_enable()
*** include/linux/rcupdate.h:
- 就只是使能抢占而已
- RCU锁和seq锁有一个区别是:RCU锁可以使用旧的数据作处理,不管数据是否被改变,而seq锁不能,如果
  在处理完之后发现被保护的数据被改变就要重新用新的数据再做一次处理.
** void fastcall call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
*** kernel/rcupdate.c:
- 把head插到nxttail这个链表的尾部.
- 这个函数把一个新的释放rcu旧数据的回调函数加到rcu_data这个per-cpu变量的nxttail这个成员的链
  表里去.
- 不是一个CPU经过了静止状态就调用所有的回调函数,而是所有的CPU都经过一次静止状态之后就执行一
  个tasklet来把所有在nxttail里的callback函数执行一次来释放旧的rcu数据,那么回调函数是怎么知
  道自己要释放的数据是什么呢?从回调函数的参数是struct rcu_head可以启发到被释放的数据度该是
  一个结构体,这个结构体里至少有一个struct rcu_head类型的成员,而这个成员就是做为rcu_head参数
  的那个,也是被链到nxttail的,所以可以从nxttail里把存放callback函数的struct rcu_head传给回调
  函数,回调函数再用container_of找到要释放的结构体就可以了.
** void synchronize_rcu(void)
*** kernel/rcupdate.c:
- 这个函数用completion来同步,等待所有CPU经过静止状态.
- 这个函数的实现就是把一个唤醒自己的函数插入到rcu_tasklet里.
- 有一个疑问:是不是被complete之后可能还有callback函数在tasklet里呢?可能有,这个函数的作用只
  是说经过了一个静止状态,而不是说返回之后就没有callback函数在tasklet里了.所以这个函数可以这
  样用:write一个rcu之后调用这个函数,这个函数返回之后可以读到最新的rcu数据没有延时太长的时间.
** void rcu_check_callbacks(int cpu, int user)
*** kernel/rcupdate.c:
- 这个函数有什么用呢?
** static void rcu_process_callbacks(unsigned long unused)
*** kernel/rcupdate.c:
- 就是调用__rcu_process_callbacks(),有软中断和非软中断之分.
** static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp, struct rcu_state *rsp, struct rcu_data *rdp)
*** kernel/rcupdate.c:
- 在curlist不为空和完成的rcu(rcu_ctrlblk.completed)回收不小于rcu_data.batch的要求时就将
  rcu_data.curlist的移到rcu_data.donetail中去并把rcu_data.curlist清空,这个rcu_data.donetail有什么用
  呢?rcu_data.completed是在哪里加的呢?
- 如果rcu_data.nxtlist不空且rcu_data.curlist为空(已经移到了donetail里去了),就把nxtlist移到
  curlist里去并清空nxtlist.这三个链表nxtlist,curlist,donelist就是这种关系了.设置本cpu的
  batch号为当前cur的下一个
- RCU是怎么确定所有的CPU都不同时在RCU读锁里呢?经过的静止状态只是一个过去的状态而已.好像是这
  样的:一个CPU经过静止状态之后就等待所有的CPU经过所有的静止状态,等到所有的CPU都经过静止状态
  之后就可以把新的数据替换旧的数据.
- rdp->quiescbatch = rcp->cur 标识 quiesc period 的开始， rdp->qs_pending = 1， 标识一个
  quiesc period 尚未结束; rdp->passed_quiesc = 0 则是 cpu queisc 结束指示符， 当内核代码执
  行诸如抢占动作前， 会将这个变量置 1， 在后面的时钟中断处理中， 如果发现这个值为 1， 则将
  rdp->qs_pending 置 0 标识quiesc period 的结束， 并将 cpu 在 rcp->cpumask 中的位清除， 以
  表示本 cpu 同意 grace period 结束， 至于能否结束， 则要看其他的 cpu 是否同意。
  rdp->passed_quiesc = 0 还有一个作用是将 quiesc period 开始之前就 (多次) 置位的
  passed_quiesc 的动作取消。
- grace period 的开始代表的是 rcu 观察到至少一个 cpu 已经完成了至少一次数据更新操作，但可能
  善后工作还没有做， 例如上面说的释放旧数据； 而 grace period 的结束， 则代表者 rcu 认为所
  有的其他 cpu 都不再引用写者持有的旧数据了， 因此， 可以安全释放这份数据。 grace period 是
  上一轮 grace period 结束后， 第一次观察到一个 cpu 完成数据更新操作为开始的， 这时所有
  cpu 上一轮 quiesc period 都已经结束， 都处于停止状态。 grace period 一旦启动， 则后继的肯
  定是各个 cpu 各自的 quiesc period 的启动， 停止； grace period 结束于最后一个 quiesc
  period 的结束之后。 在 grace period 执行期间， 如果其他 cpu 上也出现了写者数据更新操作，
  则视这个 cpu 上的 quiesc period 启动与否而有不同行为， 如果没有启动， 则可以认为这个更新
  操作属于这个已启动的 grace period, 否则， 则将其归类于下一个 grace period。 但这个分类并
  不是 100% 正确， 会将本该分在这个 grace period 的结果分在了下一个 grace period, 这是 smp
  系统的缓存一致性决定的， 存在偶然因素， 但产生的结果不是不可接受的 ---- 即不会引起旧数据
  的提前释放， 只会引起其释放延迟， 这个在 rcu 目标应用场景中， 是可以接受的。
- 在退出所有读操作的那一瞬间， 那就不再引用了， 那么， 在那一瞬间之后呢， 会不会再次引用这
  些旧数据？ 不会， 因为它就算再读相同的数据， 读到的也不过是写者已经更新的新数据。 最终的
  结果就是， 只要发生了一次抢占， 则本 cpu 就会同意 grace period 的结束， 也就是说， 所有写
  者不用再担心运行在这个 cpu 上的代码引用它的旧数据。 那么当最后所有 cpu 都退出 quiesc
  period 的时候， 只会剩下写者自己持有旧数据， 它可以任意操作旧数据了。
- 写者修改数据前首先拷贝一个被修改元素的副本，然后在副本上进行修改，修改完毕后它向垃圾回收
  器注册一个回调函数以便在适当的时机执行真正的修改操作。等待适当时机的这一时期称为grace
  period，而CPU发生了上下文切换称为经历一个quiescent state，grace period就是所有CPU都经历一
  次quiescent state所需要的等待的时间。垃圾收集器就是在grace period之后调用写者注册的回调函
  数来完成真正的数据修改或数据释放操作的。https://www.ibm.com/developerworks/cn/linux/l-rcu/
- 因为 call_rcu_bh将把 softirq 的执行完毕也认为是一个 quiescent state，因此如果修改是通过
  call_rcu_bh 进行的，在进程上下文的读端临界区必须使用这一变种。
- 因为donelist一开始是赋初值的,所以就算在执行的过程中对链表进行增删改查也不用赋为NULL
- rcu_data.batch大于rcu_ctrlblk.completed时(就是rcu_data.batch == rcu_ctrlblk.completed+1)
- 如果nextlist里有数据且curlist里没有数据,那么当前CPU就会等待从当前时间开始的下一个完整的
  grace period,如果所等行待的grace period结束了(rcu_data.batch>=rcu_ctrlblk.completed)且
  rcu_data.curlist不为空,那么就会把curlist赋给donelist并在这个函数的最后调用rcu_do_batch()
  来处理所有的回调函数.
** static inline void rcu_qsctr_inc(int cpu)
*** include/linux/rcupdate.h:
- 当在CPU上发生进程切换时，函数rcu_qsctr_inc将被调用以标记该CPU已经经历了一个quiescent
  state。该函数也会被时钟中断触发调用。rcu_qsctr_inc函数的确在schedule()里被调用.
- 就是把per cpu变量rcu_data.passed_quiesc改为1.
- quiesc period 与 grace period 的关系， 就是 quiesc 从来都是在 grace period 开始之后开始，
  在 grace period 结束之前结束。
** void rcu_check_callbacks(int cpu, int user)
*** kernel/rcupdate.c:
- 时钟中断触发垃圾收集器运行，它会检查：否在该CPU上有需要处理的回调函数并且已经经过一个
  grace period；否没有需要处理的回调函数但有注册的回调函数；否该CPU已经完成回调函数的处理；
  否该CPU正在等待一个quiescent state的到来；如果以上四个条件只要有一个满足，它就调用函数
  rcu_check_callbacks。
- idle_cpu(cpu)说明第cpu正在执行idle进程.
- 检查CPU是否经历了一个quiescent state:1.当前进程运行在用户态;2.当前进程为idle且当前不处在
  运行softirq状态，也不处在运行IRQ处理函数的状态；
- 通过调用函数rcu_qsctr_inc标记该CPU的数据结构rcu_data和rcu_bh_data的标记字段passed_quiesc，
  以记录该CPU已经经历一个quiescent state。
- 函数rcu_check_callbacks将调用tasklet_schedule，它将调度为_该_CPU设置的tasklet rcu_tasklet，
  每一个CPU都有一个对应的rcu_tasklet.
- rcu_process_callbacks可能做以下事情：1． 开始一个新的grace period；这通过调用函数
  rcu_start_batch实现。2． 运行需要处理的回调函数；这通过调用函数rcu_do_batch实现。3． 检查
  该CPU是否经历一个quiescent state；这通过函数rcu_check_quiescent_state实现
** static void rcu_do_batch(struct rcu_data *rdp)
*** kernel/rcupdate.c:
- rcu_data.donelist才是被tasklet执行的回收rcu函数,不是rcu_data.curlist.
- 最终会在这个函数里调用回收函数.
- 只有在__rcu_process_callbacks()里检测到donelist不为空时才调用rcu_do_batch()
- 每调用里面的一个函数就是把一个旧数据释放掉.
- 里面的maxbatch是每次执行tasklet时调用的回调函数的个数,为什么要这个东西而不是一次调用所有
  的回调函数呢?
- 如果donelist为空就把&donelist赋给donetail,否则就用tasklet_schedule再调度这tasklet.
- 在这里,注意每次调用的回调函数有最大值限制.这样做主要是防止一次调用过多的回调函数而产生不
  必要系统负载.http://blog.chinaunix.net/uid-12260983-id-2952617.html
** static void rcu_check_quiescent_state(struct rcu_ctrlblk *rcp, struct rcu_state *rsp, struct rcu_data *rdp)
*** kernel/rcupdate.c:
- 调用函数rcu_check_quiescent_state检查该CPU是否经历了一个quiescent state,如果是并且是最后
  一个经历quiescent state的CPU，那么就结束grace period，并开始新的grace period。如果有完成
  的grace period，那么就调用rcu_do_batch运行所有需要处理的回调函数。
- rcu_data.quiescbatch(Batch # for grace period)不等于rcu_ctrlblk.cur(Current batch
  number)就说明当前CPU所在的quiesc period不是在全局的grace period里的,注释里说start new
  grace period,这个应该不正确应该是quiesc period。因为不等了，也说明了开始了一个全局的
  grace period之后，当前CPU还没经过一个quiesc period,所以就要开始一个quiesc period,而开始一
  个quiesc period就是要把rcu_data里的三个变量设置一下：qs_pending设为1（标识一个quiesc
  period 尚未结束,qs_pending只在这个函数被修改），passed_quiesc设为0（在经过quiesc period时设为1，schedule()调用
  rcu_qsctr_inc()转而设passed_quiesc为1，但和qs_pending有什么区别呢？）,quiescbatch设为rcu_ctrlblk.cur指定所开始的这个
  quiesc period是在第cur个grace period里的。
- batch这个词在这里的指第几批grace period.
- 若qs_pending为0，表明对于这个CPU来说已经经过了第quiescbatch个grace period了。
- qs_pending和passed_quiesc还是有一点不同的:qs_pending - 0, passed_quiesc - 0这种情况是不可
  能的,qs_pending - 0, passed_quiesc - 1:schedule()把passed_quiesc设为1了,之后也调用了这个
  函数,但在开始一个新的grace period之前就会出现这种情况.qs_pending - 1, passed_quiesc - 0:
  因发现又开始了一个新的grace period,所以当前CPU又要开始重新检查一个quiesc state,这种情况就
  是在这个函数一开始时设置的.qs_pending - 1, passed_quiesc - 1:开始一个quiesc后也执行了
  schedule()但还没有进行这个函数rcu_check_quiescent_state()
- 搞明白rcu_data和rcu_ctrlblk里面的成员就差不多了.
- 函数的功能就是若发现开始了一个新的grace period就开始一个新的quiesc period等待经过一个静止
  状态,否则看是否经过了静止状态,若经过了静止状态就清sq_pending.
- 对这个函数和里面调用的所有的函数层层解开之后功能就是：有可能开始一个新的quiesc period;有
  可能标识当前的CPU已经经过了静止状态；有可能因为当前CPU经过了静止状态后同时发现自已是最后
  一个经过静止状态的CPU而可能因为还有下一batch而开始一个新的grace period.
** static void cpu_quiet(int cpu, struct rcu_ctrlblk *rcp, struct rcu_state *rsp)
*** kernel/rcupdate.c:
- cpus_empty()参数全为空的时候就为真。所以rcu_state.cpumask为空的话，就说明所有的CPU都经过
  了当前第batch的grace period.当所有的CPU都经过静止状态时就把rcu_ctrlblk.completed给设为
  rcu_ctrlblk.cur，所以可以通过这两个是否相等判断是否正在等待下一个grace period.
- 这个函数主要是一个CPU经过静止状态的时候调用的（当经过一次静止状态时有被
  rcu_check_quiescent_state调用）。
** static void rcu_start_batch(struct rcu_ctrlblk *rcp, struct rcu_state *rsp,
*** kernel/rcupdate.c:
- 这个函数会根据传入的参数next_pending来修改rcu_ctrlblk.next_pending.在cpu_quiet()调用的
  rcu_start_batch()是以next_pending为0调用的，因为在cpu_quiet()调用rcu_start_batch()时一定
  是最后一个cpu经过了静止状态,就因为这个所以要以next_pending为0调用rcu_start_batch()吗?
- 如果rcu_ctrlblk.next_pending为1但是rcu_ctrlblk.completed不等于rcu_ctrlblk.cur时是不会改变
  rcu_ctrlblk.cur的,因为rcu_ctrlblk.completed不等于rcu_ctrlblk.cur说明当前批的grace period
  还没有结束,所以即使有下一批的回收挂起,也不能把改变cur而开始下一个grace period.如果
  rcu_ctrlblk.next_pending为0但是rcu_ctrlblk.completed等于rcu_ctrlblk.cur时也不能改变
  rcu_ctrlblk.cur,因为rcu_ctrlblk.completed等于rcu_ctrlblk.cur说明当前已经不在grace period
  里了,但因为next_pending为0即没有下一批需要回收的rcu,所以也不能设置rcu_ctrlblk.cur以开始一
  个新的grace period.开始下一个grace period(递加rcu_ctrlblk.cur)时前会清掉
  rcu_ctrlblk.next_pending,
- 到目前为止rcu_ctrlblk结构体里的cur,completed,next_pending成员都已经明白什么意思
  了,rcu_data结构体里的quiescbatch,passed_quiesc,qs_pending,donelist,donetail也明白什么意思
  了.
- 因为rcu_qsctr_inc()会在schedule()和rcu_check_callbacks()里调用,而且rcu_check_callbacks()
  又在定时器中断处理程序里调用，所以比较明了在哪里会去确定是否经过静止状态。
- 在next_pending不为假时才会开始一个grace period.
- 读者是可以嵌套的.也就是说rcu_read_lock()可以嵌套调用.
- The Linux kernel offers a number of RCU implementations, the first such implementation
  being called "Classic RCU". More material introducing RCU may be found in the
  Documentation/RCU directory in any recent Linux source tree, or at Paul McKenney's RCU
  page. Linux RCU的第一个实现
- 将rcp->next_pending置为1.设置这个变量主要是防止多个写者竞争的情况
- 如果CPU 1上有进程调用rcu_read_lock进入临界区,之后退出来,发生了进程切换,新进程又通过
  rcu_read­_lock进入临界区.由于RCU软中断中只判断一次上下文切换,因此,在调用回调函数的时候,仍
  然有进程处于RCU的读临界区,这样会不会有问题呢?只要使用被RCU保护数据的方法正确就不会有问题,引
  用时要用rcu_dereference()函数,rcu_dereference()函数里定义了一个新的局部变量来对保护指针的
  引用,所以一下进入临界区的时候就是最新的数据了如:
  #+BEGIN_EXAMPLE
  void foo_update_a(int new_a)//这个是更新数据
  {
  struct foo *new_fp;
  struct foo *old_fp;
  
  new_fp = kmalloc(sizeof(*new_fp), GFP_KERNEL);
  spin_lock(&foo_mutex);
  old_fp = gbl_foo;
  *new_fp = *old_fp;
  new_fp->a = new_a;
  rcu_assign_pointer(gbl_foo, new_fp);
  spin_unlock(&foo_mutex);
  synchronize_rcu();
  kfree(old_fp);
  }

  int foo_get_a(void)//这个是引用数据,不能使用gbl_foo直接引用.
  {
  int retval;
  
  rcu_read_lock();
  retval = rcu_dereference(gbl_foo)->a;
  rcu_read_unlock();
  return retval;
  } 
  #+END_EXAMPLE
** static inline void init_MUTEX (struct semaphore *sem)
*** include/asm-i386/semaphore.h:
- 就是调用sema_init这个函数.
** static inline void init_MUTEX_LOCKED (struct semaphore *sem)
*** include/asm-i386/semaphore.h:
- 就是调用sema_init这个函数.
** static inline void sema_init (struct semaphore *sem, int val)
*** include/asm-i386/semaphore.h:
- 初始化struct semaphore里的三个成员,count为参为val,sleepers(ULK:Stores a flag that
  indicates whether some processes are sleeping on the semaphore.)为0,wait(ULK:Stores the
  address of a wait queue list that includes all sleeping processes that are currently
  waiting for the resource.)用init_waitqueue_head初始化.
** #define DECLARE_MUTEX(name) __DECLARE_SEMAPHORE_GENERIC(name,1)
*** include/asm-i386/semaphore.h:
- 编译时声明
** static inline void up(struct semaphore * sem)
*** include/asm-i386/semaphore.h:
- 用汇编实现，两步：1.把计数器加1，2.计数器大于0就调用__up_wakeup().
** ".globl __up_wakeup\n"
*** arch/i386/kernel/semaphore.c:
- 这个函数全用汇编实现，就连函数名也是。
- 就是把__up()这个函数的参数压入栈后调用__up()，返回后再出栈。
** static fastcall void __attribute_used__  __up(struct semaphore *sem)
*** arch/i386/kernel/semaphore.c:
- 直接调用调用wake_up()，所以可以看出信号量是用等待队列实现的，可以得出它的性能如何。
** static fastcall void __attribute_used__ __sched __down(struct semaphore * sem)
*** arch/i386/kernel/semaphore.c:
- 这个函数与down()函数结合可以看出：如果进程A获取信号X后因为count为0而进入__down(),这时A成
  为了第一个等待信号A的进程，后来进程B也因为获取信号X进入休眠，但有可能进程B先获得信号量。
- 理解这个函数sem->count, sem->sleepers之间的关系的例子：当sleepers为1,count为-1时进入
  down()之后会把count自减1为-2，接着进入__down(),接着在__down()里会把sleepers自加为2，经过
  atomic_add_negative之后count就为-1，所以为-1并不表明只有一个进程在等待，接着又把sleepers
  改为1，所以这也表明sleepers为1也不说明只有一个睡眠进程,如果这时有一个进程up了，那么count
  会为0,这时因为sem->sleepers为1，所以atomic_add_negative()为假，接着把sem->sleepers改为0，
  这时只有一个进程在等待了，且sem->count为0,当这个等待的进程被唤醒之所再进入
  atomic_add_negative()时，因为sem->sleepers为0,sem->count为0所以atomic_add_negative()为真，
  所以会继续等待，且sem->count为-1了,接着sem->sleepers改为了1,再次执行
  atomic_add_negative()时还是为真。
- 其实信号量有使用等待队列里的锁来保护整个信号量结构体的访问。所以不会对struct semaphore有
  竞争访问，若有竞争访访问，会在一个地方有问题：若进程A是第一个等待某个信号量的进程，进程A
  在某一个时刻执行完atomic_add_negative之后在sem->sleepers=1之前可能被切换去出执行进程B，这
  时B又要调用down()获取这个信号量,down()会在一开始把sem->count自减1，接着进入__down()，在执
  行完sem->sleepers++之后被切换出去执行A，而这时sem->sleepers为2，但是恢复执行A之后会执行
  sem->sleepers=1这一句，所以当B再次恢复执行时就是执行int sleepers = sem->sleepers,而
  sem->sleepers已被改成了1而不是恢复执行前的2，所以在某个进程执行up()之前sem->count是-2，当
  某个进程真的执行up()时，按理说应该会唤醒一个进程，但是因为sem->count原来为-2所以最终也没
  有唤醒进程。所以必须有一个锁给以保护才可以,这个锁就是sem->wait->lock自旋锁了。
** static inline int down_trylock(struct semaphore * sem)
*** include/asm-i386/semaphore.h:
- 这个函数在一开始把sem->count减-1,如果为负数就进入__down_failed_trylock宏，转而直接调
  用__down_trylock()
** static fastcall int __attribute_used__ __down_trylock(struct semaphore * sem)
*** arch/i386/kernel/semaphore.c:
- 在一开始把sleepers赋予了sem->sleepers+1，接着再用atomic_add_negative()把sleepers加到
  sem->count里去。因为sleepers是从sem->sleepers加多了1，所以这个1把sem->count在
  down_trylock()里减的1给抵消了。
** static fastcall int __attribute_used__ __sched __down_interruptible(struct semaphore * sem)
*** arch/i386/kernel/semaphore.c:
- 这个函数与__down()类似，多了一个检查信号量的步骤（因为是interruptible的等待，可以被信号唤
  醒），还有一个地方不一样就是设置进程状态为TASK_INTERRUPTIBLE.
** static inline void init_rwsem(struct rw_semaphore *sem)
*** lib/rwsem-spinlock.c:
*** include/asm-i386/rwsem.h:
- linux通用的struct rw_semaphore和i386里的struct rw_semaphore是不同的。
- struct rw_semaphore里的wait_list是一个struct rwsem_waiter的链表，rwsem_waiter->flags表明
  了rwsem_waiter->task是读的还是写的。
- rw_semaphore->count的定义有点复杂：ULK:(Stores two 16-bit counters. The counter in the
  most significant word encodes in two's complement form the sum of the number of
  nonwaiting writers (either 0 or 1) and the number of waiting kernel control paths. The
  counter in the less significant word encodes the total number of nonwaiting readers and
  writers.)
** void fastcall __sched wait_for_completion(struct completion *x)
*** kernel/sched.c:
- ULK在complete一节一开始所举的例子是有会出现吗？up()是用汇编实现的，实现的时候是一开始就自
  加了sem->count,转而进入__up_wakeup(),这个函数在最后调用__up(),转而调用__wake_up()用
  sem->wait作为参数,在__wake_up()里先把sem->wait->lock给锁上之后就以非同步的方式调
  用__wake_up_common()，一定要用非同步的方式，因为现在持有自旋锁，__wake_up_common()退出之
  后就解sem->wait->lock锁，而ULK说的被抢占的情况只能发现在这个解锁之后，但解锁之后就是退
  到__up()再直接退到__up_wakeup()再直接退到up(),所以就算在解锁之后被等待信号量的进程抢占了
  之后把信号量结构体释放也不会有问题，因为在解锁之后不会再被调用。
- 与信号量的区别就是：信号量的sem->wait->lock不会完全保护sem->count的访问，因为在up()和
  down()的一开始在没有加锁的情况下就修改了sem->count,而wait_for_completion()和complete()是
  在加了锁之后才会访问completion->done.
- completion的等待都是用WQ_FLAG_EXCLUSIVE插入的，被插入的等待是放在队列尾的，是把进程的状态
  改为TASK_UNINTERRUPTIBLE的
- 如果在进入函数时发现done大于0时就不会进入休眠，这点与信号量是一样的。
- 信号量和completion都是用等待队列来实现同步的。
** unsigned long fastcall __sched wait_for_completion_timeout(struct completion *x, unsigned long timeout)
*** kernel/sched.c:
- 不同的是调用了schedule_timeout()而不是schedule(),在发现超时就会马上退出而不再获取。
- schedule_timeout()是与schedule()一样来处理切换的吗？而不会管时间来调度，就是说不会因为时
  间到了而调度某个进程，是在这某个进程被切回来执行之后而判断与所要求的切回时间有多大的差距。
** int fastcall __sched wait_for_completion_interruptible(struct completion *x)
*** kernel/sched.c:
- 这个的区别就是把进程的状态改成TASK_INTERRUPTIBLE，在改成这个状态之前和
  down_interruptible()一样也会检查是否还有未处理的信号。
** #define local_irq_disable() 	__asm__ __volatile__("cli": : :"memory")
*** include/asm-i386/system.h:
- i386就是用一个指令把本地的中断给禁止了，
** #define irqs_disabled()
*** include/asm-i386/system.h:
- 查看本地中断传递是否被禁止.ULK:yields the value one if the IF flag of the eflags
  register is clear, the value one if the flag is set.
** #define local_bh_disable()
*** include/linux/interrupt.h:
- 在preempt_count的软中断位置了加1，所以若要使能软中断就要使local_bh_disable()和
  local_bh_enable()的调用次数匹配。这点与local_irq_save()和local_irq_restore()不一样。
** #define time_after(a,b)
*** include/linux/jiffies.h:
- a的时间在b之后就为真。
- http://decimal.blog.51cto.com/1484476/410673 若b是固定的，那么无论b从哪里开始，只要开始时
  a等于b，且a是在递增的且递增的次数小于有符号整型的最大值，那么这个宏就是安全的，对于
  HZ=100，数据长度是32位的，2147483647/100秒 = 0.69年 = 248.5天才会出现不正确的判断。
- jiffies_64要溢出需要几百万年，这样就可以保存从开机到现在所经过的时间了。
** static inline u64 get_jiffies_64(void)
*** kernel/time.c:
*** include/linux/jiffies.h:
- 可以通过BITS_PER_LONG来判断。
- 若BITS_PER_LONG不是64位的，那么就要用顺序锁来读取jiffies_64了.
** struct timespec
*** include/linux/time.h:
- tv_sec成员是秒，从1970-7-1开始计算起。
- tv_nsec是纳秒
** void __init time_init(void)
*** arch/i386/kernel/time.c:
- 从cmos获取的值赋给了xtime.tv_sec
- xtime.tv_nsec设了一个特殊的值以在5min之后就会溢出。
- 设置单调时间，但是看不懂这个设置什么含义.
- 选择时间对象.
- 设置时间中断处理函数。
** irqreturn_t timer_interrupt(int irq, void *dev_id, struct pt_regs *regs)
*** arch/i386/kernel/time.c:
- 这个是时钟中断函数，注意它那执行了哪些函数。
- mark_offset()这个函数主要就是更新jiffies_64并记录相对应的
- ULK:a few timer interrupts can be lost, for instance when interrupts remain disabled for
  a long period of time; in other words, the kernel does not necessarily update the xtime
  variable at every tick. However, no tick is definitively lost, and in the long run,
  xtime stores the correct system time. The check for lost timer interrupts is done in the
  mark_offset method of cur_timer; 禁止中断会把产生的中断丢失，原来mark_offset真的是来防止
  时间中断丢失。这里有关如何使用tsc来防止中断丢失：http://book.51cto.com/art/200810/93782.htm
- 好像只有在调用这个函数的时候才会增加jiffies_64啊且在增加jiffies_64之后很快就会在下面所调
  用的update_times()函数里把wall_jiffies给补上，若是这样就只有在timer_interrupt()函数里才会
  出现jiffies_64比wall_jiffies大的情况，但是为什么在do_gettimeofday()会用jiffies减
  wall_jiffies呢？
- 接着调用do_timer_interrupt().
- 时钟中断：由系统定时硬件以周期性的间隔产生,hz：上述间隔由hz的值设定，hz是一个与体系结构相关的常数
- 全局变量xtime所维持的当前时间通常是供用户来检索和设置的，而其他内核模块通常很少使用它（其
  他内核模块用得最多的是jiffies），因此对xtime的更新并不是一项紧迫的任务，所以这一工作通常
  被延迟到时钟中断的底半部（bottom half）中来进行。由于bottom half的执行时间带有不确定性，
  因此为了记住内核上一次更新xtime是什么时候，Linux内核定义了一个类似于jiffies的全局变量
  wall_jiffies，来保存内核上一次更新xtime时的jiffies值。好像更新xtime不是在底半部做的，因为
  更新xtime.tv_nsec是在update_wall_time_one_tick()做的,而是在update_wall_time()被调用，
  update_wall_time_one_tick()更新xtime.tv_nsec，update_wall_time()更新xtime.tv_sec。
** static inline void do_timer_interrupt(int irq, void *dev_id, struct pt_regs *regs)
*** arch/i386/kernel/time.c:
- 只是调用了do_timer_interrupt_hook()
** static inline void do_timer_interrupt_hook(struct pt_regs *regs)
*** include/asm-i386/mach-default/do_timer.h:
- 连续调用了do_timer(),profile_tick(),update_process_times()
** void do_timer(struct pt_regs *regs)
*** kernel/timer.c:
- 为什么这里又要递增jiffies_64呢？不是已经在mark_offset里加了吗？好像mark_offset里的增加。
  这两个地方增加是不一样的，在mark_offset()里加的丢失的时钟中断的时间，而在这里加的是定时到
  来的时间。是jiffies_64是在中断丢失的情况下增加的。
- 调用了update_times
** static inline void update_times(void)
*** kernel/timer.c:
- 调用了update_wall_time()
- jiffies和wall_jiffies是什么关系呢？只有在这个函数wall_jiffies被修改，把它与jiffies的差值
  加到wall_jiffies,
- 调用calc_load()来更新均衡负载,所以更新均衡负载每个时钟周期都计算的.不是这样的要进入
  calc_load函数才知道,里面有一个循环定时器,经过了LOAD_FREQ个tick之后才会计算更新均衡负载.
** static void update_wall_time(unsigned long ticks)
*** kernel/timer.c:
- 调用ticks次update_wall_time_one_tick().和second_overflow()
- 为什么second_overflow()要在xtime.tv_sec增加之后才调用呢?
** static void update_wall_time_one_tick(void)
*** kernel/timer.c:
- 这个函数主要是用来更新xtime中的tv_nsec成员的。
- 这个函数的实现考虑到了NTP和adjtimex()系统调用。
- time_adjust是指要调整的时间，单位是微秒的（scale us）,它会在do_settimeofday()和
  do_adjtimex()被修改成0，在这个函数里会被修改成time_next_adjust,而time_next_adjust会在
  do_adjtimex()里被修改,所以do_timeadjx()调用之后不会马上改变时间，只是在时钟中断处理函数执
  行时才会改变,因为time_adjust_step是微秒的,所以要乘1000,又因为是在中断定时器时调用了这个函
  数所以要加上tick_nsec.
- 从代码来看就是把xtime.tv_nsec加上（减去）time_phase上的高几位。time_phase和time_adj是与
  second_overflow()有关的
** static void second_overflow(void)
*** kernel/timer.c:
- 用来处理微秒数成员溢出的情况。
** void update_process_times(int user_tick)
*** kernel/timer.c:
- 从do_timer_interrupt_hook()可以看出在多处理器的情况下才会调用这个函数.
- 在do_timer_interrupt_hook()调用这个函数时的参数使用了user_mode(reg)来判断reg是在用户还是
  在内核态以得出当前这个tick是在用户还是在内核态下发生的.但是这样的计算方法有点粗糙,因为在
  一个tick内一个用户进程可以先在用户态然后进程调用系统调用以进入内核态,接着完成系统调用并返
  回用户态执行.
- ULK:cutime and cstime are provided in the process descriptor to count the number of CPU
  ticks spent by the process children in User Mode and Kernel Mode, respectively.For
  reasons of efficiency, these fields are not updated by update_process_times( ) , but
  rather when the parent process queries the state of one of its children (see the section
  "Destroying Processes" in Chapter 3).
- cputime_add()这个宏只是将两个参数相加.
- 在这里调用rcu_check_callbacks()来更新rcu.
** void run_local_timers(void)
*** kernel/timer.c:
- 在update_process_times()里被调用来唤醒定时器中断处理的底半部软中断。
** void account_user_time(struct task_struct *p, cputime_t cputime)
*** kernel/sched.c:
- 认识一下struct cpu_usage_stat的成员：user:从系统启动开始累计到当前时刻，用户态的CPU时间，
 不包含 nice值为负进程。nice: 从系统启动开始累计到当前时刻，nice值为负的进程所占用的CPU时间。
 system: 从系统启动开始累计到当前时刻，核心时间.idle :从系统启动开始累计到当前时刻，除IO等
 待时间以外其它等待时间.  iowait 从系统启动开始累计到当前时刻，IO等待时间.irq:从系统启动开
 始累计到当前时刻，硬中断时间.softirq: 从系统启动开始累计到当前时刻，软中断时间./proc/stat
 文件里的第一行会显示这些信息。
- 在这里cputime一定加到进程描述符的utime里，进程的nice大于0时会把cputime加到cpustat->nice里
  否则加到cpustat->user里，可见utime的时间等于cpustat->nice加cpustat->user.为什么要这样做呢？
** void account_system_time(struct task_struct *p, int hardirq_offset, cputime_t cputime)
*** kernel/sched.c:
- 除了在account_user_time()里更新的cpustat成员，其它的成员在这个函数里更新。
- stime与cpustat里的成员是无关的。hardirq_count() - hardirq_offset的结果会不会忽略掉一个硬
  中断呢？
- stime等于cpustat->irq,cpustat->softirq,cpustat->system,cpustat->iowait,cpustat->idle之和。
  hardirq是优先级最高的，其次是softirq,在进程是idle的时候要分情况，不是idle进程就一定加计到
  cpustat->idle上，即使现在执行的是idle进程，但若有进程在等待io(runqueue->nr_iowait)，那就
  加计到cpustat->iowait而不是cpustat->idle。
** #define jiffies_to_cputime(__hz)
*** include/asm-generic/cputime.h:
- 用于将一个时钟中断转换为处理器时间
** static inline void init_timer(struct timer_list * timer)
*** include/linux/timer.h:
- 清base,设magic,初始化lock
- ULK:Indeed, a sleeping process may be woken up before the time-out is over; in this
  case, the process may choose to destroy the timer.
- ULK:Invoking del_timer( ) on a timer already removed from a list does no harm, so
  removing the timer within the timer function is considered a good practice.在定时器函数里
  用del_timer()来删除自身。
** int del_timer(struct timer_list *timer)
*** kernel/timer.c:
- ULK:del_timer()和del_timer_sync()之间的区别主要是后者用于多处理器中，以防止某个CPU在使用某个
  定时器结构时另一个CPU在调用del_timer()来删除它，而del_timer_sync()会等待。
- ULK:因为del_timer_sync()要考虑定时器自注册的可能所以慢又复杂，如果可以确定被删除的定时器
  没有自注册，那么就可以用del_singleshort_timer_sync().
- 因为每个CPU都有一个tvec_base_t的结构体，所以timer在哪个CPU插入就在哪个CPU执行。那如果被
  mod_timer()了呢？
- TVR应该是timer vector root的缩写,对应的是TVN。
- tv1 - 2的8次方-1的ticks, tv2 - 2的8+6次方-1ticks, tv3 - 2的8+6+6次方-1ticks, tv4 - 2的
  8+6+6+6次方-1ticks.
- 这些定时器是用软中断执行的,若软中断被禁止的时间长了也可能导致定时器的执行时间不精确。
- tv1,tv2等这些成员的类型是list_head的数组，所以这个数组的每一个成员就是一个特定的tick的定
  时器。但为什么还要分tv1,tv2,tv3这几组呢？
** static void run_timer_softirq(struct softirq_action *h)
*** kernel/timer.c:
- run_timer_softirq()这个函数是与TIMER_SOFTIRQ的执行函数.
- 这个函数里判断base->timer_jiffies小于jiffies就马上调用__run_timers().
** static inline void __run_timers(tvec_base_t *base)
*** kernel/timer.c:
- timer_jiffies开始是初始化为jiffies,接着就会在__run_timers()一直自加,因为timer_jiffies只是
  用来确定与jiffies的差值,所以不用管是否溢出,time_after_eq()这些函数会解决.
- 关于定时器这种级联设计是如何工作的:因为timer_jiffies是递增的,假设一开始是0,在大于255之前
  所有在tv1里的定时器被会被查一次即使没有定时器放在相应的数组中,当timer_jiffies的低8位因为
  溢出而全又变为0的时候,而第9位变为了1,这一位就是tv2的最低位,在这种情况下会调用cascade()把
  tv2里的所有定时器按照各自相应的定时时间放到tv1的255个元素里,放到第9位的定时器都有可能是
  255个里的任意一个,这时__run_timers()又可以在tv1找定时器来运行了,再等到tv1里的定时器又运行
  完之后,且因为timer_jiffies的第9位已为1,那么再进1就使得第10位为1而所有的位都为0了,所以又到
  了调用cascade()的时候了,就这样以此类推.用这种算法也不用担心在任何时候在任何地方插入定时器
  都不会有问题,因为这不是先进先出又不是先进后出.
- 如果要把一个新的定时器插入,是不是被插的地方要这样计算呢:用当前CPU的timer_jiffies提出从新
  定时器设定的expires为1的最高一位开始的所有低位值加上新的定时器设定的expires值就是被插入的
- 应该不是每个HZ的时间就执行一次__run_timer(),就是时钟中断是可能丢失的,而__run_timer()是在
  时钟中断产生后才会有一次执行的机会.jiffies可能会一次时钟中断加多个数而timer_jiffies是自加
  的.这样的话会使得所有的定时器的执行都延时.而且很多时候都是禁止中断的.
- 进入这个函数一般就只会把tv1里的某一个下标的所有定时器执行完,但如果执行的过程中又产生了时
  钟中断那么就再执行一次.
- 从这个函数可以看出是如何解决jiffies一次加上大于1的值的,因为这个函数里有一个while循环比较
  jiffies与timer_jiffies而且在while里会一直自加timer_jiffies,所以到最后timer_jiffies还是会
  追上jiffies的且也会把因jiffies跳增而遗下的定时器处理完.
** static void internal_add_timer(tvec_base_t *base, struct timer_list *timer)
*** kernel/timer.c:
- 这个函数就是根据定时器的expires和当前的timer_jiffies来确定把指定的定时器插入到相应的tv的
  下标里.
- 创建新的定时器时是用jiffies和需要延时的时间来计算expires的,但是在这个函数里的idx的计算却
  是expires减去timer_jiffies,是不是有点不对呢?因为timer_jiffies会丢失一些是钟.
- 得出的idx就是其实只是一个时间差,而不是下标,通过这个值判断是哪个区间以确定是在
  tv1/tv2/tv3/tv4/tv5,确定这个之后再用expries(不是idx)来确定放到哪个下标去.
- 定时器插入是FIFO形式的.
- 从这个函数可以看出tv1的组织方式是和tv2/tv3/tv4/tv5很不一样的,tv1里的任何一个元素都对应着
  一个特定的时间到期的定时器,而其它的tv的一个元素指定的是一个范围时间到期的定时器,如tv2的第
  2个元素所存放的定时器应该是在2的10次方减去2的9次方时间段内到期的定时器.
** static int cascade(tvec_base_t *base, tvec_t *tv, int index)
*** kernel/timer.c:
- 这个函数的主要功能就是把tv里的下标为index的所有链表结点传给internal_add_timer()来放到相应的位置.
** static void __devinit init_timers_cpu(int cpu)
*** kernel/timer.c:
- timer_jiffies在这里被初始化为jiffies.
- 在极端的条件下，同时会有多个 TV 需要进行 cascade 处理，会产生很大的时延。这也是为什么说
  timeout 类型的定时器是 timer wheel 的主要应用环境，或者说 timer wheel 是为 timeout 类型的
  定时器优化的。因为 timeout 类型的定时器的应用场景多是错误条件的检测，这类错误发生的机率很
  小，通常不到超时就被删除了，因此不会产生 cascade 的开销。另一方面，由于 timer wheel 是建
  立在 HZ 的基础上的，因此其计时精度无法进一步提高。毕竟一味的通过提高 HZ 值来提高计时精度
  并无意义，结果只能是产生大量的定时中断，增加额外的系统开销。因此，有必要将高精度的 timer
  与低精度的 timer 分开，这样既可以确保低精度的 timeout 类型的定时器应用，也便于高精度的
  timer 类型定时器的应用。还有一个重要的因素是 timer wheel 的实现与 jiffies 的耦合性太强，
  非常不便于扩展。因此，自从 2.6.16 开始，一个新的 timer 子系统 hrtimer 被加入到内核中。
- 内核使用全局变量 xtime 来记录这一信息，这就是通常所说的“Wall Time”或者“Real Time”。与
  此对应的是“System Time”。System Time 是一个单调递增的时间，每次系统启动时从 0 开始计时。
- 每个CPU都必须定义两个时钟源：REAL和MONOTONIC。REAL代表实时时钟，MONOTONIC代表单调递增时钟。
  两者的区别在于，当用户更改计算机时间时，REAL时钟会收到影响，但MONOTONIC不受影响。
- wall_to_monotonic 没有被EXPORT_SYMBOL,xtime被EXPORT_SYMBOL
** asmlinkage long sys_nanosleep(struct timespec __user *rqtp, struct timespec __user *rmtp)
*** kernel/timer.c:
- ULK:To be on the safe side, sys_nanosleep( ) adds one tick to the value computed by
  timespec_to_jiffies( ).加上一个tick来作补尝.
- 用TASK_INTERRUPTIBLE休眠,用schedule_timeout()
- 如果没有到时就被退出的话,那么就用当前进程的thread_info->restart_block来重新启动,这个的启
  去原理是什么呢?
- ULK:sys_nanosleep( ) system call. If the value returned by schedule_timeout( ) specifies
  that the process time-out is expired (value zero),the system call terminates. Otherwise,
  the system call is automatically restarted. 所以要想系统调用自动重启就把它放到
  restart_block里。
** fastcall signed long __sched schedule_timeout(signed long timeout)
*** kernel/timer.c:
- 原来是用定时器来实现进程切换的,定时器函数是调用wake_up_process()实现把进程切回来的,这样的
  切回时间会不会不准确呢?
- 可以指定唤醒某个进程但是不能指定切换到某个进程。
** #define udelay(n)
- the built-in function __builtin_constant_p to determine if a value is known to be
  constant at compile-time and hence that GCC can perform constant-folding on expressions
  involving that value.
- i386和arm的实现很不一样，因为i386是有时钟对象一说的，而arm没有。
** void do_gettimeofday(struct timeval *tv)
*** arch/i386/kernel/time.c:
- 这个也有在kernel/time.c里实现，这时是使用time_interpolotor的，但一般的架构都有自已的实现。
- 因为用到了cur_timer->get_offset()，所以精度比jiffies高。
- 这个函数主要做的处理是usec的，对于usec除了从xtime.tv_nsec获取之外还要考虑比jiffies更高精
  度的cur_timer,要考虑NTP的调整；sec就只是从xtime里获取。
- 因为jiffies/wall_jiffies -> lost -> xtime.tv_nsec,所以wall_jiffies的时间与xtime同步的而不
  是jiffies.在update_times()可以看出xtime与wall_jiffies.
** int do_setitimer(int which, struct itimerval *value, struct itimerval *ovalue)
*** kernel/itimer.c:
- ULK:interval timers The timerscause Unix signals (see Chapter 11) to be sent
  periodically to the process. It is also possible to activate an interval timer so that
  it sends just one signal after a specified delay.
- setitimer()的时间精度比alarm()和signal()的高。
- 不知道ovalue什么用。保存旧的timer值？
- setitimer()不是C的标准库，只是Linux的API。
- ITIMER_REAL的which收到SIGALRM,如果task_struct->signal->real_timer定时器已在等待，那么就要
  把它删除重新插入一个新的。ovalue不为0时，会把上一次的task_struct->signal->it_real_incr和
  task_struct->signal->it_real_value保存在ovalue里.
- ITIMER_VIRTUAL是用户态下的时间，收到
- task_struct->signal->real_timer这个timer_list结构体变量原来是给setitimer()和getitimer()使
  用的
** static unsigned long it_real_value(struct signal_struct *sig)
*** kernel/itimer.c:
- timer_pending()这个方法是通过timer_list->base是否为空来判断。
- 因为timer_list->expires是一个绝对时间，所以要减去jiffies。如果差值不为正就要为1，这是为什
  么呢？只有定时器pending的时候才会返回0。
** static inline void it_real_arm(struct task_struct *p, unsigned long interval)
*** include/linux/time.h:
- 这个函数的主要功能就是把task_struct->signal->real_time定时器装上。
- 把interval赋给task_struct->signal->it_real_value有什么用呢？但是interval经过调整之后才把
  它赋给signal->real_timer.expries,这个expires还要加1为了防止interval过小
** void it_real_fn(unsigned long __data)
*** kernel/itimer.c:
- 这个函数就只是在copy_singal()里赋给signal->real_timer.function的，因为这个函数调用
  it_real_arm(),且ti_real_arm()在interval不为0的时候又会把real_timer定时器装上，所以如果
  interval不为0就会一直按照interval间隔的时间产生信号.
** asmlinkage unsigned long sys_alarm(unsigned int seconds)
*** kernel/timer.c:
- 因为sys_alarm()是以ITIMER_REAL的方式调用do_setitimer()的,所以alarm()和setitimer()不能一起
  用.
** void scheduler_tick(void)
*** kernel/sched.c:
- 这个函数是在中断处理程序中调用的,所以这个函数内部是不会调用schedule()的.
- 一定要在调用update_cpu_clock()之后才更新rq->timestamp_last_tick,因为update_cpu_clock()里
  用到了rq->timestamp_last_tick.
- 若rq没有进程可执行了（正在执行idle），那么就要rebalance_tick()来移进程。rebalance之后就不
  执行后面的内容了。
- 在产生时钟中断的时候，有可能进程已不在rq->active里了,但是不是说这时TIF_NEED_RESCHED一定是
  没有设置的吗？什么情况下会出现进程正在运行而进程又不在rq->active里且TIF_NEED_RESCHED也没
  有设置。首先出现进程正在运行而进程又不在rq->active里就不知道怎么解释了,有一种情况就是进程
  A把自已从active里移出，在调用set_tsk_need_resched()的时候产生了时钟中断，这时就会出现这种
  情况，出不出现这种情况就要看会在哪里有把进程从active里移出,移出进程主要是调用
  dequeue_task(),dequeue_task()被
  deactivate_task(),pull_task(),scheduler_tick()schedule(),set_user_nice(),sys_sched_yield()
  调用，deactivate_task()又被
  schedule(),sched_setscheduler(),__migrate_task(),migration_call(),normalize_rt_tasks()调
  用,pull_task()被move_tasks()调用。
- 若current是一个实时的SCHED_RR进程就会算算时间片再判断是否调度，而SCHED_FIFO就不管时间片继续执行。
- 好像只有在这个函数里才会把first_time_slice设为0
- 在进程时间片用完的时候且expired_timestamp(Insertion time of the eldest process in the
  expired lists)为0就把expired_timestamp设为jiffies,expired_timestamp在nr_active为0时会设为
  0，也会在nr_running为0时设为0.但是为什么要在expired_timestamp为0时(就是还没有进程插入
  rq->expired)且在产生时钟中断时把expired_timestamp设为jiffies呢?这里还有一个前提条件就是进
  程的时间片已消耗完,但是消耗完了并一定会把它移到rq->expired里去的,但是在expired_timestamp
  为0时也要设置,不为0就不用,为什么要这样子呢?
- !TASK_INTERACTIVE(p) || EXPIRED_STARVING(rq) 的意思是若进程不是一个交互进程就插入
  rq->expired,若进程是一个交互进程但过时进程已经很久没有执行了也把进程插入rq->expired,否则
  就插入rq->active.
- 调用这个函数就一定会调用rebalance_tick(),有NOT_IDLE和SCHED_IDLE区别。
- 若进程的时间片还没有完，那么就要看看这个进程的时间片是否超过了允许的时间，若是就要把它分
  小片，分小片的方法就是把进程又重新插入到rq->active的尾部并设置需要调度，而不是减小它的时
  间片，这样的结果就是让rq->active的其它进程有运行的机会而不会使rq->expired的进程有积极或消
  极的影响。
** unsigned long long sched_clock(void)
*** arch/i386/kernel/timers/timer_tsc.c:
- 这个函数返回的时钟精度至少不比jiffies差.
** static inline void update_cpu_clock(task_t *p, runqueue_t *rq, unsigned long long now)
*** kernel/sched.c:
- 用现在的时间减去p->timestamp和rq->timestamp_last_tick的最大值累加到p->sched_time.
- p->timestamp和rq->timestamp_last_tick那是绝对的时间.
- p->timestamp的注释是:Time of last insertion of the process in the runqueue, or time of
  last process switch involving the process.p->timestamp是有可能大于
  rq->timestamp_last_tick的,因为进程切换不是在时钟中断里进程切换的,且
  rq->timestamp_last_tick只有在时钟中断里被调用的scheduler_tick()里更新为sched_clock()。
- 如果某个进程访问自已timestamp时就一定是切到这个进程执行时的时间,不会是Time of last
  insertion of the process in the runqueue。
- 这个函数在只在scheduler_tick()和schedule()里调用，schedule()里调用时是作用于被切换的进程,所
  以不管某个进程执行的过程中是否有产生过时钟中断都可以正确地把进程的执行时间计到
  p->sched_time里。p->sched_time这个时间不是与当前时间同步的。p->sched_time表示的是进程一共
  执行了多长的时间。而signal->sched_time表示的是已死去的线程组中线程的执行时间。
** static inline int wake_priority_sleeper(runqueue_t *rq)
*** kernel/sched.c:
- 这个函数只被scheduler_tick()在current是idle的时候调用
- 支持超线程的时候若发现rq->nr_running还有进程调进程调度，为什么要在支持超线程的时候才判断
  rq->nr_running呢？若不支持超线程这个函数就是没有任何作用的。
- 若正在执行idle就说明了rq没有可运行的进程了，但当前CPU可能还有正在休眠的进程，放到rq里的进
  程不包括在休眠的进程。
** static int try_to_wake_up(task_t * p, unsigned int state, int sync)
*** kernel/sched.c:
- p->state的状态要包含在state里才会往下执行。
- 当p->array不为空时就说明进程已是在运行状态，不管是在active还是在expires链表中。
- 这个函数主要是在让被唤醒的进程在哪个CPU运行的选择上做了很多工作，就是调度域的问题。
- 一般情况下,被唤醒的进程是在进程描述符指定的那个CPU上执行的.进程只会在进程描述符指定的CPU
  和当前CPU上迁移.
- 如果当前的CPU的负载很高而进程描述符指定的CPU的负载很低,那么就不要移了.
- 文档sched-domain.txt:A sched domain's span means "balance process load among these
  CPUs".base scheduling domain就是一个物理CPU的scheduling domain,CPU i一定被包含在CPU i的
  base scheduling domain里.每个scheduling domain会span几个CPU.Each scheduling domain must
  have one or more CPU groups (struct sched_group).每个scheduling domain有一个或多个组.The
  union of cpumasks of these groups MUST be the same as the domain's span.某个scheduling
  domain内的所有组的cpumasks的和一定是这个scheduling domain所包含的CPU.The intersection of
  cpumasks from any two of these groups MUST be the empty set.Groups may be shared among
  CPUs as they contain read only data after they have been set up.平衡是在组与组之间的进行
  的,那么在不同scheduling domain之间移也是通过组来进行的吗?Balancing within a sched domain
  occurs between groups.组的负载是组中所有CPU负载的总和,组负载超时就移,那么如果组中的一个
  CPU很大负载而其它负载很小呢?我觉得有这种情况的组应该还有子组.The load of a group is
  defined as the sum of the load of each of its member CPUs, and only when the load of a
  group becomes out of balance are tasks moved between groups.
- At the hyperthreaded processor level: balancing attempts can happen often (every 1-2ms),
  even when the imbalance between processors is small. There is no cache affinity at all:
  since hyperthreaded processors share cache, there is no cost to moving a process from
  one to another. Domains at this level are also marked as sharing CPU power; we'll see
  how that information is used shortly.超线程一级里的CPU是会每1-2ms就均衡一次就算差别很小,
  之间没有亲和之说因为CPU之间是共享cache的.
- At the physical processor level: balancing attempts do not have to happen quite so
  often, and they are curtailed fairly sharply if the system as a whole is busy. Processor
  loads must be somewhat farther out of balance before processes will be moved within the
  domain. Processes lose their cache affinity after a few milliseconds.
- balancing attempts are made relatively rarely, and cache affinity lasts longer. The cost
  of moving a process between NUMA nodes is relatively high, and the policy reflects that.
- SD_WAKE_IDLE:when a sleeping process is about to be awakened, the normal behavior would
  be to keep it on the same processor it was using before, on the theory that there might
  still be some useful cache information there. If that processor's scheduling domain has
  the SD_WAKE_IDLE flag set, however, the scheduler will look for an idle processor within
  the domain and move the process immediately if one is found. This flag is used at the
  hyperthreading level; since the cost of moving processes is insignificant, there is no
  point in leaving a processor idle when a process wants to run.调度域有SD_WAKE_IDLE标志说
  明会把该调度域的进程移到IDLE CPU上去.用于hyperthreading level.
- When a process calls exec() to run a new program, its current cache affinity is lost. At
  that point, it may make sense to move it elsewhere. So the scheduler works its way up
  the domain hierarchy looking for the highest domain which has the SD_BALANCE_EXEC flag
  set. The process will then be shifted over to the CPU within that domain with the lowest
  load. Similar decisions are made when a process forks.调度域设置SD_BALANCE_EXEC时,若执行
  了exec()那么就移到其它CPU,因为cache肯定丢失.
- If a processor becomes idle, and its domain has the SD_BALANCE_NEWIDLE flag set, the
  scheduler will go looking for processes to move over from a busy processor within the
  domain. A NUMA system might set this flag within NUMA nodes, but not at the top level.这
  个与SD_WAKE_IDLE是不一样的,SD_WAKE_IDLE是进程被唤醒后发现有IDLE的CPU说移过去,而
  SD_BALANCE_NEWIDLE是某个CPU发现自已IDLE了就从其它的CPU上移进程过来.
- Every scheduling domain has an interval which describes how often balancing efforts
  should be made; if the system tends to stay in balance, that interval will be allowed to
  grow. The scheduler "rebalance tick" function runs out of the clock interrupt handler;
  it works its way up the domain hierarchy and checks each one to see if the time has come
  to balance things out. If so, it looks at the load within each CPU group in the domain;
  if the loads differ by too much, the scheduler will try to move processes from the
  busiest group in the domain to the most idle group. In doing so, it will take into
  account factors like the cache affinity time for the domain.
- 调度域有四种:根据不同的硬件结构自上而下有NUMA、物理、核和超线程.一个NUMA可以有多个物理
  CPU组成,一个物理CUP有多个核组成,一个核可以有多个逻辑CPU.
- 系统中的每一个逻辑CPU都会关联一组调度域，相同类型的逻辑CPU处于同一个调度域中。
  http://blog.chinaunix.net/uid-7295895-id-2941412.html
- 系统中存在的负载有：中断、异常、软中断、系统调用、任务；系统调用是在任务的上下文中执行的，
  异常不可控制且一般也是由于任务触发可以忽略，只剩下中断、软中断和任务；有些中断可以通过
  CPU的中断亲和掩码在多个CPU上做均衡(X86架构中已有此实现，CONFIG_IRQBALANCE)，软中断过高的
  系统可以考虑软中断线程化后算成任务或RPS补丁(CONFIG_NETDEVICES_RPS)将软中断负载均衡至多个
  CPU间；标准内核的负载均衡算法是针对任务的。
- 历史权重目的是用来平滑静态权重，以缓解负载均衡时系统性能发生抖动。
- 系统运行过程中，内核会不停地进入负载均衡流程对当前cpu遍历其所关联的所有调度域（自下向上）；
  对每一个调度域遍历所属的所有调度组，找出一个负载最大的调度组（不能是当前cpu所属的调度组）；
  遍历此调度组中所属的cpu，找出静态权重符合条件的目标cpu，最后将此cpu上的可运行任务迁移至当
  前cpu（称为拉任务），以期达到负载均衡状态。整个过程可抽象为如下步骤：负载均衡算法会将静态
  权重load_rq、调度域参数、历史权重cpu_load[i]等信息，按照一定的规则计算出一个动态权重load
- 有了动态权重后，就可以用来判别系统负载是否均衡了：记当前cpu动态权重值为load_current，目标
  cpu动态权重值为load_target，判别规则类似如下：imbalance_pct *load_current <
  100*load_target(try_to_wake_up()函数有用到这个判断)其中，imbalance_pct为当前cpu的某个调度
  域参数，例如物理和核域为125；判别结果为真表示目标cpu负载过重，就会从目标cpu迁移任务至当前
  cpu。
- 算法均衡的时机共有定时均衡、闲时均衡、唤醒均衡和创建时均衡四种。时钟中断时，会根据条件触
  发均衡软中断（内核为其设置了专门的软中断），相应均衡函数为load_balance()，此函数只在调度
  组中的第一个cpu或第一个idle状态cpu上作均衡；此称为定时均衡（又可分为定时忙均衡和定时闲均
  衡）。任务调度时，会检查当前cpu是否空闲，如果空闲则进行负载均衡，相应均衡函数为
  idle_balance()，与定时均衡相比，只要拉到任务就会返回属于轻量级均衡算法(但实时系统中，此为
  主要均衡时机，因为发生频率较高)；此称为闲时均衡（又称NEW_IDLE均衡）。唤醒任务时，会在
  try_to_wake_up()中根据当前cpu的调度域参数决定是否进行负载均衡；此称为唤醒均衡（唤醒均衡倾
  向于将被唤醒任务迁移至当前cpu所在的调度域中的某个cpu上，如果系统中存在过于频繁的唤醒，此
  操作在某些NUMA系统中反而会造成负载不均衡）。创建任务时，会在sched_fork()/sched_exec()中，
  将此任务迁移至一个相对空闲的CPU上运行。称此为创建时均衡(包括fork和execve)。
- 任务迁移包括拉任务(一次操作多个任务)和推任务(一次操作一个任务)。拉任务指的是从目的CPU迁移
  任务至当前CPU，对应实现为pull_task()；定时均衡、闲时均衡和唤醒均衡对应的是拉任务。推任务
  指的是从当前CPU迁移任务至目的CPU，内核在每个运行队列中存放一个内核线程负责处理推任务请求；
  定时均衡多次失败时，置推任务标志，唤醒相应内核线程；创建时均衡(sched_exec)，发推任务请求，
  唤醒相应内核线程；设置任务的cpu亲和掩码时，发推任务请求，唤醒相应内核线程。实际负载均衡过
  程中，主要为拉任务操作。
- 任务迁移也要符合一定的规则，不同优先级按照由高到低、同种优先级按照LIFO的顺序进行选择任务。
  但对如下几种类型的任务不能被迁移：1) 任务的cpu亲和（见1.9）不允许被迁移到当前cpu；2) 为目
  标cpu上的当前运行任务；3) 任务具有cache亲和性(参考try_to_wake_up)；4) 任务被迁移到当前
  cpu上会产生新的失衡。但如果任务迁移到当前cpu后优先级为最高，这时就考虑让优先级高的任务先
  运行；但如果要迁移的是目标cpu上的最高优先级任务，且最高优先级任务只有一个，也不允许迁移。
- 每一个任务都会关联一个cpu亲和属性，该属性值的每一位表示此任务运行及迁移所允许的cpu集合
  （置位表示允许），cpu亲和影响均衡操作中的任务迁移（见1.8）。cpu亲和的设置可通过线程绑定和
  排它绑定来实现（见2.2和2.3）。
- sched_domain->span:表示调度域包含的CPU集合.sched_domain->last_balance记录了上次做定时均衡
  的时间点，单位为tick，只在定时均衡中更新，和balance_interval一起用于计算下次做定时均衡的
  时间；sched_domain->balance_interval也只在定时均衡中更新，单位为毫秒，用于计算下次做定时
  均衡的时间：last_balance+msecs_to_jiffies(balance_interval)，最终根据情况将结果更新记录于
  运行队列rq->next_balance中；定时均衡成功迁移任务后，意味着此时系统的负载倾向于不均衡，将
  balance_interval置为min_interval；定时均衡没有发生任务迁移，意味着此时系统的负载倾向于均
  衡，将balance_interval加倍，但不超过max_interval(如果当前CPU的任务都已绑定，则不超过
  MAX_PINNED_INTERVAL，定义为512ms)；sched_domain->busy_factor只用于定时均衡，如果当前CPU不
  是空闲的(定时忙平衡)，就将变量interval间隔乘上此字段，这样在当前CPU忙时不会太频繁进入负载
  均衡算法，避免负载均衡算法本身带来较大的开销。sched_domain->imbalance_pct用作均衡判别，见
  1.5节；此值增大可放宽负载失衡的判别条件，从而降低负载均衡的频率；
  sched_domain->cache_nice_tries，sched_domain->nr_balance_failed用于定时均衡中连续迁移任务
  失败后，发起推任务操作，判断条件为nr_balance_failed > (cache_nice_tries+2)；busy_idx,
  idle_idx, newidle_idx, wake_idx, forkexec_idx用作下标选择运行队列中的历史权重cpu_load[]数
  组中的元素
- SD_LOAD_BALANCE表示做负载均衡；做负载均衡就会做定时均衡，定时均衡不能单独关闭；
- SD_BALANCE_NEWIDLE表示做闲时均衡；
- SD_BALANCE_EXEC表示做创建时均衡中的execve均衡；
- SD_BLANCE_FORK表示做创建时均衡中的fork均衡；
- SD_WAKE_IDLE表示开启超线程后在try_to_wake_up()中将被唤醒任务迁移至一个空闲CPU的上运行(如
  果有的话)；
- SD_WAKE_AFFINE表示在try_to_wake_up()中考虑任务的cache亲和，如果存在cache亲和就不将此任务
  拉至当前CPU；
- SD_WAKE_BALANCE功能类似于SD_WAKE_AFFINE，只是算法判别准则不同；
- SD_SHARE_CPUPOWER表示共享CPU的处理能力，主要用于同一个核的超线程CPU间；从而让操作系统感知
  到超线程硬件特性予以特殊处理；
- SD_SHARE_PKG_RESOURCES用于初始化调度组的CPU能力；
- SD_SERIALIZE主要用于不同CPU间的负载均衡算法的串行化(增加了一把自旋锁)，一般NUMA域可设置此
  选项；
- BALANCE_FOR_MC_POWER和BALANCE_FOR_PKG_POWER用于节能，负载均衡可以和CPU的变频技术以及S5节
  能技术结合起来，用以将空闲的CPU降频或休眠，
- 任务的亲和性计算是指在被唤醒时选择一个可运行的CPU，该CPU最好保存有任务以前的cache内容。任
  务的亲和性计算在try_to_wake_up中进行，计算内容是在任务所属CPU和执行唤醒操作的本地CPU之间
  选择一个运行，选择的依据是CPU的cache内容是否失效和CPU的负载情况。Linux 2.6最初的亲和性计
  算是为每个调度域设一个cache失效的时间值，当被唤醒任务的睡眠时间超过失效时间时就认为任务与
  CPU之间已失去亲和性。这种算法的缺点是无法准确估计cache是否失效，因为如果一个时间片很长的
  任务一直在运行那么cache失效的可能性就很小。当前Linux 2.6的亲和性计算是根据CPU的任务切换频
  率来估计cache的失效可能性，如果有很多时间片很短的任务在CPU上运行，那么cache失效的可能性就
  很大。
- blog.chinaunix.net/uid-7295895-id-2941412.html有关于任务亲和性的计算过程
- 如果目的CPU和任务之间具有亲和性，即cond1和cond2条件同时为假，那么应该选择目的CPU。这时目
  的CPU上cache失效的可能性小且任务加在本地CPU上以后会造成负载失衡。对于任务加在本地CPU上造
  成负载失衡的原因，可以分为三种情况：i.  本地CPU负载小于目的CPU，且处于失衡状态。但任务权
  重很大，在本地CPU加上任务以后打破了失衡，使本地CPU负载反大于目的CPU而产生新的失衡；但如果
  将任务加载目的CPU上反而会扩大失衡。ii.  本地CPU负载接近目的CPU，处于均衡状态。本地CPU权重
  加上任务权重以后造成负载的失衡。iii.  本地CPU负载大于目的CPU，且处于失衡状态。这时本地
  CPU加上任务以后使失衡更加扩大。对于情况a)鼓励将任务加在本地CPU上；对于情况b)在考虑cache有
  效性的因素下鼓励任务加在目的CPU上；对于情况c)鼓励任务加在目的CPU上。因此在调度域设置允许
  的情况下需要对本地CPU和目的CPU的负载进行判断，判断的条件如下：imbalance*this_load <=
  100*load等价于(imbalance/100)*this_load <= load其中this_load是本地CPU的权重值；imbalance
  是负载平衡系数；load是目的CPU的权重值。(imbalance/100)恒大于1，因此可以看出，如果条件成立
  那么目的CPU与本地CPU的权重值之比超过负载平衡系数，而产生负载失衡。如果以上的条件成立则出
  现了情况a)，这时应选择本地CPU；否则应选择目的CPU。
- 然而实时性系统多为实时任务，实时任务的权重值固定为177522（见1.2节），再加上某些业务过高的
  网络软中断的影响，导致内核原有的亲和算法不能很好地发挥作用，反而会起到副作用。1. 在电信级
  业务的场景中，NUMA域默认参数存在两个问题。a)产品会伴随着大量的网络收发包，进而产生大量的
  网口中断和软中断，频繁的网络软中断就会产生大量的唤醒任务操作，导致唤醒均衡算法不停地将任
  务拉至中断所在的NUMA节点的CPU上；b)同时很多业务对系统中每一个CPU的利用率比较敏感，要求各
  个CPU利用率严格均衡。所以需要更改NUMA域的默认配置参数来解决这两个问题：第1个需要去掉NUMA
  域的唤醒均衡；第2个需要配上NUMA域的闲时均衡。2. 超线程域默认参数配有SD_WAKE_AFFINE算法。
  此参数在某些业务场景会导致系统性能发生抖动，进而影响系统性能；但与SD_WAKE_BALANCE算法相比
  影响相对较小，在出现开启硬件超线程特性后如果出现了性能抖动，就可以考虑去除此参数。
- 2.6.12的内核好像只是把运行队列里的进程数乘SCHED_LOAD_SCALE就用负载了，与进程优先级没有关
  系。
- source_load()取rq->cpu_load与当前负载的最小值，target_load()取rq->cpu_load与当前负载的最
  大值，值越大进程数越多。
- 同步唤醒就是唤醒后不马上执行。这种情况下把本地CPU的负载减小被唤醒进程的负载量,就是说用假
  设被唤醒进程加到当前CPU后的负载量所算出的当前的本地CPU的负载量来进行以后的计算。
- 若task->cpu的负载小于SCHED_LOAD_SCALE/2且本地CPU的负载大于SCHED_LOAD_SCALE/2就说明
  task->cpu的负载小而本地CPU的负载大。
- task->last_ran只在schedule()与task->timestamp赋相同的值。
- task_hot()用rq->timestamp_last_tick(运行队列的上一次切换时间)减去被唤醒进程在rq里被切换出
  去的时间所得到的时间差与调度域的cache_hot_time比，若后者大就说明进程的cache亲和力不大了，
  可以拉到本地CPU上运行。在做这个判断之前要先确定SD_WAKE_AFFINE已经设置。
- 用imbalance*this_load <= 100*load来判断不均衡度是否超过了所要求的，在这判断之前要确定是否
  设置了SD_WAKE_BALANCE.
- 调用wake_idle()处理设置了SD_WAKE_IDLE标志的情况。
- 进程要移出task->cpu时，就要解原rq的锁和给新的rq加上锁，这里有个时间窗口，在这个时间窗口里
  可能会修改进程的状态使它与参数state不相符或将task->array不为空以示进程在运行状态，这些都
  要做相应的处理，但无论是否出现这两种情况，进程已经是被移到其它CPU上了。但在状态不相符的情
  况下是把task->cpu修改了，但没有将进程的task->array赋予相应CPU的运行队列（但这没有问题，若
  在加上锁时state被其它进程调用try_to_wake_up()改成了TASK_RUNNING的时候也会把task->array改
  成相应的运行队列，若在加上锁时已被改成了TASK_INTERRUPTIBLE(与参数的state不相同)，那么不用
  给task->array赋值。所以加上锁之后发现task->state若与参数的state不相同就不用管task->array
  了，也不用设置task->state了。
- http://linux.cn/thread/2082/1/1/
- rq->nr_uninterruptible有什么用呢？ULK：Number of processes that were previously in the
  runqueue lists and are now sleeping in TASK_UNINTERRUPTIBLE state(only the sum of these
  fields across all runqueues is meaningful)为什么说要加起来才有用效呢？因为从
  try_to-wake_up()可以看出：如果进程已经确定要拉到本地CPU上执行了，也就是改了task->cpu，这
  时的rq(假设是rq1)与之前的rq(假设是rq2)不一样了，所以进程在进入TASK_UNINTERRUPTIBLE时是在
  rq1上对nr_uninterruptible上做自加操作的，而拉到rq2之后就在rq2上对nr_uninterruptible做自减
  操作。
- task->activated改为-1有什么用呢？
** static int wake_idle(int cpu, task_t *p)
*** kernel/sched.c:
- 注释：wake_idle() will wake a task on an idle cpu if task->cpu is not idle and an idle
  cpu is available.
- 在扫描所有域的CPU时要注意：域的SD_WAKE_IDLE是否有设置，域所包含的cpu要在cpu_online_map里，
  将拉的到的CPU要在task->cpus_allowed里,若CPU满足这些条件且该CPU也是idle就返回这个cpu.
** static void recalc_task_prio(task_t *p, unsigned long long now)
*** kernel/sched.c:
- task->timestamp:Time of last insertion of the process in the runqueue, or time of last
  process switch involving the process
- task->activated为-1表示是从TASK_UNINTERRUPTIBLE唤醒的，在try_to_wake_up()里可以看出。
- ULK:Checks whether the process is not a kernel thread, whether it is awakening from the
  TASK_UNINTERRUPTIBLE state (p->activated field equal to -1; see step 5 in the previous
  section), and whether it has been continuously asleep beyond a given sleep time
  threshold. If these three conditions are fulfilled, the function sets the p->sleep_avg
  field to the equivalent of 900 ticks
- ULK:The sleep time threshold depends on the static priority of the process;休眠时间的阀值
  依赖进程的静态优先级。
- 这个函数前面一大段是算出合适的task->sleep_avg,最后调用effective_prio()算出task->prio.而前
  面一段就注意两种情况：1.进程不是内核线程(task->mm)且进程不是从TASK_UNINTERRUPTIBLE唤醒且
  这一次的休眠时间已大于阀值就把task->sleep_avg设为最大的值；2.进程从TASK_UNINTERRUPTIBLE唤醒且进程
  不是内核线程且task->sleep_avg大于或等于阀值或task->sleep_avg加上这一次的休眠时间大于或等于阀值。
- 从这个函数可以看出进程的动态优先级是在这里修改的。
** asmlinkage void __sched schedule(void)
*** kernel/sched.c:
- profile_hit()在这里调用了
- 如果是一个进程直接调用schedule()说明调度的原因是想要的资源不能得到满足吗？
- 在这里,prev->timestamp一定是prev被切回来执行的时间，所以用sched_clock()返回的当前时间减去
  prev->timestamp就是prev这次的执行时间(run_time)，不管执行过程中产生了多少中断。
- CURRENT_BONUS()返回的值是0-10，算出这个运行时间只是最终用task->sleep_avg减去它再赋给
  task->sleep_avg
- 好像无论是EIXT_DEAD还是EIXT_ZOMBIE都是PF_DEAD这个标志，为什么在这里会有PF_DEAD的标志呢？
  为什么为PF_DEAD时要设置EXIT_DEAD
- 若prev状态是TASK_INTERRUPTIBLE且有信号挂起，那么就要把进程状态改为TASK_RUNNING,但好像在
  schedule()函数里没有看到把进程的放到运行队列里啊?
- 只有在prev进程的状态不是TASK_RUNNING且可以抢占prev(PREEMPT_ACTIVE没设)才会调用且状态不是
  TASK_INTERRUPTIBLE和有信号挂起同时存在时的会调用deactivate_task()把进程从运行队列里除掉
- idle_balance()只在schedule()里调用,注释：idle_balance is called by schedule() if
  this_cpu is about to become idle.在rq->nr_running为空就是没有可以运行的进程时才调用
  idle_balance()
- rq->exipired_timestamp:Insertion time of the eldest process in the expired lists.如果
  nr_running为空那么就要把expired_timestamp置0.
- smt:Simple Multi-Threading，SMT（同时多线程，也称超线程）
- rq->active和rq->expires是在这里调换的，调换之后rq->expired_timestamp设为0，
  rq->best_expired_prio设为MAX_PRIO
- task->activated的注释： 0- was TASK_RUNNIG, 1- was TASK_INTERRUPTIBLE and TASK_STOPPED
  awaked by system call or kernel thread 2- was TASK_INTERRUPTIBLE and TASK_STOPPED awaked
  by interrupt handler or a deferrable function, -1- was TASK_UNINTERRUPTIBLE state and it
  is being awakened.注意是was而不是is，就是说表示的是上一个状态。而不是现在的状态，现在的状态在state里。
- 如果next进程的上一个状态是TASK_INTERRUPTIBLE或TASK_UNINTERRUPTIBLE，那么在schedule()里被
  切回来的时候才会计算它的优先级.为什么计算之前要把它从运行队列里删除呢？处理完这个
  task->activated之后就把它改回0
- 为什么切换之前要把prev的TIF_NEED_RESCHED给清掉吗？不可以设置吗？
- 因为prev执行schedule()之前已经是执行了一段时间了，所以task->sleep_avg要减去执行的时间换算的一个值。
- 在这里prev的timestamp和last_ran都改成了现在的时间。
** static inline void wake_sleeping_dependent(int this_cpu, runqueue_t *this_rq)
*** kernel/sched.c:
- 这个函数只在支持多线程的情况下才使用。
- 函数的作用是把参数this_cpu所在的域的所有超线程CPU都检查是否正在执行idle进程且运行队列还有
  其它的进程，若是就要求切换那个CPU的idle进行调度。
- 在访问运行队列时是在NUMA一级，还是在物理CPU一级，还是在核一级，还是在超线程一级呢？其实在
  超线程一级
- 从blog.chinaunix.net/uid-7295895-id-2941412.html的描述来看线程域不是只包含一个线程逻辑
  CPU，它至少包含一个核的里所有的线程逻辑CPU，所以SD_SHARE_CPUPOWER应该是在线程域里指定的，
  表示共享CPU的处理能力，从而让操作系统感知到超线程硬件特性予以特殊处理。
- 所以wake_sleeping_dependent()会在一开始就判断this_rq所在的域的SD_SHARE_CPUPOWER有没有设置，
  设置了就表明this_rq->sd域是一个线程域，这时this->rq->span里的CPU就是同一个核里的逻辑CPU。
- 因为最低一层的线程域包含的是一个核里的所有逻辑CPU，所以逻辑CPU不能自已单独成为一个域了，
  但是线程域里的调度组可以是一个逻辑CPU的。在这个函数里会用循环遍历线程域里所有的逻辑CPU的
  运行队列，可以看出per_cpu变量会用到逻辑CPU一级的。所以一个双核四线程的per_cpu变量大小是4.
- 这个函数会释放this_rq的锁后又给它加上锁，所以退出这个函数的时候可能一些情况发生了变化。
** static inline int dependent_sleeper(int this_cpu, runqueue_t *this_rq)
*** kernel/sched.c:
- SD_SHARE_CPUPOWER也要被检查
- 从代码上看，进程p是当前逻辑CPU的下一个要进行执行的进程，就是将要被切换的进程。
- 返回1给schedule()时会让它切换到idle进程。防止切换之后会与另一个逻辑CPU的进程抢CPU资源
- 在什么情况下返回0呢？1.某个逻辑CPU正在执行的进程的时间片比进程p的小且某个逻辑CPU的进程不
  是实时进程;2.p是内核线程；3.某个逻辑CPU正在执行的进程是内核线程;4.p是实时进程。以上条件满
  足一个就返回0。
- 因为schedule()调用了这个函数，所以可以看出就算运行队列里有进程也不一定切换到它们而是切换
  到idle.
** static inline void finish_task_switch(task_t *prev)
*** kernel/sched.c:
- 这个函数的主要功能就是释放一下mm和task_struct.
- 如果进程是PF_DEAD才会调用put_task_struct(),但不一定会释放进程描述符。而mm就不是了，调一次这个
  函数就调一次mmdrop()，要释放mm结构体还要等到计数为0。
** static struct sched_group * find_busiest_group(struct sched_domain *sd, int this_cpu, unsigned long *imbalance, enum idle_type idle)
*** kernel/sched.c:
- BALANCE_FOR_MC_POWER和BALANCE_FOR_PKG_POWER用于节能，负载均衡可以和CPU的变频技术以及S5节
  能技术结合起来，用以将空闲的CPU降频或休眠，具体可参考find_busiest_group()函数的实现，基本
  思想就是开启节能选项后(选项开关可通过sched_mc_power_savings和sched_smt_power_savings控制
  选中SD_POWERSAVINGS_BALANCE)将负载轻的CPU的任务迁移至负载重的CPU，直至系统中产生空闲的
  CPU。
- 这个函数首先会找出负载最重的组再以节能算法找出真正的负载最重的组。
- 从imbalance参数返回的是要移多少个进程。
** static runqueue_t *find_busiest_queue(struct sched_group *group)
*** kernel/sched.c:
- 遍历组里所有CPU找出负载最重的CPU。
** static int load_balance(int this_cpu, runqueue_t *this_rq, struct sched_domain *sd, enum idle_type idle)
*** kernel/sched.c:
- 先调用find_busiest_group()找出最忙的组，再调用find_busiest_queue()找出组中最忙的CPU.
- 调用move_tasks()批量移进程
- 若move_tasks()一个进程也没有移成，那么就要增加nr_balance_failed计数给can_migrate_task()用
- sched_domain->last_balance记录了上次做定时均衡的时间点，单位为tick，只在定时均衡中更新，
  和sched_domain->balance_interval一起用于计算下次做定时均衡的时间。在这个函数里，如果虽然
  不平衡行但move_tasks()也没有成功移过一个进程，那么就要增加下一次的定时平衡时间，使得不要
  那么频繁进行对这个域做平衡。
- 如果是move_tasks()没有移成功过一个进程且失败率大于sched_domain->cache_nice_tries+2那就要
  设置忙运行队列的active_balance(Flag set if some process shall be migrated from this
  runqueue to another).如果之前active_balance之前没有设置过就要唤醒最忙运行队列的迁移进程的
  内核线程(sched_domain->migration_thread)
- 如果move_tasks()有成功移过进程，那么就把nr_balance_failed给清零，并把balance_interval设为
  min_interval
- 如果没有找到最忙的组或最忙的运行队列或最忙的队列是本地队列，同时balance_interval比
  max_interval要小，那么就要将balance_interval翻倍。
** static void double_lock_balance(runqueue_t *this_rq, runqueue_t *busiest)
*** kernel/sched.c:
- 不同运行的队列的锁是否有某种关系呢？为什么在busiest的锁trylock不成功且busiest小于this_rq
  时会先解this_rq->lock这个锁再加锁呢？busiest比this_rq的地址小说明了什么问题呢？
** static int move_tasks(runqueue_t *this_rq, int this_cpu, runqueue_t *busiest, unsigned long max_nr_move, struct sched_domain *sd, enum idle_type idle)
*** kernel/sched.c:
- 参数sd只是给做一些调度统计用的，和传给can_migrate_task()用的。
- 先考虑移expired链表上的，若expired上没有进程就再考虑移active里的。若是从busiest里的
  expired移，就要移到this_rq的expired上，active同理。
- 这里的goto用得好恐怖啊
- 这个函数就是把指定的CPU的运行队列里的进程放到指定的其它CPU运行队列里。
- 从skip_bitmap这个点开始下面的几行代码来看，看从active里的最高优先级进程开始移，接着再从
  expired的最高优先级开始,直到把参数指定要移的进程数max_nr_move已经移完。
** static inline int can_migrate_task(task_t *p, runqueue_t *rq, int this_cpu, struct sched_domain *sd, enum idle_type idle)
*** kernel/sched.c:
- 在什么情况下不移呢：进程正在运行；进程不允许在那个CPU上执行；被移的进程所在的CPU对于该进
  程是cache-hot的；但是如果其它的sliblings CPU都是idle的或太多平衡操作失败了，就算进程是
  cache-hot的也要移。
** static inline void pull_task(runqueue_t *src_rq, prio_array_t *src_array, task_t *p, runqueue_t *this_rq, prio_array_t *this_array, int this_cpu)
*** kernel/sched.c:
- 把进程在原来运行队列已经不运行的时间改成在新运行队列已经不运行的时间，做法就是用进程上一
  次被切换出去的时间戳(task->timestamp)减去原来运行队列所记录的上一次时间中断的时间
  (timestamp_last_tick)(这个时间差有可能是正数(进程的上一次切换是自愿的且在切换的之后没有发
  生过时钟中断)，也有可能是负数))再加上本地运行队列的timestamp_last_tick.
- 在这个函数里如果发现被移的进程的优先级比当前的进程还高，那么就设置当前进程的调度标志。
** asmlinkage long sys_nice(int increment)
*** kernel/sched.c:
- increment是一个增量,这个进程优先级的增量。
- nice的范围是-20到19
- 进程的优先级是0到140的
- 如果nice为负数，就是增量为负，那么要超级用户才可以操作。
** void set_user_nice(task_t *p, long nice)
*** kernel/sched.c:
- 要加上运行队列的锁，因为进程可能正在另一个CPU调度中被调度。
- 如果是一个实时进程，那么就调用PRIO_TO_NICE来修改它的优先级,因为NICE_TO_PRIO是这样定义
  的#define NICE_TO_PRIO(nice) (MAX_RT_PRIO + (nice) + 20)，所以调用之后原来是实时的进程就
  不再是实时进程了。但是有这样的注释：but as expected it wont have any effect on
  scheduling until the task is not SCHED_NORMAL.
- 因为会修改进程优先级，所以要先把进程从运行队列里删除，修改完之后再插入到合适的优先级。
- 其实nice值是可以通过静态优先级来换算的，所以进程描述符里没有类似nice这样的成员。修改nice
  值就是修改进程的静态优先级。
- 通过nice值修改进程的动态优先级和静态优先级的时是不同的，但是从代码的结果看修改的结果是一
  样的啊，为什么要多此一举呢？修改后动态优先级和静态优先级是一样的。
- nice()系统调用是修改当前进程的.但这个函数是可以指定不同的进程的.所以在sys_nice()系统调用
  里调用的set_user_nice()是以current作为参数调用的.
- 如果被修改的进程的动态优先级被改小了,那么就要把被修改进程所在的运行队列正在运行的的进程调
  度一下而不管被修改的进秵的动态优先级是否比当前执行的进程的优先级高.或如果动态优先级被改大
  了同时被修改的完程正在执行那么也对自已进行调度.
** #define rt_task(p)		(unlikely((p)->prio < MAX_RT_PRIO))
*** include/linux/sched.h:
- 通过优先级来判断就可以了，就是那么简单。
** asmlinkage long sys_getpriority(int which, int who)
*** kernel/sys.c:
- PRIO_PROCESS:Selects the processes according to their process ID,找出指定进程的nice,范围
  是40-1.
- PRIO_PGRP:Selects the processes according to their group ID,找出指定进程所在的进程组的
  nice值最大的进程.
- PRIO_USER:Selects the processes according to their user ID,因为用户的进程没有用链表链起
  来,所以要遍整个进程链表才可以找出所有的进程.
** asmlinkage long sys_setpriority(int which, int who, int niceval)
*** kernel/sys.c:
- 结构上与sys_getproirity()类似,只是不是比较而是调用了set_one_prio()修改nice.
** static int set_one_prio(struct task_struct *p, int niceval, int error)
*** kernel/sys.c:
- 若进程的用户id(uid)不等于当前进程的有效用户id(euid)且进程的有效用户id(euid)不等于当前进程
  的有效用户id(euid),那就不允许修改除非是超级用户.
- 最后还是与sys_nice()一样调用set_user_nice()来修改进程的nice.
** asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len, unsigned long __user *user_mask_ptr)
*** kernel/sched.c:
- 主要是调用了sched_getaffinity()
** long sched_getaffinity(pid_t pid, cpumask_t *mask)
*** kernel/sched.c:
- 找出进程pid可以被调度到的CPU的掩码,就是task->cpus_allowed.
** asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len, unsigned long __user *user_mask_ptr)
*** kernel/sched.c:
- 就是调用sched_setaffinity()
** long sched_setaffinity(pid_t pid, cpumask_t new_mask)
*** kernel/sched.c:
- 主要是调用cpuset_cpus_allowed()找出允许执行该进程的CPU，再用参数掩码相与,最后的结果给
  set_cpus_allowed()
** int set_cpus_allowed(task_t *p, cpumask_t new_mask)
*** kernel/sched.c:
- 新的掩码不在cpu_online_map里的话就返回。
** asmlinkage long sys_sched_getscheduler(pid_t pid)
*** kernel/sched.c:
- 就只是返回task->policy.
- SCHED_FIFO, SCHED_RR , or SCHED_NORMAL (the latter is also called SCHED_OTHER)
** asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param __user *param)
*** kernel/sched.c:
- 获取的是实时进程的优先级，就是task->rt_priority,把task->rt_priority放到参数的
  param->sched_priority.
** 
- ULK:Child process stopped or terminated, or got signal if traced.所以在停止或终于或被跟踪
  时收到。
- 常规信号与中断一样是不会排队的，但是POSIX的实时信号是会排队的。
- 信号在处理时是不可以被其它的信号中断的
- 从用户态切到内核态时会检查是否有信号在挂起
- ULK:If a process receives a signal while it is being traced, the kernel stops the
  process and notifies the tracing process by sending a SIGCHLD signal to it. The tracing
  process may, in turn, resume execution of the traced process by means of a SIGCONT
  signal.被跟踪的进程收到信号时将会停止进程接着会发SIGCHLD给跟踪进程，若要恢复被跟踪进程的
  执行，那么跟踪进程就要发SIGCONT给被跟踪进程。
- 进程描述符的ptrace的PT_PTRACED为0就说明进程没有被跟踪。
- 信号的阻塞和怱略是不一样的，ULK:A signal is not delivered as long as it is blocked; it
  is delivered only after it has been unblocked. An ignored signal is always delivered,
  and there is no further action.原来怱略的信号总是被递送的。
- 多线程间，信号的处理函数要共享，但有各自的挂起和阻塞掩码，调用kill()和sigqueue()时，按
  POSIX的要求要把信号发给线程组里所有的进程；发给一个线程的信号可以任意选择一个线程,但若信
  号是致命的那么要杀死线程组中所有的线程。
- 信号的处理函数结构体只在进程描述符里，没有在signal_sturct里，因为多线程里信号处理函数是可
  以共享的，线程的进程描述符的信号处理函数指针指向领头线程的就可以了。
- 关于进程描述符的notifier的说明是:Pointer to a function used by a device driver to block
  some signals of the process.
- 进程描述符的real_blocked的解释是Temporary mask of blocked signals (used by the
  rt_sigtimedwait( ) system call)。
- 几个信号相关的结构体之间的关系：进程描述符struct task_struct(1)包含struct
  sigpending(2),struct signal_struct(3),struct sighand_struct(4);struct sigpending(2)不包含
  struct sigqueue(5)但是它的list成员是链到struct sigqueue的;struct signal_struct(3)包含
  struct sigpending(2);struct sighand_struct(4)包含struct k_sigaction(6),struct
  sigqueue(5)包含struct siginfo_t结构体
- 一个信号处理时可以指定是否mask该信号，在k_sigaction的sa_flags域的SA_NODEFER, SA_NOMASK标
  志。但同样是k_sigaction还一个类似的成员sa_mask指定当运行信号处理程序时被mask的信号。
- pending的信号是用struct sigpending链起来的.
- struct sigpending里的signal成员是sigset_t类型的，只是一个挂起信号的掩码，具体的挂起信号的
  信息还要通过struct sigpending里的list成员链起来的sigqueue的si_signo来标识具体是那个信号。
** static inline int signal_pending(struct task_struct *p)
*** include/linux/sched.h:
- 检查了TIF_SIGPENDING标志。
- 这个函数不会检查进程描述符的pending和signal->shared_pending之类的东西的，这些在
  recalc_sigpending_tsk()里进程。
** fastcall void recalc_sigpending_tsk(struct task_struct *t)
*** kernel/signal.c:
- 为什么要检查进程描述符的signal->group_stop_count呢？
- 还检查进程描述符的pending和signal->shared_pending，从检查的方法可以知道
  PENDING(&t->pending, &t->blocked)，被block的信号是可以放在sigpending的队列里的，但是要看
  这个信号是否真的pending中，就要与block掩码相对比。
** static int rm_from_queue(unsigned long mask, struct sigpending *s)
*** kernel/signal.c:
- 从这个函数可以看出siginfo_t->si_signo不是以从1-32这32个数字来标识信号的，而是用位掩码的方
  式来标识的。
- 要删除一个挂起的信口，就要删除signal_struct->signal里的掩码，把相应的sigqueue从链表里删除，
  释放相应的sigqueue结构体。
** static void flush_sigqueue(struct sigpending *queue)
*** kernel/signal.c:
- 与rm_from_queue的相类似。
** void flush_signals(struct task_struct *t)
*** kernel/signal.c:
- 这个函数会清掉进程的TIF_SIGPENDING，再对进程的pending调用flush_sigqueue(),再对进程的
  signal->shared_pending调用flush_sigqueue().
** int send_sig(int sig, struct task_struct *p, int priv)
*** kernel/signal.c:
- 以priv是否等于0作为info的参数调用send_sig_info()
** int send_sig_info(int sig, struct siginfo *info, struct task_struct *p)
*** kernel/signal.c:
- 以相同的参数调用specific_send_sig_info()
- 关于specific_send_sig_info()的info的参数：ULK:0 means that the signal has been sent by a User Mode process, 1
  means that it has been sent by the kernel, and 2 means that is has been sent by the
  kernel and the signal is SIGSTOP or SIGKILL.
** void force_sig(int sig, struct task_struct *p)
*** kernel/signal.c:
- 以1的info调用force_sig_info()
- 这是对进程私有的信号的处理，不是共享的信号.
- 如果信号是被阻塞的或信号是被怱略的(SIG_IGN)，那么就要把信号的处理函数改为SIG_DFL且改成不
  阻塞，接着还要设置TIF_SIGPENDING。最后还是以相同的参数调用specific_send_sig_info().
** void force_sig_specific(int sig, struct task_struct *t)
*** kernel/signal.c:
- 和force_sig()差不多，有点不同的是force_sig()会在信号被阻塞的情况下也把信号的处理程序改成
  SIG_DFL。
- 和force_sig()最主要的区别是调用specific_send_sig_inof()时是以info为2来调用的。
** int send_group_sig_info(int sig, struct siginfo *info, struct task_struct *p)
*** kernel/signal.c:
- 以相同的参数调用group_send_sig_info()
** int kill_pg(pid_t pgrp, int sig, int priv)
*** kernel/signal.c:
- 以priv是否为0的结果作为参数调用kill_pg_info()
** int kill_pg_info(int sig, struct siginfo *info, pid_t pgrp)
*** kernel/signal.c:
- 以相同的参数调用__kill_pg_info()
** int kill_proc(pid_t pid, int sig, int priv)
*** kernel/signal.c:
- 以priv是否为0的结果作为参数调用kill_proc_info()
** int kill_proc_info(int sig, struct siginfo *info, pid_t pid)
*** kernel/signal.c:
- 先用find_task_by_pid()再用找到的进程描述符调用group_send_sig_info()
** static int specific_send_sig_info(int sig, struct siginfo *info, struct task_struct *t)
*** kernel/signal.c:
- 若info是真实分配空间的且si_code是SI_TIMER那么就要把返回值设成
  info->si_sys_private,SI_TIMER表示信号的发送者是Timer expiration
- 若信号是被怱略就退出，但返回值可能在SI_TIMER的情况下被修改了。同样在信号已在挂起的时候也
  退出。
- 用send_signal()把信号加上.
- 若信号没有被block就调用signal_wake_up()
- 在调用signal_wake_up()时,若信号是SIGKILL那么就以resume为1的参数调用signal_wake_up().
- 若成功给进程发送了一个信号,那么就会把这个进程唤醒,让它处于可执行的状态,但是
  signal_wake_up()里调用的kick_process()有一个不足,就是若被唤醒的进程是本地CPU的进程且被唤
  醒之后的优先级比current的高,但这时不会进行进程切换.而不是本地CPU的进程时就不是这样处理,而
  是不管优先级都会给所在的进程发一个切换中断.
** static int sig_ignored(struct task_struct *t, int sig)
*** kernel/signal.c:
- 被跟踪(PT_PTRACED)的情况下无论如何是不能怱略信号的，优先级最高.
- 如果私有的信号block掩码有标上该信号,那么也是不能忽略的.
- 只有在处理函数指定是SIG_IGN或是SIG_DFL且默认的处理是怱略的时候才可以怱略该信号.
** void signal_wake_up(struct task_struct *t, int resume)
*** kernel/signal.c:
- 进入这个函数就一定会设置TIF_SIGPENDING
- 一定会唤醒在TASK_INTERRUPTIBLE状态的进程的
- 若信号是SIGKILL,那么参数resume就是1,这时也会唤醒在TASK_STOPPED和TASK_TRACED状态下的进程的.
- 调用wake_up_state()之前不判断进程的状态,因为会出现竞争,所以没有必要.
** int fastcall wake_up_state(task_t *p, unsigned int state)
*** kernel/sched.c:
- 就直接调用try_to_wake_up(),若try_to_wake_up()返回0,那么表示进程已经是可以运行的了.
** void kick_process(task_t *p)
*** kernel/sched.c:
- 这个函数主要的功能就是若进程t不是本地CPU的进程且又不是正在运行的进程,那么就给它所在的CPU
  发一个调度的中断.不是这样理解的,而是若进程t不是本地CPU的进程且又是正在运行的进程,那么就给
  它所在的CPU发一个调度中断,因为ULK:each process checks the existence of pending signals
  when returning from the schedule( ) function, the interprocessor interrupt ensures that
  the destination process quickly notices the new pending signal.
** static int send_signal(int sig, struct siginfo *info, struct task_struct *t, struct sigpending *signals)
*** kernel/signal.c:
- 若info是2,那么表示信号是SIGKILL或SIGSTOP或是由force_sig_specific()发送的,这时就不用作插入
  sigqueue的工作,但也要把sigpending->signal相应的位掩码给标上.因为:It is important to let
  the destination process receive the signal even if there is no room for the
  corresponding item in the pending signal queue. Suppose, for instance, that a process is
  consuming too much memory. The kernel must ensure that the kill( ) system call succeeds
  even if there is no free memory;
- SI_USER:0:sent by kill(), sigsend(), raise()
- SI_KERNEL:0x80:sent by the kernel from somewhere
- SI_QUEUE:-1:sent by sigqueue()
- 如果调用__sigqueue_alloc()时的最后一个参数为1那么就允许超过限制分配sigqueue.在这个函数里
  调用时是信号是常规信号且info小于2(这里不可能等于2，因为之前已作判断了，信号不是内核发送的
  SIGKILL或SIGSTOP)或info是分配了空间时信号是由kill(),sigsend(),raise()或内核发送的
  (info->si_code>=0就是SI_USER和SI_KERNEL),但是想不通的是为什么由用户态和内核态发送的常规信
  号可以超过限制。什么情况下才不能超过限制呢？其实超过限制也不一定说明分配就会失败。
- 如果成功分配了sigqueue,那么就给它的info初始化，若info的参数是0或1，就要单独初始化info里的
  成员，注意在info为1的情况下si_pid和si_uid是0的.
- 关于返回EAGAIN的情况：ULK：an item will not be added to the signal pending queue,
  because there are already too many pending signals, or there is no free memory for the
  sigqueue data structure, or the signal is immediately enforced by the kernel. If the
  signal is real-time and was sent through a kernel function that is explicitly required
  to queue it, the function returns the error code -EAGAIN。注意在这里info不可能为2.不是在
  SI_USER的情况下发的实时信号就要求重试。
** int group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)
*** kernel/signal.c:
- 主要是调用__group_send_sig_info().
- 为什么要在p->sighand不为空时才会执行相应的动作呢？
** static int check_kill_permission(int sig, struct siginfo *info,
*** kernel/signal.c:
- ULK:The signal is delivered only if at least one of the following conditions holds: The
  owner of the sending process has the proper capability (usually, this simply means the
  signal was issued by the system administrator; see Chapter 20). The signal is SIGCONT
  and the destination process is in the same login session of the sending process. Both
  processes belong to the same user.
- 以下这个是保证发这个信号的肯定是用户态下发的而不是在内核态发的，不管它是不是SIGKILL或
  SIGSTOP.如果真的是SIGKILL或SIGSTOP那调用了这个函数的__group_send_sig_info()该怎么办呢？
 #+BEGIN_EXAMPLE
(!info || ((unsigned long)info != 1 &&
				   (unsigned long)info != 2 && SI_FROMUSER(info)))
 #+END_EXAMPLE
- 以下的保证信号是SIGCONT且是同一个会话才允许发送。
 #+BEGIN_EXAMPLE
  ((sig != SIGCONT) ||
  (current->signal->session != t->signal->session))
 #+END_EXAMPLE
- 当前的进程euid和uid分别与别进程t的suid和uid比较。
 #+BEGIN_EXAMPLE
  (current->euid ^ t->suid) && (current->euid ^ t->uid)
  && (current->uid ^ t->suid) && (current->uid ^ t->uid)
 #+END_EXAMPLE
- http://zhumeng8337797.blog.163.com/blog/static/10076891420112410364978/ set-user-ID
  (suid)/set-group-ID(sgid):设置用户ID,这是相对于文件来说的.设置了set-user-ID位的可执行程
  序,执行时,进程的effective user ID与saved set-uesr-ID都为程序文件所属用户的ID. 时real
  user ID与effective user ID就不一定相等了.这类程序称之为SUID程序,这类程序有特殊的用途.典型
  的例子:passwd程序,ping程序等.如果一个文件被设置了SUID或SGID位，会分别表现在所有者或同组用
  户的权限的可执行位上。
- 除了一般的user id 和group id外，还有两个称之为effective 的id，就是有效id，上面的四个id表
  示为：uid，gid，euid，egid。内核主要是根据euid和egid来确定进程对资源的访问权限。一个进程
  如果没有SUID或SGID位，则euid=uid egid=gid，分别是运行这个程序的用户的uid和gid。例如kevin
  用户的uid和gid分别为204和202，foo用户的uid和gid为 200，201，kevin运行myfile程序形成的进程
  的euid=uid=204，egid=gid=202，内核根据这些值来判断进程对资源访问的限制，其实就是kevin用户
  对资源访问的权限，和foo没关系。如果一个程序设置了SUID，则euid和egid变成被运行的程序的所有
  者的uid和gid，例如kevin用户运行myfile，euid=200，egid=201，uid=204，gid=202，则这个进程具
  有它的属主foo的资源访问权限。SUID的作用就是这样：让本来没有相应权限的用户运行这个程序时，
  可以访问他没有权限访问的资源。passwd就是一个很鲜明的例子。SUID的优先级比SGID高，当一个可
  执行程序设置了SUID，则SGID会自动变成相应的egid。
- 而进程的suid是用来因执行一个设置了suid程序文件而用来保存进程的euid的。
** int __group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)
*** kernel/signal.c:
- 调用的send_singal()是把信号放在共享挂起信号上的.接着调用__group_complete_signal()来选择一
  个线程处理相应的信号,若是致命信号就调会给所有的线程发SIGKILL信号.
** static void handle_stop_signal(int sig, struct task_struct *p)
*** include/linux/sched.h:
- 是什么样的进程才会有SIGNAL_GROUP_EXIT标志呢？是发起组退出的那个进程吗？
- 用sig_kernel_stop()检查信号是不是停止信号，若是先把线程组里的共享SIGCONT信号删掉，再把其
  它线程的私有SIGCONT给删除.所以可以看出给一个组发停止信号就会把所有的线程都continue.
- SIGNAL_STOP_CONTINUED只是在handle_stop_signal()里赋值，当正在进行组停止的时候收到一个
  SIGCONT信号。SIGNAL_STOP_CONTINUED有什么用呢？
- 正在组退出的时候如果信号是SIGCONT且不是被跟踪的话为什么调用do_notify_parent_cldstop()的时
  候参数是p->group_leader呢？好像也不必要知道为什么，只要记住是这样就可以了：如果一个进程要
  用SIGCHLD通知它的父进程的话，且这个进程没有被跟踪，那么要通知的进程将是该进程所在的进程组
  的领头进程的父进程，而不是该进程的父进程。
- 为什么在信号是SIGCONT且在组退出的时候会把p->signal->group_stop_count清零呢？
- group_stop_count是记录当前线程组中未停止的线程的个数，在通过tkill系统调用的时候发送给某个
  线程停止信号（SIGSTOP，SIGTSTP，SIGTTIN，SIGTTOU），这会使得线程组中的所有线程停止。在进
  程fork线程的时候，如果flag是CLONE_THREAD(复制线程)，多个线程会共用一个signal_struct,如果
  tkill一个信号到某个线程，但是这个tkill会把信号发送到线程task_struct的pending中，这个字段
  是属于线程的私有信号结构。按照道理，当这个线程由于系统调用，异常，中断等返回用户态后，会
  处理线程中的信号，无论是私有的还是共有的信号都会处理。那些信号都是私有信号，那么就tkill对
  应的线程会，只有那一个线程会采取stop操作。但是实际上，内核中对停止信号（SIGSTOP，SIGTSTP，
  SIGTTIN，SIGTTOU），会采取特殊的处理方式，就是stop信号会使得线程组中其他线程将状态转换为
  TASK_STOPED,使得整个线程组停止。就是说一个线程的停止会导致整个线程组的停止。这个特性靠
  signal_struct的group_stop_count变量来实现，对于一个线程组来说每个线程的使用的都是同一个
  signal_struct,这个变量来记录，还有多少个线程还未stop。这样可以想到逻辑就是：如果一个线程
  收到了SIGSTOP信号，group_stop_count的值就是线程组中的线程个数，同时这个线程会通过
  signal_wake_up来告诉线程组中的线程有信号需要处理，那么线程组中的每个线程都会调用信号处理
  函数，每个线程都会首先处理stop信号，判断group_stop_count如果大于0就代表当前线程需要被停止
  了，停止之前将group_stop_count减一，如果线程组中的线程都停止了，group_stop_count=0,那么就
  通知父进程告诉他子线程组停止了.http://blog.chinaunix.net/uid-27767798-id-3496692.html. 所
  以在这里把group_stop_count清零表示还没有停止的进程就不要再停止了。接着在下面调用的
  do_notify_parent_cldstop()是为了用SIGCHLD通知父进程子线程组停止了，父进程可以去做其它的事
  情了。
- stop类的信号是对整个线程组起作用的，一个线程收到了stop信号会迫使线程组中其它的线程也停止，
  如果在这停止线程组过程中收到了SIGCONT信号，那么group stop会立刻停止，使得线程组继续执行，
  已经停止的进程应该是没有办法通过一个SIGCONT信号来唤醒已经停止的其他线程了，只能tkill每一
  个线程，发送SIGCONT信号，来唤醒停止的线程。
  http://blog.chinaunix.net/uid-27767798-id-3496692.html
- group_stop_count等于1说明其他的线程都已经结束了，还差当前进程未结束。
  http://blog.chinaunix.net/uid-27767798-id-3496692.html
- 如果在group stop的过程中，或者get_signal_to_deliver函数已经从信号挂起队列中已经dequeue出
  了一个stop信号，线程组中的一个线程收到了SIGCONT信号，意味着线程组不用在继续stop了，这时需
  要将gourp_stop_count变为0。不让线程组再继续stop了，这里前面讲到的signal_struct标志位的
  SIGNAL_STOP_DEQUEUED，就起到了作用，在dequeue出stop信号的时候，会加上这个标志位。在执行
  do_signal_stop的之前，也就是第一个stop的进程处理的时候，也会判断这个SIGNAL_STOP_DEQUEUED
  标志位，这时如果收到了SIGCONT信号，清除标志位SIGNAL_STOP_DEQUEUED是有必要了，起到了避免了
  收到了SIGCONT，而继续停止第一个线程的作用.  http://blog.chinaunix.net/uid-27767798-id-3496692.html
- tsk->signal->flags的标志就4个而已:SIGNAL_STOP_STOPPED:job control stop in
  effect.SIGNAL_STOP_DEQUEUED:stop signal dequeued.SIGNAL_STOP_CONTINUED:SIGCONT since
  WCONTINUED.SIGNAL_GROUP_EXIT:group exit in progress
- SIGNAL_STOP_STOPPED说明所有的线程都已经停止了
  http://blog.chinaunix.net/uid-27767798-id-3496692.html
- SIGNAL_STOP_DEQUEUED，这个标志位的意思是，从信号队列中已经dequeue了一个stop的信号.
- SIGNAL_STOP_CONTINUED:当线程组收到SIGCONT信号时,无论是中组停止的过程中还是最近完成了一次
  组停止(会置上SIGNAL_STOP_STOPPED),那会设置SIGNAL_STOP_CONTINUED.这上可以从这个函数里看出
  来.这个标志要经过这样一个过程.
- CLD_CONTINUED:这个基本只在handle_stop_signal()调用do_notify_parent_cldstop()时使用,所以可
  以从这个使用上找出它有什么用,使用它的情况是当线程组接到一个SIGCONT时且之前线程组已完成了
  所有线程停止(SIGNAL_STOP_STOPPED)而不是在组停止的过程中时就用do_notify_parent_cldstop()就
  会使用.那么是不是想用这个标志说明从一个所有线程停止的状态里收到了一个SIGCONT信号.
- CLD_STOPPED:是在收到SIGCONT信号时且组停止还在进行时调用do_notify_parent_cldstop()时使用,
  所以它的意思是想告诉父进程线程组已完成停止且是组停止被中断的情况下完成的.
- 在收到一个SIGCONT时,就算正在组停止中,也要把线程组中的共享stop信号给删掉,再逐个逐个把线程
  组中的所有的线程的stop类信号给删掉.不管线程私有信号蔽避是否有蔽避SIGCONT,如果组收到
  SIGCONT信号时就一定唤醒线程组里的所有在TASK_STOPPED线程,若SIGCONT自定义了处理函数且没有被
  蔽避,那么也会唤醒TASK_INTERRUPTIBLE的进程
** static void do_notify_parent_cldstop(struct task_struct *tsk, struct task_struct *parent, int why)
*** kernel/signal.c:
- 这个函数是在收到一个SIGCONT时调用的。这个函数的一个主要目的就是给父进程发一个SIGCHLD信号，
  而在发之前要根据不同的情况给info初始化,同时还唤醒父进程。是不是想给父进程发SIGCHLD信号时
  都会调用这个函数呢？
- 说明一下这个不好理解的struct siginfo结构体，si_code成员不只ULK上介绍的，除了
  SI_USER（kill(),raise()）,SI_KERNEL(Generic kernel
  function),SI_QUEUE(sigqueue()),SI_TIMER(Timer expiration),SI_ASYNCIO (Asynchronous I/O
  completion),SI_TKILL(tkill( ) and tgkill( ))还有可以是
  SIGCHLD,SIGILL,SIGFPE,SIGSEGV,SIGBUS,SIGPOLL,SIGTRAP信号产生是的相应的si_code代码,如
  SIGILL就有ILL_ILLOPC, ILL_ILLOPN, ILL_ILLADR等的si_code值，SIGCHLD的就有CLD_TRAPPED
  CLD_STOPPED CLD_CONTINUED等。到这里想起之前有一个判断si_code是否大于0的情况，现在发现只有
  SI_KERNLE和信号为SIGILL,SIGFPE,SIGSEGV,SIGBUS,SIGTRAP,SIGCHLD,SIGPOLL的时候都是大于0的.其
  实发现struct siginfo结构体里那个联合体也针对不同的类型的si_code作的。所以若
  info->si_signo是那些特殊的信号的时候就要给info->si_code赋相应的特定的值且还要给那个联合体
  的相对应的成员做赋值。
- 在info->si_signo为SIGCHLD的时候可以看这个函数来具体认清相应的联合体里每个成员的作用。
  si_utime是赋予了子进程的utime,si_stime也是类似。si_status的注释说是exit
  code,CLD_CONTINUED时是SIGCONT,CLD_STOPPED时是signal->group_exit_code,CLD_TRAPPED时是
  tsk->exit_code.
- 在handle_stop_signal()里若信号是SIGCONT时会以CLD_STOPPED来调用do_notify_parent_cldstop()
  的,这是不是可以推出若进程是在停止状态收到SIGCONT时就会用通知父进程上子进程的上一个状态是
  停止的。如果上一个状态不是在被跟踪的情况下(CLD_STOPPED)停止的话那么就会把si_status赋予
  tsk->signal->group_exit_code,这也可以看CLD_STOPPED就是为在组停止时收到SIGCONT信号时服务的。
- 如果SIGCONT信号是被父进程怱略的或SIGCONT信号设置了SA_NOCLDSTOP(Applies only to SIGCHLD;
  do not send SIGCHLD to the parent when the process is stopped),那么就调
  用__group_send_sig_info()给父进程所在的整个线程组发一个SIGCHLD信号，用之前的初始化的
  info(si_signo是SIGCONT),给整个线程组发信号是不是想用共享的处理函数的意思呢？
- 无论有没有把信号发成功，都会唤醒父进程。
- CLD_TRAPPED是在
- group_exit_code ULK的注释：Process termination code for the thread group.为什么是
  CLD_STOPPED的时候就要把group_exit_code赋给info->si_status呢？停止时收到SIGCONT信号与组的
  退出代码有什么关系吗？
** static void __group_complete_signal(int sig, struct task_struct *p)
*** kernel/signal.c:
- EXIT_ZOMBIE, EXIT_DEAD, TASK_TRACED, TASK_STOPPED状态的进程是不可以接收信号的,但是在
  TASK_STOPPED和TASK_TRACED状态时若信号是SIGKILL把可以接收.
- wants_signal()就是看p是不是可以作为处理信号的的进程.与进程的信号阻塞有关，与进程的状态有
  关，与进程是否在退出有关(PF_EXITING和EXIT_DEAD、EXIT_ZOMBIE)进程是否在执行有关，和进程是
  否有挂起信号有关。
- 若线程组里只有一个线程，那就不用选了，就用参数进程p.若不是就要扫描整个线程组。若还是没有
  一个进程想处理这个信号的，那么就直接退出这个函数，这样做有什么后果呢？
- tsk->real_blocked这个成员好像只有在这个函数和sys_rt_sigtimedwait()里使用。
- 这个函数不像ULK上说的那么简单，仅仅是收到一个致命信号还不足以杀掉整个线程组，还有很多条件
  要满足：若是正在组退出那不用管了；不能是real_blocked的成员（这个real_blocked有什么用
  呢？）；若不是SIGKILL这个致命信号时那么进程就不能被跟踪。
 #+BEGIN_EXAMPLE
  (sig_fatal(p, sig) && !(p->signal->flags & SIGNAL_GROUP_EXIT) &&
	    !sigismember(&t->real_blocked, sig) &&
	    (sig == SIGKILL || !(t->ptrace & PT_PTRACED)))
 #+END_EXAMPLE
- 因为dump的处理方式的信号也会杀死进程，所以在杀死整个组的情况下会考虑信号是不是dump,若是就
  要转存。
- sig_fatal()检查信号是否为致命信号不仅仅是检查信号的默认行为是不是dump或terminal，还要检查
  它的处理方式是不是默认的。
- 若不能满足作致命信口的处理，那么就用选中的进程调用signal_wake_up()，调用的另一个参数就用
  信号与SIGKILL比较。
- 如果不是dump信号，那么就要设置tsk->signal->flags的SIGNAL_GROUP_EXIT。所以设置这个标志之后
  说明要进行线程组退出了。同时这里还把tsk->signal->group_exit_code退出码置成了信号，看来作
  用不止一个啊。还把tsk->signal->group_stop_count组停止计数给清零了，为什么要这样做呢？是不
  是让还在进行组停止的进程组不要再进行组停止了。为什么要把所有的线程都停止了呢？有这样的注
  释：We make all threads other than the chosen one go into a group stop so that nothing
  happens until it gets scheduled, takes the signal off the shared queue, and does the
  core dump.
- SIGNAL_GROUP_EXIT在do_group_eixt(),在__group_complete_signal(),在zap_other_threads()被置
  位。
- 处理这个core dump信号的进程还是被选中的进程。
- group_exit_task是有两种含义的，注释也说了:overloaded:notify group_exit_task when ->count
  is equal to notify_count; everyone except group_exit_task is stopped during signal
  delivery of fatal signals, group_exit_task processes the signal.一个是用于组退出结束后通
  知group_exit_task进程http://blog.chinaunix.net/uid-27767798-id-3492389.html， 另一个是在递
  送一个致命信号时希望group_exit_task是停止的，后者的这种情况是在这个函数出现的。
** int get_signal_to_deliver(siginfo_t *info, struct k_sigaction *return_ka, struct pt_regs *regs, void *cookie)
*** kernel/signal.c:
- 这个函数会用处理所有的信号.
- ULK:the do_signal( ) function is usually only invoked when the CPU is going to return in
  User Mode.不是在进程在切换时做的。
- 在满足正在进行组停止时才会调用handle_group_stop(),若没有进行组退出，那么是不用调用
  handle_group_stop()的，为什么有这个特殊的条件呢？这也是为什么handle_group_stop()叫
  handle_group_stop了
- 每处理完一个信号就会判断一下是否有组停止,是否应该调用handle_group_stop().组停止切回来后还
  会马上处理其它所有的挂起信号.
- 若有信号要处理且信号不是SIGKILL(为什么要排除SIGKILL呢?)且被跟踪,那么就要作跟踪的处理，要
  调用ptrace_stop(),这个函数下面有讲。
- 调用ptrace_stop()之后，跟踪进程可能会取消进程的信号，取消的方法是把current->exit_code清零。
  若不取消那么就继续处理，否则就处理下一个信号。
- 若跟踪进程修改了信号，就要修改参数info,若进程没有阻塞信号就给它发信号，所以注意是先通知跟
  踪进程让它处理再根据跟踪进程的反馈让被跟踪进程作相应的处理。但被阻塞的情况下也还会有很多的处理。
- 若是发现信号的处理方法是SIG_IGN那么就处理下一个信号，若发现信号是SIG_DFL那么就不用与处理
  下一个信号了，直接退出函数。
- 因为被跟踪进程可能会修改跟踪进程的信号，所以return_ka和原来的不一样了。若发现不是默认的，
  且设了SA_ONESHOT那么就把它改回默认的。
- 之前都已经检查了信号的处理是否是默认的了，但为什么后面又用sig_kernel_ignore来检测呢？而且
  为真时还处理下一个信号。嗅，知道了，原来之前检查的是SIG_IGN和不为SIG_DFL的情况，而用
  sig_kernel_ignore()检查的是为SIG_DFL且默认的处理方式是怱略。
- 在这里判断进程是否是进程1。
- 一个进程组不是孤儿进程组的条件是，该组中有一个进程，其父进程在属于同一个会话（session）的
  另一个组中.所以若信号的默认处理方式是停止但信号不是SIGSTOP且进程所在的进程组是一个孤儿进
  程组，那么就不处理这个信号，处理下一个信号。为什么SIGSTOP这个信号要作特殊处理呢？难道
  是SIGSTOP时就一定是执行do_signal_stop()不管进程是否在孤儿进程。
- do_signal_stop()(这个函数后面有讲)是在什么情况下返回0表示没有停止的呢？
  1.SIGNAL_STOP_DEQUEUED没有设置时，2.在想开始一个新的组停止而进行解加锁时被其它的纯程收了
  另一个停止信号或SIGCONT时。若返回0就开始处理下一个信号，但无论返回什么，都不能往下执行了，
  往下执行的情况只有在处理方式是默认的且处理方式是dump或terminate.
- 若处理方式是默认的且处理方式是dump或terminate，那么设置PF_SIGNALED，原来PF_SIGNALED是这样
  用的。若是一个dump信号就调用do_coredump(),若不是就调用do_group_exit()
** static inline int handle_group_stop(void)
*** kernel/signal.c:
- 对于get_signal_to_deliver()来说，这个函数返回1表示不用处理任何的信号了。
- 这个函数的只有被get_signal_to_deliver()调用，调用这个函数时已经表明是在进行了组停止。
- 当发现tsk->signal->group_exit_task为current时，有这样的注释：Group stop is so we can do
  a core dump, We are the initiating thread, so get on with it.表明组停止已经结束了，就差
  current这个进程了。但为什么要正在进行组退出的时候才这样做呢？有什么关系吗？
- 若group_exit_task为current，无论是否是在进行组退出，都要处理信号，还要把group_exit_task清零。
- 若group_exit_task不为current，但current->signal->flags置了SIGNAL_GROUP_EXIT，表示current
  所在的进程组在进行组退出，且处录地组退出的那个进程不是current,这种情况下也要返回0表示要处
  理信号。所以可以得出只要线程组在进行组退出，那么就要处理信号，不管是否是在进行组停止.
- 若没有进行组退出且在进行组停止，那么就不能处理信号。
- 从代码可以看出，组停止是在这里处理的，这也是为什么这个函数叫handle_group_stop,从代码来看，
  正在进行组停止时处理停止信号很简单，就是把当前进程的退出代码(current->exit_code)设置为组
  的退出代码(current->signal->group_exit_code)，再设置进程为TASK_STOPPED。为什么要设置退出
  代码呢？停止跟退出代码有什么关系呢？
- 结合get_signal_to_deliver()来看，因为是在一开始在get_signal_to_deliver()调用
  handle_group_stop()的，所以若想进行一个组停止时，一旦把signal->group_stop_count设为不等于
  0，那么其它的线程就不会处理其它的信号了，因为是在get_signal_to_deliver()一开始调用
  handle_group_deliver()的。
- 因为处理了一个线程的停止信号，所以current->signal->group_stop_count要自减，若为0了就表明
  所有的线程都已进行了一次停止，所以要设置SIGNAL_STOP_STOPPED.
- 如果一个进程已经处理了自已停止，那么就调用finish_stop()
** static void finish_stop(int stop_count)
*** kernel/signal.c:
- 在这里参数也有可能会为负？这个函数是handle_group_stop()和do_signal_stop()调用的,而
  handle_group_stop()在被get_signal_to_deliver()调用之前已经验证了group_stop_count大于0，所
  以在handle_group_stop()里调用finish_stop()时一定可以保证stop_count参数不为负数，所以出现
  负数的情况应该是在do_signal_stop()里调用。
- 这个函调用do_notify_parent_cldstop()时都是用CLD_STOPPED的，且都是在stop_count(其实就是
  group_stop_count)不为正，所以可能看出CLD_STOPPED是在所有的线程都完成了停止之后才可以使用
  的吗？不是的，可以从handle_stop_signal()函数看出，在信号是SIGCONT且正在进行组停止
  （group_stop_count>0）的时候也会把CLD_STOPPPED传给do_notify_parent_cldstop()的，所以
  CLD_STOPPED这个信号的作用应该是组停止结束后就以CLD_STOPPED调用do_notify_parent_cldstop()
  通知父进程，不管是不是在停止过程中有被SIGCONT中断，若是中断了也算是组停止结束。
- 为什么要在group_stop_count为负数的时候给current->parent发通知呢?在被踪跟的情况下发还可以理解.
- group_stop_count为0表示组停止结束,所以给current->group_leader->real_parent发信号.不是
  current->group_leader->parent.
- 调用这个函数就一定会调度,因为调用了schedule().这个函数被handle_group_stop()调
  用,handle_group_stop()是被get_signal_to_deliver()在进行组停止时调用的.所以进行组停止的时
  候若其中一个线程在执行的时候,处理信号时就会调用get_signal_to_deliver() ->
  handle_group_stop()(没有进行组退出的情况) -> finish_stop() -> schedule().
- 等从schedule()切换回来的时候把current->exit_code给清掉了.exit_code有在
  handle_group_stop()被设置成current->signal->group_exit_code的,所以这个exit_code应该是给
  schedule()使用的.
** static void ptrace_stop(int exit_code, int nostop_code, siginfo_t *info)
*** kernel/signal.c:
- 这个函数主要功能就是想以CLD_TRAPPED调用do_notify_parent_cldstop()，通知跟踪进程被跟踪的进
  程已收到一个信号。
- 通知完之后马上调用schedule()进行切换。
- 若正在进行组停止，那么就要停止数减1，从被get_signal_to_deliver()的角度看，就是若处理一个
  非SIGKILL信号且被跟踪时且正在进行组退出，那么就要把group_stop_count自减1。虽然减1了，但是
  不一定会调用do_notify_parent_cldstop(),就算group_stop_count是0也不会以CLD_STOPPED调用
  do_notify_parent_cldstop().
- 要想以CLD_TRAPPED调用do_notify_parent_cldstop()还要先满足一大堆的乱七八糟的条件。不满足就
  不调用do_notify_parent_cldstop(),不进行进程的切换，不修改进程的执行状态。
- 把current->last_siginfo设为info和把current->exit_code设为exit_code可能是给
  do_notify_parent_cldstop()用的。若不能调用do_notify_parent_cldstop()那么
  current->exit_code要改成nostop_code.在get_signal_to_deliver()调用这个函数的时候exit_code
  参数和nostop_code参数都是信号编号。
- tsk->last_siginfo是给跟踪用的。
** static int do_signal_stop(int signr)
*** kernel/signal.c:
- 关于这里为什么要检查SIGNAL_STOP_DEQUEUED信号，上面有讲。
- 若进程所在的线程组正在进行一个组退出，那么就不用再启动下一个组退出了。
- 在进行组退出的时候，是不会发一个停止信号给所有线程的而是直接停掉它，所以在个函数里发现有
  停止信号不是因为有组停止所引起的。
- 在这里处理组停止的情况与handle_group_stop()函数里后面的类似，就是group_stop_count自减，为
  0就设置SIGNAL_STOP_STOPPED，把current->signal->group_exit_code赋给current->exit_code，设
  置set_current_state(TASK_STOPPED)。handle_group_stop()是只在get_signal_to_deliver()一开始
  处理信号时被调用.而handle_group_stop()在一开始是检查了有没有进行组退出。
- 若进程没有正在进行一个组退出，且线程组只有一个线程，那么也把current->exit_code和
  current->signal->group_exit_code赋予信号，为什么要这样做呢？不是不如果进程因一个信号的处
  理方式是停止的话都会这样设置exit_code和group_exit_code.也把进程状态设置为TASK_STOPPED，也
  设置SIGNAL_STOP_STOPPED.
- 若没有在进行组退出，且线程组只有这个线程，那么j说要开始一个新的组停止了。因为
  tasklist_lock程sighand->siglock要按顺序获得，所以要先放了siglock锁，所以在这个时间窗口里
  有可能产生一个SIGCONT信号和另一个停止信号，因为在收到一个SIGCONT的时候会设置
  SIGNAL_STOP_DEQUEUED，这个时候就要直接退出了。在这个函数里还要判断group_stop_count是否为
  0就是因为刚才解锁后重新上锁可能导致另一个线程开始了一个组停止，若是这样的话说把
  group_exit_code提出来并以current线程的名义给group_stop_count自减，否则就开始一个新的组停
  止，开始一个组停止时要把group_exit_code赋以signr,所以从这里可以看出进行一个组停止时
  group_exit_code是一个因它而停止的信号。接着把exit_code赋以了signr,所以一个进程若因一个组
  停止而停止，那么它的current->exit_code的值就是存放因它而停止的信号，从这个函数可以看出
  current->exit_code不一定与group_exit_code相同的吗？这里还要检查该线程是不是最后一个停止的
  线程是因为可能在之前的解锁再加锁的过程中产生了另一停止信号而使得开始了另一个组进停止并使
  得该线程是最后一个停止的线程。
** int fastcall do_signal(struct pt_regs *regs, sigset_t *oldset)
*** arch/i386/kernel/signal.c:
- 这个函数会调用get_signal_to_deliver(),返回非零表示有信号被deliver了。在
  get_signal_to_deliver()函数里可能会改变info和ka。若有信号被deliver,那么就调用
  handle_signal()
- do_signal()调用一次只处理一个信号，ULK：Notice how do_signal( ) returns after having
  handled a single signal. Other pending signals won't be considered until the next
  invocation of do_signal( ). This approach ensures that real-time signals will be dealt
  with in the proper order.
- 一开始要判断是否是一个系统调用，若是就要设置相应的重启方式。
- 关于如何在内核态和用户态之间切换ULK有说：A nonblocked signal is sent to a process. When
  an interrupt or exception occurs, the process switches into Kernel Mode. Right before
  returning to User Mode, the kernel executes the do_signal( ) function, which in turn
  handles the signal (by invoking handle_signal( )) and sets up the User Mode stack (by
  invoking setup_frame( ) or setup_rt_frame( )). When the process switches again to User
  Mode, it starts executing the signal handler, because the handler's starting address was
  forced into the program counter. When that function terminates, the return code placed
  on the User Mode stack by the setup_frame( ) or setup_rt_frame( ) function is
  executed. This code invokes the sigreturn( ) or the rt_sigreturn( ) system call; the
  corresponding service routines copy the hardware context of the normal program to the
  Kernel Mode stack and restore the User Mode stack back to its original state (by
  invoking restore_sigcontext( )). When the system call terminates, the normal program can
  thus resume its execution.
** asmlinkage long sys_kill(int pid, int sig)
*** kernel/signal.c:
- si_signo就是sig,si_code是SI_USER，si_pid是当前进程的tgid,si_uid是uid.si_pid是比较特殊的。
- 调用kill_something_info().
** static int kill_something_info(int sig, struct siginfo *info, int pid)
*** kernel/signal.c:
- ULK:pid = 0 ,The sig signal is sent to the thread group of the process whose PID is
  equal to pid .所以是以kill_pg_info()的方式调用的。
- ULK:pid = -1,The signal is sent to all processes, except swapper (PID 0), init (PID 1),
  and current.但是看代码时不是这样的，有一个这样的判断:p->tgid != current->tgid,所以
  current所在的线程组里的所有线程都不能接收信号。这样做是不是因为若给current的线程组发信号，
  最终可能会在该组中选择current来处理信号。
- ULK:pid < -1,he signal is sent to all thread groups of the processes in the process
  group -pid.
** asmlinkage long sys_tkill(int pid, int sig)
*** kernel/signal.c:
- 这个函数的si_pid也是赋以current->tgid,为什么是tgid呢？
- si_code是SI_TKILL
** asmlinkage long sys_tgkill(int tgid, int pid, int sig)
*** kernel/signal.c:
- ULK:The sys_tgkill( ) service routine performs exactly the same operations as sys_tkill( ),
  but also checks that the process being signaled actually belongs to the thread group
  tgid. This additional check solves a race condition that occurs when a signal is sent to
  a process that is being killed.与sys_tkill()的不同就是多了一个p->tgid == tgid的判断，
** static inline unsigned int task_timeslice(task_t *p)
*** kernel/sched.c:
- 这个函数是通过静态优先级来计算进程的时间片的
** static inline runqueue_t *task_rq_lock(task_t *p, unsigned long *flags)
*** kernel/sched.c:
- 这个函数三个目的：禁止本地变量，获取rq->lock，获取运行队列。但这三个操作要一致性。
** static inline runqueue_t *this_rq_lock(void)
*** kernel/sched.c:
- 这个函数不像task_rq_lock()要用循环使得一致性，因为这个
** static void dequeue_task(struct task_struct *p, prio_array_t *array)
*** kernel/sched.c:
- 把进程从array这个链表里删除，array可以是active或expires.
- 相关的东西有nr_active, run_list, array->queue, p->prio, array->bitmap.
** static void enqueue_task(struct task_struct *p, prio_array_t *array)
*** kernel/sched.c:
- 把一个进程插入array链表，不判断位图是否设置了，因为这会耗掉时间。
- 还会把进程的array设成array.
** static void requeue_task(struct task_struct *p, prio_array_t *array)
*** include/linux/list.h:
- 把一个进程从原来链表里删除再插到其它的链表。
** static inline void enqueue_task_head(struct task_struct *p, prio_array_t *array)
*** include/linux/list.h:
- 这个函数与enqueue_task_head()的差别是一个插入头一个插入尾。
** static int effective_prio(task_t *p)
*** kernel/sched.c:
- 根据静态优先级和bonuse来计算优先级。
** static inline void __activate_task(task_t *p, runqueue_t *rq)
*** kernel/sched.c:
- 调用enqueue_task()把进程插入active,并自增nr_running,nr_running与nr_active是不一样的。
** static inline void __activate_idle_task(task_t *p, runqueue_t *rq)
*** kernel/sched.c:
- 这个函数与__activate_task()的区别是调用enqueue_task_head()把进程插入到active
** static void activate_task(task_t *p, runqueue_t *rq, int local)
*** kernel/sched.c:
- 这个函数主要作用是更新进程的动态优先级(recalc_task_prio()),更新task->activated状态，更新
  task->timestamp,调用__activate_task()把进程插入运行队列的激活链表里.
- 这个函数被try_to_wake_up()和__migrate_task()调用.
- 要用sched_clock()来获取时间
- 若是多核的话且进程不在本地CPU的话就要结合timestamp_last_tick来调整时间。
** static void deactivate_task(struct task_struct *p, runqueue_t *rq)
*** kernel/sched.c:
- 就只是调用dequeue_task()把进程从运行链表里删除,不管是不是active链表还是expires链表.
- 这个函数没有activate_task()那么复杂.
- 因为task->timestamp是与插入队列有关的，但是与删除没有关系。所以不用管。
** static void resched_task(task_t *p)
*** kernel/sched.c:
- 还挻多函数调用它的。
- 这个函数有单多核版，单核版的只是调用set_tsk_need_resched(), 而多核版的话除了设置
  TIF_NEED_RESCHED之外还要给所在的CPU发一个调度中断。
** static int migrate_task(task_t *p, int dest_cpu, migration_req_t *req)
*** kernel/sched.c:
- 这个函数里有一段检查代码很奇怪，p->array为空了难道task_running()还有可能是真吗？如果条件
  为真的话就直接调用set_task_cpu()设置进程的cpu,task->thread_info->cpu.若条件为假，那么就用
  completion原语来实现进程的迁移。
 #+BEGIN_EXAMPLE
	if (!p->array && !task_running(rq, p)) {
 #+END_EXAMPLE
** void wait_task_inactive(task_t * p)
*** kernel/sched.c:
- 这个函数只被在跟踪时调用两次。
- 这个函数就是等待进程从运行队列里删掉，这个函数有做优化，就是若被等待的进程没有正在运行那
  么就调用yeild()让进程等待时间长一点。
** static inline unsigned long source_load(int cpu)
*** kernel/sched.c:
- 计算负载的方式就是rq->nr_running乘以一个常量。
** int fastcall wake_up_process(task_t * p)
*** kernel/sched.c:
- 就是调用try_to_wake_up(),唤醒所有在停止状态的进程，包括
  TASK_STOPPED,TASK_TRACED,TASK_INTERRUPTIBLE,TASK_UNINTERRUPTIBLE的进程,但不是同步唤醒。
** int fastcall wake_up_state(task_t *p, unsigned int state)
*** kernel/sched.c:
- 以state调用try_to_wake_up()
** void fastcall sched_fork(task_t *p)
*** kernel/sched.c:
- 只给copy_process()调用。
- 注释说了，这个函数是为一个新fork的进程做调度相关的设置。
- 注释有说，这个函数里把进程标为正在执行的，但是并没有实际插入到运行队列，这可以保证没有谁
  可以让它运行，就算是一个信号或其它的外部事件不能唤醒和把它插入到一个运行队列。
- 因为fork一个进程时，那个新的进程其实是有一个自旋锁的，所以要给thread_info->preempt_count
  补尝1。
- 上面有关于时间片父进程如何共享的说明。
- 因为如果current->time_slice的时间为1，那么分享之后的时间就是0了，这时要把它改回1(为什么要
  改回1呢？)并调用scheduler_tick()
** unsigned long nr_running(void)
*** kernel/sched.c:
- 计算所有的CPU的进程数的总和
** unsigned long nr_uninterruptible(void)
*** kernel/sched.c:
- 计算所有的CPU的不可中断的进程的总和
** unsigned long long nr_context_switches(void)
*** kernel/sched.c:
- 计算所有CPU的切换次数。
** unsigned long nr_iowait(void)
*** kernel/sched.c:
- 计算所有CPU的io等待次数。
** static int find_idlest_cpu(struct task_struct *p, int this_cpu, struct sched_domain *sd)
*** kernel/sched.c:
- 先找出p->cpus_allowed与sd->span的cpu的交集CPU中最低负载CPU，但这个CPU还不是找到的那个，还
  要和当前CPU用一个公式比较
** static void sched_migrate_task(task_t *p, int dest_cpu)
*** kernel/sched.c:
- 若目的CPU不是允许的CPU(cpus_allowed)就不能移。
- 移进程到一个CPU是用线程和completion实现的，migrate_task()里有init_completion(),这个函数有
  wait_for_completion()，migration 线程里应该有complete().migration线程就只有这种时候被用到
  吗？
** void sched_exec(void)
*** kernel/sched.c:
- 注释说了，执行sched_exec()的时候是平衡的时机，因为进程有最小的有效内存和cache覆盖面。
- 先找标有SD_BALANCE_EXEC的域,再从中用find_idlest_cpu()找出最不忙的cpu,找到了就调用
  sched_migrate_task().
** static int load_balance_newidle(int this_cpu, runqueue_t *this_rq, struct sched_domain *sd)
*** kernel/sched.c:
- 这个函数由idle_balance()调用idle_balance()又由schedule()调用.注释说了,当this_rq将成为
  idle时(NEWLY_IDLE)是被schedule()调用.
- 这个函数可以说是load_balance()的简化版,先用find_busiest_group()找出最忙的组,再用
  find_busiest_queue找出最忙的运行队列,最后调用move_tasks()移进程.
** static inline void idle_balance(int this_cpu, runqueue_t *this_rq)
*** kernel/sched.c:
- 注释说了,这个函数在this_rq快成为idle时由schedule()调用,所以要检查SD_BALANCE_NEWIDLE.
** static void active_load_balance(runqueue_t *busiest_rq, int busiest_cpu)
*** kernel/sched.c:
- 把busiest_rq里的进程移到一个idle CPU里。
- 注释说了,这个函数被migration线程调用,是在rq->active_balance设置时被调用.
- 因为SD_LOAD_BALANCE的标志有继承性,所以若有发现这个标志没有设置就不用再向更高的层去搜索了.
- 这个函数主要是三层循环，最外层是循环调度域的，次外层是循环一个调度里的所有组的，最内层是
  循环一个组内的所有的CPU的。
- 因为组间的CPU的交集可能不为空，所以要记录下以访问过的CPU（visited_cpus）
- 最主要的就是找到CPU之后调用move_tasks().
** static void rebalance_tick(int this_cpu, runqueue_t *this_rq, enum idle_type idle)
*** kernel/sched.c:
- 这个函数是遍历所有的域，不是单独某个域。
- 发现原来rq->cpu_load只在这里更新的，且更新的方法是用现在的负载和旧的负载求平均。
- 没有设置SD_LOAD_BALANCE就不用再做处理了。
- 这个函数最主要的就是调用load_balance()，调用load_balance()前要确定调用的时间间隔有没有到，
  就是sched_domain->balance_interval,但sched_domain->balance_interval不能直接使用，要结合
  SCHED_IDLE、sched_domain->busy_factor、sched_domain->last_balance. 若有调用
  load_balance()那么就要更新sched_domain->last_balance.
** asmlinkage void __sched preempt_schedule(void)
*** kernel/sched.c:
- 这个函数里会判断current是否禁止抢占或有禁止中断，若是返回不能调用schedule()
- 在调用schedule()之前会用PREEMPT_ACTIVE来禁止抢占。
** void __devinit init_idle(task_t *idle, int cpu)
*** kernel/sched.c:
- 初始化idle进程，可以看出一个idle只允许在一个CPU上执行，把运行队列的idle赋以了idle进程。
** int set_cpus_allowed(task_t *p, cpumask_t new_mask)
*** kernel/sched.c:
- 这个函数的主要功能就是设置进程的cpus_allowed
- 设置前要注意新的CPU mask与cpu_online_map有没有交集
- 若发现进程当前所在的CPU不包含在新设置的mask里的时候就要调用migrate_task()把进程移过去。
** static void __migrate_task(struct task_struct *p, int src_cpu, int dest_cpu)
*** kernel/sched.c:
- 把进程从src_cpu中移到dest_cpu里去
- 移之前要判断进程是不是被移走了，是否是不允许移到那个CPU
- 如果之前进程已经是插入到一个运行链表里，那么就要重新计算task->timestamp了，还要把进程从原
  来的运行链表删除再插入到新的运行链表.
- 移了之后若发现进程可以抢占current，那么就调用resched_task()
** static int migration_thread(void * data)
*** kernel/sched.c:
- 这个函数是一个线程函数。处理本地进程所要求的迁移到其它CPU的任务.
- 这个函数处理migration_req_t迁移进程的任务链表,若都处理完了就调度。
- 若发现CPU offline了，那么就不断地调用schedule()
- 若运行队列要求迁移进程(rq->active_balance)，那么就调用active_load_balance(),把rq运行队列
  的进程移到域的其它idle cpu里.除了会调用active_load_balance()之外，还会处理迁移进程任务链表里的任务。
- 迁移任务有两种一个是REQ_MOVE_TASK，这个会调用__migrate_task(),还有一个类型是
  REQ_SET_DOMAIN，这个只是设置运行队列里的调度域。
** static int migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)
*** kernel/sched.c:
- 迁移进程是以SCHED_FIFO的调度策略执行的,且是以实时进程的优先级运行。
** void __devinit cpu_attach_domain(struct sched_domain *sd, int cpu)
*** kernel/sched.c:
- 为什么是本地CPU的时候就不用任务链表来改调度域呢？
** #define pte_none(x)		(!(x).pte_low)
*** include/asm-i386/pgtable-2level.h:
** static inline int pte_none(pte_t pte)
*** include/asm-i386/pgtable-3level.h:
- 这个要比较高低位。
- 只有在打开PAE时才有高低之分。所以在x86里使用3级的页表的时候就是打开了PAE
- 对于32位页表项（没有打开PAE）是有12位的标志位，剩下的20位是表示一个页框的物地址，要记住这
  个页表项的20位地址是如何和线性地址结合来找到一个页框的。
- 要知道page table(pte)和page offset是不一样的
- 所谓的扩展分页就是offset从12位(4K)变成了22位(4M)
** #define pmd_clear(xp)	do { set_pmd(xp, __pmd(0)); } while (0)
*** include/asm-i386/pgtable.h:
** #define set_pmd(pmdptr, pmdval) (*(pmdptr) = (pmdval))
*** include/asm-i386/pgtable-2level.h:
** #define pte_same(a, b)		((a).pte_low == (b).pte_low)
*** include/asm-i386/pgtable-2level.h:
** #define	pmd_bad(x)	((pmd_val(x) & (~PAGE_MASK & ~_PAGE_USER)) != _KERNPG_TABLE)
*** include/asm-i386/pgtable.h:
- 
** #define PAGE_MASK	(~(PAGE_SIZE-1))
*** include/asm-i386/page.h:
- 所以有效位是0的，无效位是1的。
** #define _KERNPG_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED | _PAGE_DIRTY)
*** include/asm-i386/pgtable.h:
- 当前在内存，可读写，已被访问，已被修改的页就是_KERNPG_TABLE了
** #define	pmd_bad(x)	((pmd_val(x) & (~PAGE_MASK & ~_PAGE_USER)) != _KERNPG_TABLE)
*** include/asm-i386/pgtable.h:
- (~PAGE_MASK & ~_PAGE_USER)根据离散里的De Morgan’s laws可以改成~(PAGE_MASK | _PAGE_USER)
  这样就好看多了
** static inline int pte_file(pte_t pte)		{ return (pte).pte_low & _PAGE_FILE; }
*** include/asm-i386/pgtable.h:
- ULK:Reads the Dirty flag (when the Present flag is cleared and the Dirty flag is set,
  the page belongs to a non-linear disk file mapping;
** #define mk_pte_huge(entry) ((entry).pte_low |= _PAGE_PRESENT | _PAGE_PSE)
*** include/asm-i386/pgtable.h:
- Sets the Page Size and Present flags of a Page Table entry
** static inline pte_t pte_wrprotect(pte_t pte)	{ (pte).pte_low &= ~_PAGE_RW; return pte; }
*** include/asm-i386/pgtable.h:
** static inline pte_t pte_rdprotect(pte_t pte)	{ (pte).pte_low &= ~_PAGE_USER; return pte; }
*** include/asm-i386/pgtable.h:
** static inline pte_t pte_exprotect(pte_t pte)	{ (pte).pte_low &= ~_PAGE_USER; return pte; }
*** include/asm-i386/pgtable.h:
** #define pgd_index(address) (((address) >> PGDIR_SHIFT) & (PTRS_PER_PGD-1))
*** include/asm-i386/pgtable.h:
- 找到页框所对应的global page table的下标.
** #define pgd_offset(mm, address) ((mm)->pgd+pgd_index(address))
*** include/asm-i386/pgtable.h:
- 从这里可以看出页表和内存描述符结合起来了。
- 产生的是address对应的pgd的页表项的所在的线性地址。
** #define pgd_offset_k(address) pgd_offset(&init_mm, address)
*** include/asm-i386/pgtable.h:
- 其实就是pgd_offset(init_mm, address)
** #define pgd_page(pgd)				(pud_page((pud_t){ pgd }))
*** include/asm-generic/pgtable-nopud.h:
- ULK:Yields the page descriptor address of the page frame containing the Page Upper
  Directory referred to by the Page Global Directory entry pgd . In a two- or three-level
  paging system, this macro is equivalent to pud_page() applied to the folded Page Upper
  Directory entry.
- pgd的页表项是pgd_t类型的还是pud_t类型的。
** #define pmd_offset(pud, address) ((pmd_t *) pud_page(*(pud)) + pmd_index(address))
*** include/asm-i386/pgtable-3level.h:
- pud_page()返回的是pud的页表项,也可以看出pud的页表项是pmd_t类型的。
** #define mk_pte(page, pgprot)	pfn_pte(page_to_pfn(page), (pgprot))
*** include/asm-i386/pgtable.h:
- 用页描述符的线性地址和指定的权限来生成一个pte.
** #define page_to_pfn(page)	((unsigned long)((page) - mem_map))
*** include/asm-i386/page.h:
- 这个函数的作用是把某个页的页描述符转为所对应的页号。
- ULK:All page descriptors are stored in the mem_map array.所以page是某个页框的的页描述符的
  物理地址。
** #define pfn_pte(pfn, prot)	__pte(((pfn) << PAGE_SHIFT) | pgprot_val(prot))
*** include/asm-i386/pgtable-2level.h:
- 为什么无论是pfn_pte()还是pfn_pmd()，都要移PAGE_SHIFT位呢？因为页表项的最高20位表示一个页
  框的物理地址，剩下的12位就是PAGE_SHIFT的大小，这个在设计时就要求一定是一致的，因为
  PAGE_SHIFT是12，所以至少要32-12位才可以表示所有的页框。
** #define pmd_page(pmd) (pfn_to_page(pmd_val(pmd) >> PAGE_SHIFT))
*** include/asm-i386/pgtable-2level.h:
- 根据参数pmd里的20位页框号来得相应的页框的页描述符的地址。至于这个页描述符的地址是线性地址
  还是物理地址，就要看mem_map是线性的还是物理的了。但根椐下面的pmd_page_kernel()使用
  了__va()可以看出是线性地址。所以返回的是页描述符的线性地址。
** #define pfn_to_page(pfn)	(mem_map + (pfn))
*** include/asm-i386/page.h:
- 根据页框号获得页描述地址。
** #define pmd_page_kernel(pmd) ((unsigned long) __va(pmd_val(pmd) & PAGE_MASK))
*** include/asm-i386/pgtable-2level.h:
- 注意与pmd_page()的不同，为什么要分pmd_page()和pmd_page_kernel()呢？
  http://bbs.csdn.net/topics/330112122 所以这个函数是获得内存小于896时，获得内核页表的.
- 那么是不是pmd_page()是给内存大于896的时候用的,而pmd_page_kernel()是给小于896内存用的呢？
- pgd/pud/pmd/pte_index/offset/page, mk_pte(), pte_offset_kernel(),pte_offset_map()
** #define __va(x)			((void *)((unsigned long)(x)+PAGE_OFFSET))
*** include/asm-i386/page.h:
- 就只是加上一个PAGE_OFFSET，那么x是物理地址呢还是线性地址呢？
- ULK:The __pa macro is used to convert a linear address starting from PAGE_OFFSET to the
  corresponding physical address, while the _ _va macro does the reverse.所以从这句话可以看
  出pmd_t的高20位的地址表示的是物理地址,ULK也是这么说的。__pa()和__va()是给内存小于896时用的。
** #define pte_offset_kernel(dir, address) ((pte_t *) pmd_page_kernel(*(dir)) +  pte_index(address))
*** include/asm-i386/pgtable.h:
- ULK:Yields the linear address of the Page Table that corresponds to the linear address
  addr mapped by the Page Middle Directory dir . Used only on the master kernel page
  tables.
- 这个宏用了pmd_page_kernel(),说明pte_offset_kernel()也是用于内存小于896的。
- ULK:The kernel maintains a set of page tables for its own use, rooted at a so-called
  master kernel Page Global Directory. After system initialization, this set of page
  tables is never directly used by any process or kernel thread; rather, the highest
  entries of the master kernel Page Global Directory are the reference model for the
  corresponding entries of the Page Global Directories of every regular process in the
  system.所以是不是说明了pmd_page_kernel()和pte_offset_kernel()这些使用PAGE_OFFSET的函数就
  是访问主内核页表的呢？
- ULK:If PAE is enabled, the kernel uses three-level paging. When the kernel creates a new
  Page Global Directory, it also allocates the four corresponding Page Middle Directories;
  these are freed only when the parent Page Global Directory is released.这句话是不是也暗含
  着如果没有打开PAE的话是不使用3级页表的呢？
- ULK:When two or three-level paging is used, the Page Upper Directory entry is always
  mapped as a single entry within the Page Global Directory.
** pgd_t *pgd_alloc(struct mm_struct *mm)
*** arch/i386/mm/pgtable.c:
- 这个函数分配一个pgd并初始化。
- 为什么PTRS_PER_PMD为1时不用初始化最后那USER_PTRS_PER_PGD个项呢？因为PTRS_PER_PMD == 1表示
  不使用PMD，所以不用给pgd的项从pmd_cache那分配pmd,那么PTRS_PER_PMD == 1时是不是要给pgd的项
  分配pte呢？
- 从以下代码可以看出PAGE_OFFSET的好处了，因为页表项的高20位是物理地址，而从pmd_cache里分配
  的内存是在内核范围内的，所以可以通过__pa(pmd)来获得pmd的物理地址，如果不使用PAGE_OFFSET的
  话而使用普通的虚拟地址那么就很麻烦。
 #+BEGIN_EXAMPLE
		set_pgd(&pgd[i], __pgd(1 + __pa(pmd)));
 #+END_EXAMPLE
** void pgd_free(pgd_t *pgd)
*** arch/i386/mm/pgtable.c:
- 有注释说打开PAE的情况下，用户的pgd项在使用前会被填充重写，就是最后的那1G的空间。所以要释
  放pgd时要先释放它们。那除了这些项之外的其它项在哪里被释放了呢？
** static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
*** include/linux/mm.h:
- 这个是在有4级页表的时候才使用的.
- 如果pgd参数所指的pgd项为空的话就分配pud，
** static inline int pgd_none(pgd_t pgd)		{ return 0; }
*** include/asm-generic/pgtable-nopud.h:
- 相应的项是0那么就返回1，就是说如果对应的项是空的那么就返回1，就是说参数pgd为0的话就返回1。
  pgd_none的检查对象是pgd.
** #define pud_none(pud)				0
*** include/asm-i386/pgtable-3level.h:
- pud为0返回1。
** #define pmd_alloc_one(mm, address)		NULL
*** include/asm-generic/pgtable-nopmd.h:
** #define pmd_alloc_one(mm, addr)		({ BUG(); ((pmd_t *)2); })
*** include/asm-i386/pgalloc.h:
- 这是不打开PAE的时候才可以用的。为什么是2的指针呢？
** static inline pmd_t *pmd_alloc_one (struct mm_struct *mm, unsigned long addr)
*** include/asm-x86_64/pgalloc.h:
- 这个就会调用get_zeroed_page()了，所以可以看出是分配一个页的做为一个pmd页表的。
- 返回的类型是pmd_t的指针,说明一个pmd页表里的项的类型是pmd_t的。
** pmd_t fastcall *__pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
*** mm/memory.c:
- 这个函数只有一个实现，是不未定义__PAGETABLE_PMD_FOLDED时才可以调用的。这个
** static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
*** include/linux/mm.h:
- 这个是在不定义__ARCH_HAS_4LEVER_HACK时才可以调用的。
- 这个函数调用了__pmd_alloc()
** pte_t fastcall *pte_alloc_map(struct mm_struct *mm, pmd_t *pmd, unsigned long address)
*** mm/memory.c:
- 这个函数是用于从kmap那里分配一个页表.
- 先用pmd_present()来判断Present标志看pmd所指向的page middle页表项所描述的page table是否存
  在。若存在就没有必要再分配了，但是如果被回收了又怎么处理呢？难道被回收ULK：If the entry of a Page
  Table or Page Directory needed to perform an address translation has the Present flag
  cleared, the paging unit stores the linear address in a control register named cr2 and
  generates exception 14: the Page Fault exception.原来访问一个不存在页时分页单元是这样来处
  理来发出Page Fault。
- mm->nr_ptes只在这里增加，说明要给进程分一个页表就要通过这个函数。
- 在ULK里没有介绍struct page_state结构体,这个结构体被声明成了一个per-cpu变量,所以会对每个
  CPU的所有页的状态进行跟踪,但是页不会属于特定的CPU,那么这种跟踪有什么用呢?而这个
  pte_alloc_map()是用来分配一个page table页表的,所以每分配一个页表就会増加
  nr_page_table_pages.这就可以通过把所有CPU的这个值加起来得到.
- pmd_populate(mm, pmd, new)就是把一个值赋给pmd,mm没有用到,对于new是由alloc_pages()分配的，
  应该是一个页描述符的虚拟地址，但是可以通过page_to_pfn()来找到它的页框号。对于
  pmd_populate()是用_PAGE_TABLE作为它的标志，就是(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |
  _PAGE_ACCESSED | _PAGE_DIRTY)，所以这个是使用了_PAGE_USER标志，也就是说使用
  pte_alloc_map()函数分配的页表是给用户空间使用的。
- 最后还调用了pte_offset_map()，就是调用kmap_atomic()(如果没有使用高端内存就使用
  page_address())，所以就是作为高端内存映射的。
** struct page *pte_alloc_one(struct mm_struct *mm, unsigned long address)
*** arch/i386/mm/pgtable.c:
- 这个函数只有pte_alloc_map()调用。
- 这个函数是分配一个页作为一个page table.
- 只是调用了alloc_pages(),用__GFP_HIGHMEM表示可以在高端内存分配，本应该这样的，因为页表是属
  于进程的。用了__GFP_REPEAT表示可能会休眠。用了__GFP_ZERO表示会清零。
** pte_t fastcall * pte_alloc_kernel(struct mm_struct *mm, pmd_t *pmd, unsigned long address)
*** mm/memory.c:
- 这个与pte_alloc_map()不同了，不是通过高端内存来分配,所以调用了pte_alloc_one_kernel()
** pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)
*** arch/i386/mm/pgtable.c:
- 使用了__get_free_page()来获取一个页表，所以不能分配高端内存，这也是因为内核不能访问高端内
  存.所以这个函数返回的内存是内核可以访问的。
** #define pmd_populate_kernel(mm, pmd, pte) set_pmd(pmd, __pmd(_PAGE_TABLE + __pa(pte)))
*** include/asm-i386/pgalloc.h:
- 这个函数被pte_alloc_kernel()调用
- 这个函数用来设置pmd项的值的。
- 与pmd_populate()的主要的不同是物理地址的计算不同了，因为是低端地址，所以可以用__pa()来获
  取物理地址。
** static inline void pte_free(struct page *pte)
*** include/asm-i386/pgalloc.h:
- 这个是调用__free_page()的，那么被释放的内存应该是不分高低端内存的。
** static inline void pte_free_kernel(pte_t *pte)
*** include/asm-i386/pgalloc.h:
- 这个函数调用了free_page(),转而调用free_pages(),转而调用__free_pages(),里面有一个用
  virt_to_page()的调用
** #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
*** include/asm-i386/page.h:
- 这个宏的意思就是把一个地址转为struct page *的地址, 而被转的地址是一个低端地址.
** void __init paging_init(void)
*** arch/i386/mm/init.c:
- 这个函数是用来初始化主内核全局页表的，主要就是调用pagetable_init()
** static void __init pagetable_init (void)
*** arch/i386/mm/init.c:
- 为什么要在打开PAE的情况下要把页表项指向empty_zoro_page呢?
- cpu_has_pse检测pse,是从boot_cpu_data那里来的,转而从new_cpu_data那里来的,有注释这样说:cpu
  data as detected by the assembly code in head.S.若设置这个标志那么就设置cr4寄存器的
  PSE位.pse的全称是page size extern.
- 与cpu_has_pse相似,有一个cpu_has_pge,但是为什么会设置_PAGE_GLOBAL而不设置_PAGE_GLOBAL呢?
- 然后调用kernel_physical_mapping_init()
** static void __init kernel_physical_mapping_init(pgd_t *pgd_base)
*** arch/i386/mm/init.c:
- 这个函数会把主内核页表里的全局页表从虚拟地址PAGE_OFFSET开始的页表项到最后一个页表项都分配
  相应的pud,pmd和pte(只要存在相应的级页表存在)。
- 注释这样说它的作用: This maps the physical memory to kernel virtual address space, a
  total of max_low_pfn pages, by creating page tables starting from address PAGE_OFFSET.
- max_low_pfn : Page frame number of the last page frame directly mapped by the kernel
  (low memory)
- 因为要映射的线性地址是从PAGE_OFFSET开始的,所以要用pgd_index()来找到对应的项的下
- 为什么会出现pfn不小于max_low_pfn后再continue的情况呢？在max_low_pfn小于1G(896M)的时或在使
  用3级页表时(因为只要1个pgd页表的项就可以表示1G了)，但是因为还要给后面的pgd页表项分配pmd页
  表，所以还要continue来调用one_md_table_init().
- 如果在80x86（32bit）上使用扩展页表，那么只使用一级一页表，就是pgd,因为offset要用22位才可
  以达到4M的页框，如果80x86上使用扩展页表且打开PAE,那么页框是2M的，使用的是2级页表外加一个
  PDPT。
- 这个is_kernel_text()在虚拟地址是大于PAGE_OFFSET且小于__init_end的时候才返回1。
** static pmd_t * __init one_md_table_init(pgd_t *pgd)
*** arch/i386/mm/init.c:
- 在这里可以看出对于打开了PAE的32位x86来说是使用了3级页表的,而不打开PAE的32位x86来说是使用
  了2级页表.所以在打开了PAE的时候这个函数里会分一个pmd页表, 而使用3级页表的时候的
  pud_offset()是直接把参数pgd转换pud_t类型后返回的.在不打开PAE的时候因为pud_offset()和
  pmd_offset()都是返回参数的,所以也不用分配什么页表,所以到最后整个函数返回的就是参数pgd.所
  以在调用这个函数的kernel_physical_mapping_init()里的最外层的for循环的pmd在使用2级页表的时
  候就是pgd的值
** static pte_t * __init one_page_table_init(pmd_t *pmd)
*** arch/i386/mm/init.c:
- pmd_page_kernel(pmd)函数返回pmd页表项pmd参数所对于page talbe的页表的页的虚拟地址。
- pte_offset_kernel(pmd, 0)的意思是从pmd页表项pmd所对应的page talbe页表里找出第二个参数所对
  应的pte页表项。
- 总结一下...page...类的函数返回的是参数所对应的页表项的下一级页表的页框；...offset...类的
  函数返回的是把第一个参数作为页表项找出下一级页表的页框(使用...page...)后再用第二参数作为
  虚拟地址从页表中找到对应的页表项(使用...index()函数)。
** static void __init page_table_range_init (unsigned long start, unsigned long end, pgd_t *pgd_base)
*** arch/i386/mm/init.c:
- 以参数pgd_base为全局页表的首地址，把start到end之间的虚拟地址都分配相应的pmd页表和pte页表。
- 如果没有使用pud这一级页表，那么PUD_SHIFT等于PGDIR_SHIFT，又如果没有使用pmd这一级页表，那
  么PMD_SHIFT等于PUD_SHIFT。所以在这个函数里如果一共使用了2级页表，那么vaddr += PMD_SIZE就
  是vaddr += PGDIR_SIZE.
** static void __init permanent_kmaps_init(pgd_t *pgd_base)
*** arch/i386/mm/init.c:
- 这个函数用来初始化(给所有的永久映射虚拟地址分配页表和页框(就是调用了
  page_table_range_init))永久映射页表的。最后把基虚拟地址映射的页框赋给了pkmap_page_table.
- 先是调用page_table_range_init()来把从PKMAP_BASE开始长度为PAGE_SIZE*LAST_PKMAP的虚拟地址的
  页表全分配了，再连续调用pud_offset(),pmd_offset(),pte_offset_kernel()来找出PKMAP_BASE地址
  对应的
- 永久映射使用的页表是主内核的页表(swapper_pg_dir)，只使用一个pte页表，所以大小就是一个pte
  页表所能表示的范围。
- 永久映射也作为内核映射高端内存的其中一种机制。
** void zap_low_mappings (void)
*** arch/i386/mm/init.c:
- 为什么要保存原来的swapper_pg_dir页表保存在swsusp_pg_dir呢？
- 把swapper_pg_dir里从0到USER_PTRS_PER_PGD的所有页表项那清零了，所以到最后主内核页表的前
  USER_PTRS_PER_PGD都是空的。




- 内核可以采用三种不同的机制将页框映射到高端内存；分别叫做永久内核映射、临时内核映射以及非
  连续内存分配。
- http://www.linuxidc.com/Linux/2012-02/53457.htm
- 永久内核映射允许内核建立高端页框到内核地址空间的长期映射。使用主内核页表中一个专门的页表，
  其地址存放在变量pkmap_page_table中，那是不是说明了永久内核映射所能映射的范围是2的22次方的
  大小呢？（如果Page Table是10位，offset是12位）。页表中的表项数由LAST_PKMAP宏产生。
- pkmap_count数组包含LAST_PKMAP个计数器，pkmap_page_table页表中的每一项都有一个。
- 这里由定义可以看出永久内存映射为固定映射下面的4M/2M空间(用了PMD_MASK), 注意是在FIXADDR_BOOT_START处地址减的
 #+BEGIN_EXAMPLE
#define PKMAP_BASE ((FIXADDR_BOOT_START - PAGE_SIZE * (LAST_PKMAP + 1)) \   
            & PMD_MASK)
 #+END_EXAMPLE
- 高端映射区逻辑页面的分配结构用分配表(pkmap_count)来描述，它有1024项，对应于映射区内不同的
  逻辑页面。当分配项的值等于0时为自由项，等于1时为缓冲项，大于1时为映射项。映射页面的分配基
  于分配表的扫描，当所有的自由项都用完时，系统将清除所有的缓冲项，如果连缓冲项都用完时，系
  统将进入等待状态。
- 为了记录高端内存页框与永久内核映射包含的线性地址之间的联系，内核使用了
  page_address_htable散列表。该表包含一个page_address_map数据结构，用于为高端内存中的每一个
  页框进行当前映射。而该数据结构还包含一个指向页描述符的指针和分配给该页框的线性地址。
- page_address()函数返回页框对应的线性地址
- 有注释这样说：
 #+BEGIN_EXAMPLE
/*
 * Ordering is:
 *
 * FIXADDR_TOP
 * 			fixed_addresses
 * FIXADDR_START
 * 			temp fixed addresses
 * FIXADDR_BOOT_START
 * 			Persistent kmap area
 * PKMAP_BASE
 * VMALLOC_END
 * 			Vmalloc area
 * VMALLOC_START
 * high_memory
 */
 #+END_EXAMPLE

** void *kmap_atomic(struct page *page, enum km_type type)
*** arch/i386/mm/highmem.c:
- 这个函数用来映射临时内存映射(fix map)的.
- 为什么要禁止抢占呢？有注释说:even !CONFIG_PREEMPT needs this, for in_atomic in
  do_page_fault
- 对于每个类型的内存，不同的CPU都是映射到相同的内存，为什么要这样做呢？
** #define __fix_to_virt(x)	(FIXADDR_TOP - ((x) << PAGE_SHIFT))
*** include/asm-i386/fixmap.h:
- 从这个函数可以看出，参数x是一个km_type的一个下标，一个下标表示一个页框（因为用了
  PAGE_SHIFT），因为用减法，所以可以看出下标越大那么它的虚拟地址越小，
- FIXADDR_TOP是一个宏，所以是固定的，是0xfffff000，因为大于0xc0000000，所以是在内核虚拟地址
  内的。但是为什么不是0xffffffff呢？要把最后的4k的大小的虚拟内存出来呢？
- 这个宏在ULK里的实现时使用了编译器的特点做到了检查参数。
** static void set_pte_pfn(unsigned long vaddr, unsigned long pfn, pgprot_t flags)
*** arch/i386/mm/pgtable.c:
- 这个函数的作用就是通过vaddr找到相应的page talbe页表项，然后再用参数页框号pfn和标志初始化
  找到的页表项。
** void __set_fixmap (enum fixed_addresses idx, unsigned long phys, pgprot_t flags)
*** arch/i386/mm/pgtable.c:
- 这个用idx通过__set_fixmap()找到相应的虚拟地址，找到之后再用set_pte_pfn()来设置找到的虚拟
  地址所对应的Page Talbe页表项。
** void *page_address(struct page *page)
*** mm/highmem.c:
- ULK：returns the linear address associated with the page frame, or NULL if the page
  frame is in high memory and is not mapped.
- 这个函数只由内核用来查找一个内核使用的线性地址所对应的页框的页描述符。
- 通过查页描述符的PG_highmem来判断该页是否在高端内存里。
- 如果是低端内存那么就调用lowmem_page_address()使用PAGE_SHIFT,PAGE_OFFSET,_va()这些东西找到
  虚拟地址。
- 虽然是内核使用的，但内核也可以使用高端内存，因为有内核有3种方式可以将内核虚拟地址映射到高
  端内存。但是这个函数在找高端内存时只能找永久映射部分的，那么其它的两种方式就找不到了吗？
- 如果是高端内存，那么就用page_slot()找到该页对应的散列项，找到之后再对找到散列项链表里的所
  有结点进行遍历找到相应的结点，找到结点之后就返回结点里的virtual成员，这个成员就是要找的虚
  拟地址，结点还有一个page成员表示该结点对应哪个页描述地址。
** void fastcall *kmap_high(struct page *page)
*** mm/highmem.c:
- 这个函数的作用就是把参数page指向的高端内存页框映射到永久虚拟地址里去。
- 先用page_address()来找出page所指向的虚拟地址，如果这个地址没有映射，那么就会返回0，每次调
  用这个函数都会把这个页指向的虚拟地址的使用计数加1，如果小于2，那么就BUG()。如果为0，那么
  就说明这个页没有被映射，那么就调用map_new_virtual(),
** static inline unsigned long map_new_virtual(struct page *page)
*** mm/highmem.c:
- 这个函数的作用就是为参数page从永久映射区里找一个线性地址来关连起来，并设置相应的页表项。
- last_pkmap_nr是用来保存上一次调用这个函数时找到的合适的虚拟地址的,累加时使用
  LAST_PKMAP_MASK相与，所以可以自动处理回绕。
- 如果last_pkmap_nr为0表示回绕了，可以刷新一下不使用的页表项了，用flush_all_zero_pkmaps()刷
  新
- 如果LAST_PKMAP个项都扫描过了,那么就要休眠一下等其它的进程释放内存,这也说明这个函数会休眠,不
  能作原子操作.
- 等待释放内存有一个专门等待队列叫pkmap_map_wait,把等待对象插入之后就调度,调度回来就把对象
  从等待队列里删除,这时还要检查是不是对象又被分配出去了,如果是又被分配出去的话就不用再等待
  了而是把现在的虚拟地址返回,为什么不用再等待了,难道再次被其它进程分配的就是想要的吗?的确是
  我们想要的因为调用这个函数就是想把一个页框找一个永久映射的虚拟地址把它给关联起来,所以调用
  page_address()时发现这个页框已有虚拟地址与它关联,那么就返回这个地址就可以了.
- 如果休眠回来发现页框没有被映射,那么就再用原来的步骤扫描一次.
- 若找到了相应的虚拟地址,就用页框号和相应的kmap_prot标志初始化页表项,把在pkmap_count相应的
  项设为1,再调用set_page_address()在相应的散列表项里加上这个页框相应的结点.
** static void flush_all_zero_pkmaps(void)
*** mm/highmem.c:
- 会用循环遍历所有的永久映射页表项来清除不使用的页表项.
- 页表项对应的pkmap_count项不为1表示还使用,为1表示没有使用了.为1时就设回0.
- 用pte_clear()直接把对应的页表项清零就行了.
** void set_page_address(struct page *page, void *virtual)
*** mm/highmem.c:
- 这个函数用于永久内存映射的.ULK:insert a new element in the page_address_htable hash
  table
- 有个一叫page_address_pool的内存池,是struct page_address_map的内存池
- 如果参数virtual不为空,那么就从page_address_pool里取一个struct page_address_map,然后用参数
  page和virtual初始化struct page_address_map,最后把struct page_address_map加到散列表里去.
- 如果参数virtual为空,那就以参数page为标准从相应的散列表里找到相应的结点struct
  page_address_map,找到了之后就从散列表里删除这个结点,再把这个结点加到page_address_pool内存
  池里,但是没有把删除的结点清零.
** void fastcall kunmap_high(struct page *page)
*** mm/highmem.c:
- 先用page_address()和参数page找到相应的虚拟地址,再用找到虚拟地址和PKMAP_NR()找到对应的下
  标,再用找到的下标和pkmap_count数组找到使用计数减1,为1就会调用witqueue_active()看
  pkmap_map_wait队列是否为空,若不为空,就调用wake_up()把pkmap_map_wait把唤醒队列里的进程.

** void *kmap_atomic(struct page *page, enum km_type type)
*** arch/i386/mm/highmem.c:
- 临时映射(fix-map)不是把某个页框一直映射到一个页表项来达到不休眠的,而是有一个内核模块使用
  来达到的,所以不使用时被映射的页框会被释放.
- 之前对enum fixed_address的理解有错误,以为只对高端内存有用,原来还有其它的使用,就是除了
  CONFIG_HIGHMEM,还有CONFIG_ACPI_BOOT,CONFIG_X86_IO_APIC这类的东西.而enum km_type里的成员才
  是一个成员对应一个模块的,那么enum fixed_address里的所有可能的成员的地址的大小是不是是
  128M呢?如果是这样子就有疑问了,高端内存只有一个内核模块使用吗?现在又明白一点了,不是高端内
  存只有一个内核模块使用,而是高端内存有多个内核模块使用,要想知道有谁使用,不是看enum
  fixed_address,而是看enum km_type,km_type里面的东西才是说明有哪些内核模块使用的,如
  KM_USER0,KM_PTE0,KM_IRQ0等等.


** static void __init kmap_init(void)
*** arch/i386/mm/init.c:
- 这个函数被paging_init()调用,paging_init()被setup_arch()调用,在paging_init()里是先调用
  pagetable_init()再调用kmap_init()的.
- 调用完pagetable_init()之后,fix-mapping的线性地址(对于特定的模块已经是固定的了)已经指向了
  特定的Page Table.所以以虚拟地址调用kmap_get_fixmap_pte()(用pte_offset_kernel()实现)返回就
  是对应的页表项了.
- kmap_pte指向开头的Page Table页表项,这个在kmap_atomic()里使用,kmap_prot为所有fix-mapping页
  表项的权限,这个也在kmap_atomic()里使用.这个函数就只是初始化这两个变量而已.而相应的页表项
  在paging_init()里被初始化了.
** void *kmap_atomic(struct page *page, enum km_type type)
*** arch/i386/mm/highmem.c:
- 这个函数为什么要禁止抢占呢?在这里禁止抢占而在kunmap_atomic()使能抢占.
- 因为fix-mapping的页表paging_init()里被初始化,所以在这个函数里只要把页框号填到相应的页表项
  就可以了,而不用分配页表项,所以这个函数执行起来也很快.

- mem_map包含了所有的页框,而每个zone包含了其中的一段,zone结构体里的zone_mem_map指向了所包含
  的那段的开头的下标,size表示包含段的大小.free_area是一个结构体,而zone结构体里的free_area成
  员是一个free_area结构体数组.
- zone结构体里的free_area数组包含了11个不同页大小的链表.页表描述符的lru把链表里的页描述符连
  起来.
- struct free_zone结构体里的free_list成员是页描述符的链表头.
- 页描述符的private表明了该页被链在哪个free_area元素下,就是下标.
- 从pte_offset_map()的实现来看，kmap_atomic()返回的是一个pte页表的起始地址,因为传给
  kmap_atomic()的page是pmd_page()返回的地址。
** static struct page *__rmqueue(struct zone *zone, unsigned int order)
*** mm/page_alloc.c:
- 这个函数从伙伴系统里分配页.
- 如果对应order的那个链表没有结点了的话那么就要从其它的更大的order的链表里拆分了.拆分之后的
  合并在expand()函数里处理.
- 因为是在一组连续的页框里的第一个页框设置了页描述符的private成员为order而其它的页没有设置,所
  以要把找到的连续的页框的第一个页框的页描述符的private给设为0,还要把该页的页描述符的flags
  标志的PG_private给清掉.
- zone->nr_free减1
- zone->free_pages要减去order对应的页的大小,而不管是否从其它更大的order里拆出页来.
** static inline struct page *expand(struct zone *zone, struct page *page,int low, int high, struct free_area *area)
*** mm/page_alloc.c:
- 这个函数被__rmqueue()调用来处理因不满足分配大小的要求而拆其它更大的页组.
- 用循环来处理拆页合页操作,low是要求的order,high是实际被分到的order,要循环high-low次.
** static inline void __free_pages_bulk (struct page *page, struct zone *zone, unsigned int order)
*** mm/page_alloc.c:
- 一个order为x的链表里的一个结点就是一个页描述符,而这个页描述符对应的页框之后连续2^x个页框
  就是一组的,那么结点的那个页的页框地址是不是一定是2^x*2^12的倍数呢?而在这个页的后面连续页
  框就不能是这个的倍数了.这也就是为什么合并两个块时要ULK说的这个条件:The physical address
  of the first page frame of the first block is a multiple of 2 x b x 2^12.
- 还要注意一个问题,就是被链接到free_area的页描述符所对应的页框号的最低11位不能全为0
- 每个循环里page局部变量只会减小或不变,不会增大因为combined_idx不会比page_idx大,但这样的设
  计是有问题的吧?因为这样的设计只能让page为头的这一block不能与之前的block合并,只能与后面的
  合并.这句话中的后面那部分说错了,page的确是只能减小不能增大,
- 在ULK里很好的说明了buddy这个变量,但是没有说明combined_idx这个变量.
- 其实在合并时就两种情况:
 #+BEGIN_EXAMPLE
  1, page_idx的order位为0时有以下情况

  __find_combined_index()返回的值等于page_idx,返回的值是赋给combined_idx的,所以在后面执行
  page=page+(combined_idx-page_idx)之后page其实不变.

  __page_find_buddy()返回的值是page+order_size就是page+(1<<order),返回的值是赋给buddy的,所
  以在执行完page=page+(combined_idx-page_idx)之后,page < buddy的.所以也可以得出如果两个块合
  并,那么是物理地址最小的那页作为块头被链到块链表里.

  这种情况就是与比page地址大的地址合并.

  --------------------------------------------------------------------
  2, page_idx的order位为1时有以下情况

  __find_combined_index()返回的值小于(这个与情况1不同)page_idx,返回的值是赋给
  combined_idx的,所以在后面执行page=page+(combined_idx-page_idx)之后page变小,小了
  (1<<order)(与1不同).

  __page_find_buddy()返回的值是page-order_size(这个与情况1不同)就是page-(1<<order),返回的值
  是赋给buddy的,所以在执行完page=page+(combined_idx-page_idx)之后,page = buddy的.所以也可以
  得出如果两个块合并,那么是物理地址最小的那页作为块头被链到块链表里.

  这种情况就是与比page地址小的地址合并.

  -----------------------------------------------------------------
  这两种情况下的page都会指向该块的最小的地址.

 #+END_EXAMPLE
** static int rmqueue_bulk(struct zone *zone, unsigned int order,unsigned long count, struct list_head *list)
*** mm/page_alloc.c:
- 这个函数的作用就是从zone区里分配count个大小为2^order的块,把这些块插到list链表里去,是用
  page->lru链起来的.
** static struct page *buffered_rmqueue(struct zone *zone, int order, unsigned int __nocast gfp_flags)
*** mm/page_alloc.c:
- 如果order是大于0的话,那么就和调用__rmqueue()一样,如果order为0那么就是从per-CPU page
  frame caches里获取了,
- 虽然__rmqueue()也可以分配order为0的页框,但是在分配时有可能会拆更大的块,而且在释放时很可
  能又会把这页框合并,如果是是频繁分配和释放的话就会很浪费时间.所以需要这样的单页cache.
** static void fastcall free_hot_cold_page(struct page *page, int cold)
*** include/linux/page-flags.h:
- 这个函数是把页释放到per-CPU caches里,所以order肯定是0.
- 这个函数的会增加page_state->pgfree
- per-CPU cache是放在zone->pageset链表里的,而且是每个CPU都有,每个zone都有自已的pageset.
- struct per_cpu_cpuset里就只有一个struct per_cpu_pages 类型的2元数组pcp,一个是hot cache一
  个是cold cache.
- 这个函数会调用free_pages_bulk()来释放所有的页框.但是这个函数不只是释放order为0的块.
** static int free_pages_bulk(struct zone *zone, int count, struct list_head *list, unsigned int order)
*** mm/page_alloc.c:
- 这个函数会把count个order为order的块放到zone里,而这些链表是放在list链表里的.
- zone->pages_scanned在这里清零,为什么要这样做呢?pages_scanned的解释ULK:Counter used when
  doing page frame reclaiming in the zone.
- zone->pages_unreclaimable也清0,ULK:Flag set when the zone is full of unreclaimable
  pages.

- per-CPU cache分配器所得到的页是以order为0调用buffered_rmqueue()来分配的,转而调用
  rmqueue_bulk(),再转而调用__rmqueue();zone分配器是通过__alloc_pages()来分得页的,转而以
  order可能不为0调用buffered_rmqueue(),转而有可能调用__rmqueue()或rmqueue_bulk().所以可以得
  出这两个分配器最终都是调用__rmqueue()分配页的.

- zone->pages_low:Low watermark for page frame reclaiming; also used by the zone allocator
  as a threshold value.

** int zone_watermark_ok(struct zone *z, int order, unsigned long mark, int classzone_idx, int can_try_harder, int gfp_high)
*** mm/page_alloc.c:
- 这个函数返回1表示有足够的页.
- can_try_harder为1表示可以在更难的情况下来分配,所以min会比较大,gfp_high表示min可以小一点.
- 首先要确的如果真的分配了(1<<order)+1个页之后,那么保留页框(zone->lowmem_reserve[])和
 #+BEGIN_EXAMPLE
	if (free_pages <= min + z->lowmem_reserve[classzone_idx])
 #+END_EXAMPLE
- 但是第二个条件是什么意思呢?ULK里的看不明白,到底与zone->pages_low有什么关系呢?
** struct page * fastcall __alloc_pages(unsigned int __nocast gfp_mask, unsigned int order, struct zonelist *zonelist)
*** mm/page_alloc.c:
- 第一次扫描zone->zonelist里的区时不用can_try_harder和gfp_high,所以第一次扫描时调用的
  zone_watermark_ok()的最后两个参数为0.
 #+BEGIN_EXAMPLE
		if (!zone_watermark_ok(z, order, z->pages_low,
				       classzone_idx, 0, 0))
 #+END_EXAMPLE
- 扫描完第一次之后如果没有可用的页就要唤醒所有zone的回收页框线程了。
- 第二次扫描的时候调用zone_watermark_ok()时比第一次加上can_try_harder和__GFP_HIGH.还有一个
  和第一次扫描不一样的地方是加多一个是等待的判断，不知道这个判断有什么用。
- ULK：and it is trying to reclaim page frames (either the PF_MEMALLOC flag or the
  PF_MEMDIE flag of current is set)。TIF_MEMDIE只有select_bad_process()和__alloc_pages()读
  取来判断，只有__oom_kill_task()设置。
 #+BEGIN_EXAMPLE
	if (((p->flags & PF_MEMALLOC) || unlikely(test_thread_flag(TIF_MEMDIE)))
 #+END_EXAMPLE
- 若进程想回收页框且不在中断(软硬)中就执行第三次扫描，buffer_rmqueue()这个函数是不休眠的.但
  是第三次的扫描是不管wait的,但是已经不用zone_watermark_ok()了。zone_watermark_ok()只有第一
  次和第二次的时候使用。
- 第三次扫描完之后如果不能等待那么不再做什么了，而是直接退出。唤醒进程是在第一次扫描时。
 #+BEGIN_EXAMPLE
	if (!wait)
		goto nopage;
 #+END_EXAMPLE
- 第三次扫描完之后如果可以等待就马上调用cond_resched()看是不是可以调度以其它进程可以回收内
  存。
 #+BEGIN_EXAMPLE
  rebalance:
	cond_resched();
 #+END_EXAMPLE
- 调度回来之后就设置PF_MEMALLOC，表明该进程开始回收内存了,同时也要设置一个reclaim_state,之
  后就调用try_to_free_pages()
 #+BEGIN_EXAMPLE
	p->flags |= PF_MEMALLOC;
	reclaim_state.reclaimed_slab = 0;
	p->reclaim_state = &reclaim_state;
	did_some_progress = try_to_free_pages(zones, gfp_mask, order);
	p->reclaim_state = NULL;
	p->flags &= ~PF_MEMALLOC;
 #+END_EXAMPLE
- try_to_free_pages()之后还要还要看看是不是需要调度。
 #+BEGIN_EXAMPLE
	cond_resched();
 #+END_EXAMPLE
- 调度回来之后若try_to_free_pages()返回成功就再以第二次相同的方法扫描。
 #+BEGIN_EXAMPLE
	if (likely(did_some_progress)) {
 #+END_EXAMPLE
- 若try_to_free_pages()没有成功返回，且__GFP_FS设置了，__GFP_NORETRY没设，那么就以第一次扫
  描的方式来扫描。且在这种情况下若没找到内存，那么就回到这个函数的开头重新执行。

** int __sched cond_resched(void)
*** kernel/sched.c:
- 如果TIF_NEED_RESCHED设置了，那么就会禁止抢点然后调度再使能中断，如果调度回来之后发现
  TIF_NEED_RESCHED还设置，那么再禁止抢点然后调度再使能中断。
- 如果__GFP_NORETRY没有设置且order不大于3或__GFP_REPEAT设置了，那么就调用
  blk_congestion_wait()来等待。如果__GFP_NORETRY没有设置且__GFP_NOFAIL设置了，那么就调用也
  blk_congestion_wait()
- 这个函数一共有5个地方来扫描zone.
** fastcall void __free_pages(struct page *page, unsigned int order)
*** mm/page_alloc.c:
- 如果是保留的页就不用释放页，那么保留页怎么处理呢？
- 如果是page->_count减1之后不为0，那么也不能释放页，因为还有其它进程在使用。
- 如果order为0，那么调用free_hot_page()放到per-CPU cache里。
- 如果order不为0，那么就调用__free_pages_ok()
** void __free_pages_ok(struct page *page, unsigned int order)
*** mm/page_alloc.c:
- 要修改page_state->pgfree.
- 为什么使用MMU才会递减page->_count计数器呢？
 #+BEGIN_EXAMPLE
#ifndef CONFIG_MMU
	if (order > 0)
		for (i = 1 ; i < (1 << order) ; ++i)
			__put_page(page + i);
#endif
 #+END_EXAMPLE
- 要把这个块插到一个空的链表头里去。
 #+BEGIN_EXAMPLE
	list_add(&page->lru, &list);
 #+END_EXAMPLE
- 最后调用free_pages_bulk()把页释放。



- slab allocator是属于Memory Area Management这一部分的。
- 在slab分配器里一个cache有多个slab，一个slab有多个object
- ULK:A first cache called kmem_cache whose objects are the cache descriptors of the
  remaining caches used by the kernel. The cache_cache variable contains the descriptor of
  this special cache.
- 关于对齐因子的解释，ULK：The objects managed by the slab allocator are aligned in
  memorythat is, they are stored in memory cells whose initial physical addresses are
  multiples of a given constant, which is usually a power of 2. This constant is called
  the alignment factor.最大的对齐因子是一个页的大小
- ULK:microcomputers access memory cells more quickly if their physical addresses are
  aligned with respect to the word size (that is, to the width of the internal memory bus
  of the computer).Thus, by default, the kmem_cache_create( ) function aligns objects
  according to the word size specified by the BYTES_PER_WORD macro.
- ULK:When creating a new slab cache, it's possible to specify that the objects included in it
  be aligned in the first-level hardware cache. To achieve this, the kernel sets the
  SLAB_HWCACHE_ALIGN cache descriptor flag. The kmem_cache_create( ) function handles the
  request as follows: If the object's size is greater than half of a cache line, it is
  aligned in RAM to a multiple of L1_CACHE_BYTESthat is, at the beginning of the
  line. eOtherwise, the object size is rounded up to a submultiple of L1_CACHE_BYTES; this
  ensures that a small object will never span across two cache lines.
- Slab中引用着色机制是为了提高L1缓冲的效率。我们知道linux是一个兼容性很高的平台，但现在处理
  器的缓冲区方式多种多样，有的每个处理器有自己的独立缓存。有的很多处理器共享一个缓存。有的
  除了一级缓存（L1）外还是二级缓存（L2），因此，linux为了更好的兼容处理平台，只优化了一级缓
  存.http://blog.chinaunix.net/uid-9543173-id-1988997.html
- 使用着色的原因的一个是让不相同大小的对象尽量不放在同一个cache line里，另一个原因是

** void __init kmem_cache_init(void)
*** mm/slab.c:
- 这个函数的注释我喜欢，先来个总的说明，
- cache_chain这个是所有kmem_cache_t的链表头。cache_cache是第一个结点。
  https://www.ibm.com/developerworks/cn/linux/l-linux-slab-allocator/ 有图说明
** kmem_cache_t *kmem_cache_create (const char *name, size_t size, size_t align, unsigned long flags, void (*ctor)(void*, kmem_cache_t *, unsigned long), void (*dtor)(void*, kmem_cache_t *, unsigned long))
*** mm/slab.c:
- size的大小是指object的大小。
- 开始要调整参数size,不足一个字的要补足一个字
 #+BEGIN_EXAMPLE
	if (size & (BYTES_PER_WORD-1)) {
		size += (BYTES_PER_WORD-1);
		size &= ~(BYTES_PER_WORD-1);
	}
 #+END_EXAMPLE
- 处理对齐的那段代码还是看不明白。
- 要在cache的大小小于512M或内部碎片的有足够的空间放slab描述符时才放在外部，这时会设置CFLGS_OFF_SLAB.
 #+BEGIN_EXAMPLE
Slab descriptors can be stored in two possible places:
External slab descriptor
Stored outside the slab, in one of the general caches not suitable for ISA DMA pointed to by
cache_sizes (see the next section).
Internal slab descriptor
Stored inside the slab, at the beginning of the first page frame assigned to the slab.
The slab allocator chooses the second solution when the size of the objects is smaller than 512MB or
when internal fragmentation leaves enough space for the slab descriptor and the object descriptors
(as described later)inside the slab. The CFLGS_OFF_SLAB flag in the flags field of the cache descriptor
is set to one if the slab descriptor is stored outside the slab; it is set to zero otherwise.
 #+END_EXAMPLE
- 为什么要在对象的大小大于页大小的1/8时就把slab描述符放在外部呢？难道是因为对象太大以至于没
  有多余的碎片了吗？有一个这样的注释:Size is large, assume best to place the slab management obj off-slab
  (should allow better packing of objs).所以这样做也只是一个猜想的结果。
 #+BEGIN_EXAMPLE
	/* Determine if the slab management is 'on' or 'off' slab. */
	if (size >= (PAGE_SIZE>>3))
		/*
		 * Size is large, assume best to place the slab management obj
		 * off-slab (should allow better packing of objs).
		 */
		flags |= CFLGS_OFF_SLAB;
 #+END_EXAMPLE
- cache->gfporder: Logarithm of the number of contiguous page frames included in a single
  slab. ULK:If the slab cache has been created with the SLAB_RECLAIM_ACCOUNT flag set, the
  page frames assigned to the slabs are accounted for as reclaimable pages when the kernel
  checks whether there is enough memory to satisfy some User Mode requests.就是说设置了
  SLAB_RECLAIM_ACCOUNT标志,那么表示这个slab里的页框在当内核检查是否有足够的内存来满足用户模
  式请求时可以回收这个slab.
- 在SLAB_RECLAIM_ACCOUNT设置且对象的大小小于页大小时,就把cache->gfporder设为0,这样做是不是
  因为页可以被轻易回收所以就把gfporder设置尽量小,且又因为一个对象的大小小于一个页大小,所以
  可以把cache->gfporder设成最小0.
- 第一个do{}while循环主要的作用是确定cache->gfporder,cache->num和内部碎片(left_over局部变
  量).cache->gfporder一开始是0,在这个循环的过程中会不断地增加.

 #+BEGIN_EXAMPLE
			cachep->gfporder++;
 #+END_EXAMPLE
  如果cache->gfporder在增加的过程中超过了MAX_GFP_ORDER,那么就要退出循环

 #+BEGIN_EXAMPLE
			if (cachep->gfporder >= MAX_GFP_ORDER)
				break;
 #+END_EXAMPLE
  在这里cache->num一定不能等于0

 #+BEGIN_EXAMPLE
			if (!cachep->num)
				goto next;
 #+END_EXAMPLE
  关于offslab_limit的注释：Max number of objs-per-slab for caches which use off-slab
  slabs. Needed to avoid a possible looping condition in cache_grow().所以有做这样的处理：
 #+BEGIN_EXAMPLE
			if (flags & CFLGS_OFF_SLAB &&
					cachep->num > offslab_limit) {
				/* This num of objs will cause problems. */
				cachep->gfporder--;
				break_flag++;
				goto cal_wastage;
			}
 #+END_EXAMPLE
- slab_break_gfp_order就只有0或1这两种值，所以一个cache里最多只能有1个页或2个页。
 #+BEGIN_EXAMPLE
			if (cachep->gfporder >= slab_break_gfp_order)
				break;
 #+END_EXAMPLE
- 可允许的内部碎片是这样子计算的
 #+BEGIN_EXAMPLE
			if ((left_over*8) <= (PAGE_SIZE<<cachep->gfporder))
				break;	/* Acceptable internal fragmentation. */
 #+END_EXAMPLE
- 如果设了CFLGS_OFF_SLAB且内部碎片不小于slab描述符和所有对象描述符的大小,那么就清掉CFLGS_OFF_SLAB

 #+BEGIN_EXAMPLE
	/*
	 * If the slab has been placed off-slab, and we have enough space then
	 * move it on-slab. This is at the expense of any extra colouring.
	 */
	if (flags & CFLGS_OFF_SLAB && left_over >= slab_size) {
		flags &= ~CFLGS_OFF_SLAB;
		left_over -= slab_size;
	}
 #+END_EXAMPLE
- 内部和外部的存放不同那么计算slab_size的大小也不一样。
 #+BEGIN_EXAMPLE
	slab_size = ALIGN(cachep->num*sizeof(kmem_bufctl_t)
				+ sizeof(struct slab), align);
	if (flags & CFLGS_OFF_SLAB) {
		/* really off slab. No need for manual alignment */
		slab_size = cachep->num*sizeof(kmem_bufctl_t)+sizeof(struct slab);
	}

 #+END_EXAMPLE
- cache->colour_off，ULK:Basic alignment offset in the slabs.
- cache->colour,ULK：Number of colors for the slabs 
- cache->colour_off就是cache_line_size()，就是cache行的大小。
  http://hi.baidu.com/zengzhaonong/item/52a529a5371a86248919d38b
 #+BEGIN_EXAMPLE
	cachep->colour_off = cache_line_size();
 #+END_EXAMPLE
- 原来cache->colour是这样计算的，就left_over直接除cache->colour_off.
 #+BEGIN_EXAMPLE
	cachep->colour = left_over/cachep->colour_off;
 #+END_EXAMPLE
- cache->slab_size就是slab描述符加上所有的object描述符，如果是在内部，那么还要加上最后一个
  对象的后面的对齐字节
 #+BEGIN_EXAMPLE
	slab_size = ALIGN(cachep->num*sizeof(kmem_bufctl_t)
				+ sizeof(struct slab), align);
 #+END_EXAMPLE
 #+BEGIN_EXAMPLE
	if (flags & CFLGS_OFF_SLAB) {
		/* really off slab. No need for manual alignment */
		slab_size = cachep->num*sizeof(kmem_bufctl_t)+sizeof(struct slab);
	}
 #+END_EXAMPLE
- cache->flags,ULK:flags Set of flags that describes permanent properties of the cache.
- cache->gfpflags,ULK:gfpflags Set of flags passed to the buddy system function when
  allocating page frames.在这个函数里只会设置GFP_DMA
 #+BEGIN_EXAMPLE
	if (flags & SLAB_CACHE_DMA)
		cachep->gfpflags |= GFP_DMA;
 #+END_EXAMPLE
- cache->slabp_cache,ULK:Pointer to the general slab cache containing the slab descriptors
  (NULL if internal slab descriptors are used;)
 #+BEGIN_EXAMPLE
	if (flags & CFLGS_OFF_SLAB)
		cachep->slabp_cache = kmem_find_general_cachep(slab_size,0);
 #+END_EXAMPLE
- 看到最后一个函数alloc_slabmgmt的最后一段注释，说效果甚微，这是为什么呢？这其实就是最终撤
  销slab着色的原因之一。我们看到如果slab的数目只有cachep->colour个的话，这个slab着色的效果
  就 太好了，但是这往往不太现实，slab的数目有时是相当大的，这样的话，slab着色实际上只是帮了
  一点点小忙而已，它仅仅保证了最开始的几个slab不会map到同一个cpu cache line，但是待slab逐渐
  增加以后，后面的slab将还是会无情的打仗，从而造成cpu访问cache频频失效，这种能救几个算几个
  的思想可能对于人类救灾是有效的，毕竟生命高于一切(当然不包括三氯氰胺事件)，但是对于系统设
  计，这种效果的机制不如不要，因为我们用大量的代码维持了一个效果甚微的方案，这是不值得的，
  软件设计就是这样，每笔账都要算清，赔本的生意绝对不做，内核开发者的慧眼识别出了这个滥竽充
  数的所谓的巧妙算法，绝然地移除了它，在分配器从slab发展到slub以后，这个问题相对简单了许多，
  slub的思想就是简单，不要那么多花里胡哨的算法，就是简单，简单就是美，这确实是一句真理，冲
  突就冲突呗，只要我们带来的益处超过了冲突带来的麻烦，这就是值得的，鸵鸟算法在这种情况下就
  是有效的，确实是这样。hi.baidu.com/zengzhaonong/item/52a529a5371a86248919d38b


** static void cache_estimate(unsigned long gfporder, size_t size, size_t align, int flags, size_t *left_over, unsigned int *num)
*** mm/slab.c:
- 一开始cache->gfporder是0,因为cache是在这个函数分配的且分配之后马上清零.所以cal_wastage处
  的循环一开始的cache->gfporder是0的.
- http://oss.org.cn/kernel-book/ch06/6.3.3.htm 说明了着色区和补尝着色区是如何分布的,这篇文
  章对我理解着色的具体实现有很大的帮助。对于一个cache里的第一个slab,它是这样分布的：一开始
  一个cache->colour_off的大小(在alloc_slabmgmt()可以看出),紧接着是slab描述符，紧接着是一连串的
  object描述符，这些object描述符中间不用添加对齐字节，紧接着是因为要对齐cpu cache line所添
  加的字节，紧接着是cache->colour_off*cache->colour_next个字节，紧接着是一堆连续的object，
  但是这些object之间要添加上对齐字节。cache->colour_off是一个cpu cache line所占的字节数，对
  于第一个slab，cache->colour_next是0，第二个就是1，以此类推，cache->colour_next在
  cache_grow()里自增。
 #+BEGIN_EXAMPLE
	while (i*size + ALIGN(base+i*extra, align) <= wastage)
		i++;
 #+END_EXAMPLE
- cache_estimate:指定slab的大小后，返回slab中的对像个数以及剩余空间数
- 因为对象的描述符是放在slab描述符之后的,所以如果slab描述符放在内部,那么对象的结构体也是放
  在内部的.
 #+BEGIN_EXAMPLE
	if (!(flags & CFLGS_OFF_SLAB)) {
		base = sizeof(struct slab);
		extra = sizeof(kmem_bufctl_t);
	}
 #+END_EXAMPLE
- 

** static void *kmem_getpages(kmem_cache_t *cachep, unsigned int __nocast flags, int nodeid)
*** mm/slab.c:
- When the slab allocator creates a new slab, it relies on the zoned page frame allocator
  to obtain a group of free contiguous page frames. For this purpose, it invokes the
  kmem_getpages( ) function,所以这个函数是给
- 参数flags要加上cache->gfpflags的标志
- 用alloc_pages()分配页之后要用page_address()来获取虚拟地址。
- 如果设置了SLAB_RECLAIM_ACCOUNT，那么就增加slab_reclaim_pages,这个变量表示有多少个
  slab_pages页可以在页少时可以回收。
- page_state->nr_slab也要增加
- 还要修改page描述符的PG_slab。
** static void kmem_freepages(kmem_cache_t *cachep, void *addr)
*** mm/slab.c:
- 相对于kmem_getpages()来说，这个函数就是释放的。
- 因为通过virt_to_page()来获得页描述符，所以可以确定slab不会在高端内存。
- 如果current->reclaim_state不为空，表示current要回收页，所以要增加
  current->reclaim_state->reclaimed_slab.
- 这里会调用free_pages()释放页
- 若设了SLAB_RECLAIM_ACCOUNT,那么就要减slab_reclaim_pages

** static int cache_grow(kmem_cache_t *cachep, unsigned int __nocast flags, int nodeid)
*** mm/slab.c:
- the slab allocator assigns a new slab to the cache by invoking cache_grow( ) . 
- 这段代码计算了当前要分配的slab里的第一个对象要比第一个slab里的第一个对象的位置偏移多少.
 #+BEGIN_EXAMPLE
    offset = cachep->colour_next;
	cachep->colour_next++;
	if (cachep->colour_next >= cachep->colour)
		cachep->colour_next = 0;
	offset *= cachep->colour_off;
 #+END_EXAMPLE


** static struct slab* alloc_slabmgmt(kmem_cache_t *cachep, void *objp, int colour_off, unsigned int __nocast local_flags)
*** mm/slab.c:
- 如果cache->dtor不为空就要对每个对象调用dtor(),
 #+BEGIN_EXAMPLE
	if (cachep->dtor) {
		int i;
		for (i = 0; i < cachep->num; i++) {
			void* objp = slabp->s_mem+cachep->objsize*i;
			(cachep->dtor)(objp, cachep, 0);
		}
 #+END_EXAMPLE
- 释放一个slab也可以用rcu来释放
 #+BEGIN_EXAMPLE
	if (unlikely(cachep->flags & SLAB_DESTROY_BY_RCU)) {
		struct slab_rcu *slab_rcu;

		slab_rcu = (struct slab_rcu *) slabp;
		slab_rcu->cachep = cachep;
		slab_rcu->addr = addr;
		call_rcu(&slab_rcu->head, kmem_rcu_free);
 #+END_EXAMPLE
** void *kmem_cache_alloc(kmem_cache_t *cachep, unsigned int __nocast flags)
*** mm/slab.c:
- 调用__cache_alloc()
** static inline void *__cache_alloc(kmem_cache_t *cachep, unsigned int __nocast flags)
*** mm/slab.c:
- kmem_bufctl_t类型原来就是一个整型而已,因为它是存放一个地址的而已.
- cache->array的元素存放不同CPU的可用的空闲的对象

** static void *cache_alloc_refill(kmem_cache_t *cachep, unsigned int __nocast flags)
*** mm/slab.c:
- 若共享链表里有空闲的,那么就把空闲的对象移到相应的链表,而不是在共享链表里使用分配,就是说共
  享链表里的所有对象都是空闲的.拷贝过去的只是一个地址而已,也就是说在紧跟struct array_cache
  后面的是对象的地址,不是对象本身.
 #+BEGIN_EXAMPLE
			memcpy(ac_entry(ac), &ac_entry(shared_array)[shared_array->avail],
					sizeof(void*)*batchcount);
 #+END_EXAMPLE
- 无论如何,都可以通过以下的代码找到slab->list所指向的在slabs_partial或在slabs_free里的结点.
 #+BEGIN_EXAMPLE
		entry = l3->slabs_partial.next;
		if (entry == &l3->slabs_partial) {
			l3->free_touched = 1;
			entry = l3->slabs_free.next;
			if (entry == &l3->slabs_free)
				goto must_grow;
		}
		slabp = list_entry(entry, struct slab, list);
 #+END_EXAMPLE
- array_cache->avail的最大值是batchcount,说明紧跟array_cache后面的只有batchcount个对象地址.
- slab描述符是怎样被链起来的呢?应该是通过slab->list,又因为slab->list是kmem_list3结构体里的
  slabs_full或slabs_partail或slabs_free链表里的一个结点,又因为kmem_cache_t->lists是
  kmem_list3结构体,这样子就可以把kmem_cache_t和所有的slab关联起来了.
** static inline void __cache_free(kmem_cache_t *cachep, void *objp)
*** mm/slab.c:
- 若array_cache->avail还没有到限制(array_cache->limit)的话就把对象放到array_cache结构体的后
  面可以了,array_cache->limit一定是不小于array_cache->batchcount的.这是不是说明array_cache
  结构体后面的对象在地址上不一定是连续的呢?是不是也有可能后面的对象不在同一个slab上?
- array_cache结构体后的对象因为在空时才分配batchcount个,没有超过array_cache->limit,而会因为
  释放过多而超过array_cache->limit.
- 虽然每个CPU都有自已的array_cache结构体,但是不能保证回收到某个CPU的array_cache的对象一定是
  从该个CPU的array_cache分配出去的.因为进程会被迁移到不同的CPU执行.
- 若array->avail大于array_cache->limit就要调用cache_flusharray()来释放batchcount个对象。
 #+BEGIN_EXAMPLE
		cache_flusharray(cachep, ac);
 #+END_EXAMPLE
- 若array->avail小于array_cache->limit，那么就要把对象放回array_cache结构体后面。
 #+BEGIN_EXAMPLE
		ac_entry(ac)[ac->avail++] = objp;
 #+END_EXAMPLE
** static void cache_flusharray(kmem_cache_t *cachep, struct array_cache *ac)
*** mm/slab.c:
- 如果cache->lists->shared不为空且还有空余的空间来存放新的对象，就一定会把对象放到共享队列
  里，直到填满整个共享队列。但是因为共享队列剩余的空间可以小于batchcount，且在这种情况下是
  不会处理多余的对象，那么就可能释放的对象数小于batchcount。所以调用free_block()的
  batchcount参数不一定是array_cache->batchcount。
- 释放array_cache结构体后面的对象是紧跟在结构体后面的，而不是离结构体最远的那个。所以要把
  batchount之后的所有对象移到紧跟结构体后面。
** static void free_block(kmem_cache_t *cachep, void **objpp, int nr_objects)
*** mm/slab.c:
- Remember that the lru.prev field of the descriptor of the slab page points to the
  corresponding slab descriptor.
 #+BEGIN_EXAMPLE
		slabp = GET_PAGE_SLAB(virt_to_page(objp));
 #+END_EXAMPLE
- 这个函数是对一个对象一个对象独立释放的,不是一起释放的,这也说明了array_cache结构体后面的对
  象的地址不一定是连续的,更有可能的是这些对象不一定在同一个slab里.
- 传给这个函数的objpp是array_cache结构体后面的地址,因为objpp是指针的指针,所以是指向结构体后
  面连续的对象.所以可以用循环来释放
 #+BEGIN_EXAMPLE
	for (i = 0; i < nr_objects; i++) {
 #+END_EXAMPLE
- 现在才知道slab结构体后面的对象描述符原来是这个作用的,和slab->free结合在一起就实现了一个空
  闲对象的链表。slab->free存放的是第一个空闲的对象索引，而slab结构体后面的对象描述符是存放
  对于该对象下一个空闲对象的索引。就是如果slab->free是1，那么索引为1的对象是空闲的，而下一
  个空闲对象是这样获取的：因为slab->free为1，所以下一个空闲对象的索引存放在紧跟在slab描述符
  后面的第1个对象描述符里，注意对象描述符是一个整型而已。
 #+BEGIN_EXAMPLE
		slab_bufctl(slabp)[objnr] = slabp->free;
		slabp->free = objnr;
 #+END_EXAMPLE
- 以下的代码找到对象的索引
 #+BEGIN_EXAMPLE
		objnr = (objp - slabp->s_mem) / cachep->objsize;
 #+END_EXAMPLE
- 释放一个对象slab->inuse就要自减
- 当某个slab里的所有对象为空闲时
 #+BEGIN_EXAMPLE
		if (slabp->inuse == 0) {
 #+END_EXAMPLE
  且整个cache的空闲对象超过了上限，就要destroy这个slab了
 #+BEGIN_EXAMPLE
			if (cachep->lists.free_objects > cachep->free_limit) {
 #+END_EXAMPLE
  释放时要把free_objects减去cache->num(一个slab的对象数)
 #+BEGIN_EXAMPLE
				cachep->lists.free_objects -= cachep->num;
 #+END_EXAMPLE
  若没有超过上限，就要把这个slab插到slabs_free链表里。
 #+BEGIN_EXAMPLE
				list_add(&slabp->list,
				&list3_data_ptr(cachep, objp)->slabs_free);
 #+END_EXAMPLE
- 若inuse不为0，那就直接插到slabs_partial里
 #+BEGIN_EXAMPLE
			list_add_tail(&slabp->list,
				&list3_data_ptr(cachep, objp)->slabs_partial);
 #+END_EXAMPLE
** void *__kmalloc(size_t size, unsigned int __nocast flags)
*** mm/slab.c:
- 这个函数是用来分配2^n大小的slab对象的。
- 被这个函数分配的对象大小为2^n的cache是在kmem_cache_init()建立的。这个cache就general
  cache.
- 这个函数先调用__find_general_cachep()来找出应该哪个cache来分配。再调用__cache_alloc()来分
  配对象。
** static inline kmem_cache_t *__find_general_cachep(size_t size, int gfpflags)
*** mm/slab.c:
- 所有的general cache都存放在malloc_sizes里。
- 通过对比大小找到合适cache。
 #+BEGIN_EXAMPLE
	while (size > csizep->cs_size)
		csizep++;
 #+END_EXAMPLE
- 因为一个大小的cache有分DMA和非DMA两种对象，所以要通过gfpflags参数来判断
 #+BEGIN_EXAMPLE
	if (unlikely(gfpflags & GFP_DMA))
		return csizep->cs_dmacachep;
	return csizep->cs_cachep;
 #+END_EXAMPLE
** void kfree(const void *objp)
*** mm/slab.c:
- 这个函数对应kmalloc()的
** 
- ULK:a memory pool allows a kernel componentsuch as the block device subsystemto allocate
  some dynamic memory to be used only in low-on-memory emergencies.
- reserved page frames是与memory pool不一样的，前者用于满足中断处理程序和临界区请求的原子分
  配而建立的，而后者为某个特定的内核部分而保留的动态内存，memory pool 一般不使用。
- 从memory pool里分配内存可能是要等待的,在mempool_alloc()里休眠。
- memory pool只是做为slab对象里的一个缓冲区，memory pool的元素有可能是特殊slab里的对象，有
  可能是general cache里的对象。当是特殊slab时，那么mempool_t->alloc是
  mempool_alloc_slab(),mempool_t->free是mempool_free_slab(),里面对分别调用
  kmem_cache_alloc()和kmem_cache_free(), 这时mempool_t->pool_data指向的是cache 描述符。
** mempool_t * mempool_create(int min_nr, mempool_alloc_t *alloc_fn, mempool_free_t *free_fn, void *pool_data)
*** mm/mempool.c:
- mempool_t->min_nr是在创建时确定的，不是所有的mempool都是一样的。
- mempool_t结构体是用kmalloc()来分配的，没有用特殊的cache.
- mempool_t->elements原来是一个指针的指针，它指向的是一堆连续的指针
 #+BEGIN_EXAMPLE
	pool->elements = kmalloc(min_nr * sizeof(void *), GFP_KERNEL);
 #+END_EXAMPLE
- 所有在memory pool里的元素都是从参数pool_data那里来的。
 #+BEGIN_EXAMPLE
		element = pool->alloc(GFP_KERNEL, pool->pool_data);
 #+END_EXAMPLE
- 这个函数会创建mempool_t->min_nr个元素，对于mempool_t->min_nr，ULK是这样解释的：min_nr
  Maximum number of elements in the memory pool，注意是maximum
- 用一个类似array_cache结构体后面来存储对象地址的方式来实现mempool_t->element，那怎么正确的
  分配与释放的问题呢？
** static void add_element(mempool_t *pool, void *element)
*** mm/mempool.c:
- mempool_t->curr_nr就是内存池第一个空闲元素的位置。那是不是在被回收的元素就要是最近被分配
  的元素呢？原来理解错了mempool_t->curr_nr意思了，ULK:curr_nr Current number of elements
  in the memory pool.所以和array_cache->avail相似。
** void * mempool_alloc(mempool_t *pool, unsigned int __nocast gfp_mask)
*** mm/mempool.c:
- 要分配到内存池的元素，就要调用这个函数。
- 一开始分配元素时没有直接从内存池里分配的，所以一开始是调用mempoolt->alloc来分配的。
- 第一次调用mempool_t->alloc时是不想等待的，
 #+BEGIN_EXAMPLE
	gfp_temp = gfp_mask & ~(__GFP_WAIT|__GFP_IO);

repeat_alloc:

	element = pool->alloc(gfp_temp, pool->pool_data);
 #+END_EXAMPLE
- 每次休眠回来都是先从mempool_t->alloc里分配的，
 #+BEGIN_EXAMPLE
	prepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);
	smp_mb();
	if (!pool->curr_nr)
		io_schedule();
	finish_wait(&pool->wait, &wait);

	goto repeat_alloc;
 #+END_EXAMPLE
  且休眠回来之后调用mempool_t->alloc可能会阻塞
 #+BEGIN_EXAMPLE
	/* We must not sleep in the GFP_ATOMIC case */
	if (!(gfp_mask & __GFP_WAIT))
		return NULL;

	/* Now start performing page reclaim */
	gfp_temp = gfp_mask;
 #+END_EXAMPLE
- 若从mempool_t->alloc里分配不到元素时就会从内存池里分配
 #+BEGIN_EXAMPLE
	if (likely(pool->curr_nr)) {
		element = remove_element(pool);
		spin_unlock_irqrestore(&pool->lock, flags);
		return element;
	}
 #+END_EXAMPLE
** void mempool_free(void *element, mempool_t *pool)
*** mm/mempool.c:
- 若内存池的元素数没有超过限制就把它放回内存池，并唤醒等待队列
 #+BEGIN_EXAMPLE
		if (pool->curr_nr < pool->min_nr) {
			add_element(pool, element);
			spin_unlock_irqrestore(&pool->lock, flags);
			wake_up(&pool->wait);
			return;
        }
 #+END_EXAMPLE
  否则就释放这个元素
 #+BEGIN_EXAMPLE
	pool->free(element, pool->pool_data);
 #+END_EXAMPLE

- 非连续内存的使用场合：Linux uses noncontiguous memory areas in several ways for
  instance, to allocate data structures for active swap areas (see the section "Activating
  and Deactivating a Swap Area" in Chapter 17), to allocate space for a module (see
  Appendix B), or to allocate buffers to some I/O drivers. Furthermore, noncontiguous
  memory areas provide yet another way to make use of high memory page frames (see the
  later section "Allocating a Noncontiguous Memory Area").
- 原来非连续内存的线性地址是PAGE_OFFSET到0xffffffff这段地址的。这段地址还夹杂着固定映段和临
  时映射的线性地址。关于这段地址的解释8.3.1节开头有解释。


** void *__vmalloc_area(struct vm_struct *area, unsigned int __nocast gfp_mask, pgprot_t prot)
*** mm/vmalloc.c:
- 这个函数是一个页一个页地分配给整个连续的线性地址的
 #+BEGIN_EXAMPLE
		area->pages[i] = alloc_page(gfp_mask);
 #+END_EXAMPLE
- vm_struct->size的大小是包括安全区的，所以要减去一页的大小
 #+BEGIN_EXAMPLE
	nr_pages = (area->size - PAGE_SIZE) >> PAGE_SHIFT;
 #+END_EXAMPLE
  但是vm_struct->nr_pages是没有包括安全区的。
- vm_struct->pages存放的是保存(nr_pages * sizeof(struct page *))个页的地址，
  vm_struct->pages是一个指针的指针。vm_struct->size是4095的倍数。
 #+BEGIN_EXAMPLE
	nr_pages = (area->size - PAGE_SIZE) >> PAGE_SHIFT;
	array_size = (nr_pages * sizeof(struct page *));
	/* Please note that the recursion is strictly bounded. */
	if (array_size > PAGE_SIZE)
		pages = __vmalloc(array_size, gfp_mask, PAGE_KERNEL);
	else
		pages = kmalloc(array_size, (gfp_mask & ~__GFP_HIGHMEM));
	area->pages = pages;
 #+END_EXAMPLE
   从kmalloc()分配到的内存有可能在高端吗？但是有分DMA区和非DMA区的，这个区是用GFP_DMA来区分
   的。所以这个函数的大部分代码就是为vm_struct->pages分配空间，再分配页框并把页框对应的页描
   述符地址放到vm_struct->pages指向的空间里。
- 分配好页之后，就调用map_vm_area()来把vm_struct->addr和这些页框关联起来，就是要建立页表。
** int map_vm_area(struct vm_struct *area, pgprot_t prot, struct page ***pages)
*** mm/vmalloc.c:
- 在这个函数里处理的是pgd页表一级的，通过调用vmap_pud_range()来处理pud一级的页表。
- 因为整个被映射的虚拟地址有可能超过一个pgd页表项所能表示的最大范围，所以要用循环来处理多个
  pgd页表项。
 #+BEGIN_EXAMPLE
	do {
		next = pgd_addr_end(addr, end);
		err = vmap_pud_range(pgd, addr, next, prot, pages);
		if (err)
			break;
	} while (pgd++, addr = next, addr != end);
 #+END_EXAMPLE
  任何一个页表里的项所映射的虚拟地址一定是连续的.就是说如果两个页表项是连续的,那么它们所表
  示的虚拟地址就是连续的.
** static inline int vmap_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end, pgprot_t prot, struct page ***pages)
*** mm/vmalloc.c:
- 这个函数处理的是pud一级的页表.它调用vmap_pmd_range()来处理pmd一级的页表.
** static inline int vmap_pmd_range(pud_t *pud, unsigned long addr, unsigned long end, pgprot_t prot, struct page ***pages)
*** mm/vmalloc.c:
- 这个函数处理的是pmd一级的页表,这调用vmaap_pte_range()处理pte一级的页表.
** static int vmap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end, pgprot_t prot, struct page ***pages)
*** mm/vmalloc.c:
- 这个函数处理的是pte一级的页表,用set_pte_at()来填充pte页表项.
- 比较关键的一点就是如何处理pages这个参数了.
** void *vmap(struct page **pages, unsigned int count,unsigned long flags, pgprot_t prot)
*** mm/vmalloc.c:
- 这个函数就是先调用get_vm_area(),再调用map_vm_area().而vmalloc()调用__vmalloc(),转而调用
  get_vm_area()到这里与vmap()还是一样的,但是转而调用的__vmalloc_area()不一样
  了,再__vmalloc_area()里在调用map_vm_area()之前多做了一些事情,就是设置了
  vm_struct->nr_pages和vm_struct->pages,还分配了页框并把页框对应的描述符地址放到了
  vm_struct->pages里.所以在调用vmap()之前要把在__vmalloc_area()函数里调用map_vm_area()之前
  所做的事情做了才可以调用vmap().
- 所以vmap()的适用场合是:很多个页已经分配好了,这些页可以连续也可以不连续,而且这些的页的页描
  述地址是已放在参数pages里了,调用这个函数的目的就是把这些页映射到一段连续的线性地址去.
- 但要注意无论是vmap()还是vmalloc()都不会把vm_struct这个结构体返回给调用者,这个结构体只是内
  部使用,返回给调用者的是vm_struct->addr这个线性地址.在解除映射时也只接收虚拟地址,而不接收
  vm_strcut这个结构体,但是不是用container_of()这个宏来用vm_structt->addr找到vm_struct这个结
  构体的,而是用扫描vmlist这个链表来找到vm_struct的,在__remove_vm_area()里有这段代码
 #+BEGIN_EXAMPLE
	for (p = &vmlist ; (tmp = *p) != NULL ;p = &tmp->next) {
		 if (tmp->addr == addr)
			 goto found;
	}
 #+END_EXAMPLE
** void vfree(void *addr)
*** mm/vmalloc.c:
- 就是直接调用__vunmap(),但是第二个参数是以1调用的,
** void vunmap(void *addr)
*** mm/vmalloc.c:
- 也就是直接调用__vumap(),但是第二个参数是以0调用的,这个参数的作用是在__vumap()里体现的.
** void __vunmap(void *addr, int deallocate_pages)
*** mm/vmalloc.c:
- 无论是又于vfree()还是vunmap(),都要调用remove_vm_area()把虚拟地址对应的页表项给清掉.
- 接下来的工作是把页框给释放掉,但这个对于vunmap()是不用做的这个工作的,因为调用
  vmap()时,vmap()所使用的页框不是在vmap()内部分配的,是由外部分配后传给vmap()的.


- All memory descriptors are stored in a doubly linked list. Each descriptor stores the
  address of the adjacent list items in the mmlist field. The first element of the list is
  the mmlist field of init_mm,
- The mm_users field stores the number of lightweight processes that share the mm_struct
  data structure
- The mm_count field is the main usage counter of the memory descripto
- We'll try to explain the difference between the use of mm_users and mm_count with an
  example.Consider a memory descriptor shared by two lightweight processes. Normally, its
  mm_users field stores the value 2, while its mm_count field stores the value 1 (both
  owner processes count as one).If the memory descriptor is temporarily lent to a kernel
  thread (see the next section), the kernel increases the mm_count field. In this way,
  even if both lightweight processes die and the mm_users field becomes zero, the memory
  descriptor is not released until the kernel thread finishes using it because the
  mm_count field remains greater than zero.
- task_struct->mm_struct->mmap把该进程所有的内存区描述符vm_area_struct给链起来了.
- Figure 9-1说明是添加一个内存区时的两种不同插入方式(是否有相同的杈限),删除一个内存区时的两
  种不同的情况(在中间删除或在边上删除).
- We have already discussed two kinds of flags associated with a page: A few flags such as
  Read/Write, Present, or User/Supervisor stored in each Page Table entry.A set of flags
  stored in the flags field of each page descriptor.We now introduce a third kind of flag:
  those associated with the pages of a memory region.
- The initial values of the Page Table flags (which must be the same for all pages in the
  memory region, as we have seen) are stored in the vm_ page_ prot field of the
  vm_area_struct descriptor. When adding a page, the kernel sets the flags in the
  corresponding Page Table entry according to the value of the vm_ page_ prot field.
- When a User Mode process asks for dynamic memory, it doesn't get additional page frames;
  instead, it gets the right to use a new range of linear addresses, which become part of
  its address space. This interval is called a "memory region."

** struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr)
*** mm/mmap.c:
- 这个函数只是通过地址addr找合适的vm_area_struct而已,找不到也不会分配.
- 首先是看mm_struct->mmap_cache是不是要找的,因为这个是最近访问的.若不是就要搜索红黑树了.
- 这个函断要找的vm_area_struct是vm_end大于addr就可以了，与vm_start无关。
** static inline struct vm_area_struct * find_vma_intersection(struct mm_struct * mm, unsigned long start_addr, unsigned long end_addr)
*** include/linux/mm.h:
- 这个函数简单，就是直接用start_addr调用find_vma()来找到包含start_addr地址的区间，再看
  end_addr是不是在个区间内，若是就是找到区间，否则就是没有这样的区间。
** unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags)
*** mm/mmap.c:
- 一些架构有自已的实现，如ARM。
- 这个函数是给用户进程分配用户态空间的区域的。
- 这个函数是从0x40000000的线性地址开始往上增加来分配的，与arch_get_unmapped_area_topdown()相反。
- mm_struct->mmap_cache是给find_vma()找区间用的，mm_struct->free_area_cache是给
  arch_get_unmapped_area()用的
- 先看addr到addr+len这一段是否是空的区间，
 #+BEGIN_EXAMPLE
	if (addr) {
		addr = PAGE_ALIGN(addr);
		vma = find_vma(mm, addr);
		if (TASK_SIZE - len >= addr &&
		    (!vma || addr + len <= vma->vm_start))
			return addr;
	}
 #+END_EXAMPLE
  不为0表示尽量在addr处开始获得空间，先是用find_vma()找一个vm_end比addr大的vm_area_struct,
  若找到且addr到addr+len这个区间内的地址都不在vm_area_struct里的话就从addr到addr+len这个区
  间里有空间可以使用了。
- 如果上一步没有找到或addr为0就要搜索整个进程的线性地址了。注意是从mm->free_area_cache这个
  vm_area_struct区间开始找的
 #+BEGIN_EXAMPLE
	start_addr = addr = mm->free_area_cache;

full_search:
	for (vma = find_vma(mm, addr); ; vma = vma->vm_next) {
 #+END_EXAMPLE
- 跳到下一个vm_area_struct的操作就是利用vm_area_struct->vm_next,这种方式的话能保证地址是连
  续搜索的，就是红黑树维护了这一点。
- 有了一个vm_area_struct，那么开始地址就是从vm_area_struct->vm_end开始的
 #+BEGIN_EXAMPLE
		addr = vma->vm_end;
 #+END_EXAMPLE
- 判断一个区间是否有存在就是用后一个vm_area_struct->start_addr的地址减去关前一个
  vm_area_struct->end_addr是否大于addr+len,若是，就表明存在区间了。
 #+BEGIN_EXAMPLE
		if (!vma || addr + len <= vma->vm_start) {
			/*
			 * Remember the place where we stopped the search:
			 */
			mm->free_area_cache = addr + len;
			return addr;
		}
 #+END_EXAMPLE
** int insert_vm_struct(struct mm_struct * mm, struct vm_area_struct * vma)
*** mm/mmap.c:
- vm_area_struct->vm_pgoff，ULK：Offset in mapped file (see Chapter 16). For anonymous
  pages, it is either zero or equal to vm_start/PAGE_SIZE (see Chapter 17).为什么要这样做呢？
  有一段这样的注释：The vm_pgoff of a purely anonymous vma should be irrelevant until its
  first write fault, when page's anon_vma and index are set.  But now set the vm_pgoff it
  will almost certainly end up with (unless mremap moves it elsewhere before that first
  wfault), so /proc/pid/maps tells a consistent story. By setting it to reflect the
  virtual start address of the vma, merges and splits can happen in a seamless way, just
  using the existing file pgoff checks and manipulations. Similarly in do_mmap_pgoff and
  in do_brk.
- 
** static void vma_link(struct mm_struct *mm, struct vm_area_struct *vma, struct vm_area_struct *prev, struct rb_node **rb_link, struct rb_node *rb_parent)
*** mm/mmap.c:
- 为什么要把vm_area_struct->vm_truncate_count设成
  vm_area_struct->vm_file->f_mapping->truncate_count呢？
- ULK:vm_truncate_count Used when releasing a linear address interval in a non-linear file
  memory mapping.
** static inline unsigned long do_mmap(struct file *file, unsigned long addr,unsigned long len, unsigned long prot,	unsigned long flag, unsigned long offset)
*** include/linux/mm.h:
- 主要调用了do_mmap_pgoff()
- 这个函数只是分配一个页大小的虚拟地址
** unsigned long do_mmap_pgoff(struct file * file, unsigned long addr, unsigned long len, unsigned long prot,unsigned long flags, unsigned long pgoff)
*** mm/mmap.c:
- ULK:the do_mmap( ) function creates and initializes a new memory region for the current
  process. However, after a successful allocation, the memory region could be merged with
  other memory regions defined for the process
- 对于匿名区,主要的工作有:分配vm_area_struct,初始化vm_area_struct里的成员(特别是vm_flags占
  了很多代码),把是匿名区就链接到vma链表(vma_link()),若页被锁就分配页
- 若file->f_op不为空表示文件操作函数不为空，但是file->f_op->mmap为空就会返回ENODEV
 #+BEGIN_EXAMPLE
		if (!file->f_op || !file->f_op->mmap)
			return -ENODEV;
 #+END_EXAMPLE 
- 通过调用get_unmapped_area()来获取空闲的区间,若不能获取就返回.
 #+BEGIN_EXAMPLE
	addr = get_unmapped_area(file, addr, len, pgoff, flags);
	if (addr & ~PAGE_MASK)
		return addr;
 #+END_EXAMPLE 
- mm->def_flags,VM_MAYREAD,VM_MAYWRITE, VM_MAYEXEC这几个标志一定是有的,
 #+BEGIN_EXAMPLE
	vm_flags = calc_vm_prot_bits(prot) | calc_vm_flag_bits(flags) |
			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
 #+END_EXAMPLE 
- ULK关于参数prot的说明：This parameter specifies the access rights of the pages included
  in the memory region.Possible flags are PROT_READ, PROT_WRITE, PROT_EXEC, and
  PROT_NONE. The first three flags mean the same things as the VM_READ, VM_WRITE , and
  VM_EXEC flags. PROT_NONE indicates that the process has none of those access rights.
- 关于flags参数有MAP_GROWSDOWN, MAP_LOCKED, MAP_DENYWRITE, and MAP_EXECUTABLE是与VM_有对应
  的，还有一些不是与VM_对应的，所以对应的那些要转为VM_
 #+BEGIN_EXAMPLE
	vm_flags = calc_vm_prot_bits(prot) | calc_vm_flag_bits(flags) |
			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
  #+END_EXAMPLE 
  但MAP_LOCKED还是要特殊处理一下的.先是允许mlock操作才会设置VM_LOCKED,VM_LOCKED是不是最终会
  影响PG_locked呢?
 #+BEGIN_EXAMPLE
	if (flags & MAP_LOCKED) {
		if (!can_do_mlock())
			return -EPERM;
		vm_flags |= VM_LOCKED;
	}
 #+END_EXAMPLE 
  接着还要看有没有超限

 #+BEGIN_EXAMPLE
	if (vm_flags & VM_LOCKED) {
		unsigned long locked, lock_limit;
		locked = len >> PAGE_SHIFT;
		locked += mm->locked_vm;
		lock_limit = current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur;
		lock_limit >>= PAGE_SHIFT;
		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
			return -EAGAIN;
	}
 #+END_EXAMPLE 
- MAP_SHARED:The former flag specifies that the pages in the memory region can be shared
  among several processes; the latter flag has the opposite effect.
- 对于MAP_SHARED和MAP_PRIVATE也要做处理,对于file是否为空会做出不同的处理:
 #+BEGIN_EXAMPLE
	if (file) {
		switch (flags & MAP_TYPE) {
		case MAP_SHARED:
			if ((prot&PROT_WRITE) && !(file->f_mode&FMODE_WRITE))
				return -EACCES;

			/*
			 * Make sure we don't allow writing to an append-only
			 * file..
			 */
			if (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))
				return -EACCES;

			/*
			 * Make sure there are no mandatory locks on the file.
			 */
			if (locks_verify_locked(inode))
				return -EAGAIN;

			vm_flags |= VM_SHARED | VM_MAYSHARE;
			if (!(file->f_mode & FMODE_WRITE))
				vm_flags &= ~(VM_MAYWRITE | VM_SHARED);

			/* fall through */
		case MAP_PRIVATE:
			if (!(file->f_mode & FMODE_READ))
				return -EACCES;
			break;

		default:
			return -EINVAL;
		}
	} else {
		switch (flags & MAP_TYPE) {
		case MAP_SHARED:
			vm_flags |= VM_SHARED | VM_MAYSHARE;
			break;
		case MAP_PRIVATE:
			/*
			 * Set pgoff according to addr for anon_vma.
			 */
			pgoff = addr >> PAGE_SHIFT;
			break;
		default:
			return -EINVAL;
		}
	}
 #+END_EXAMPLE 
- 若是映射文件的,在MAP_SHARED情况下,设了PROT_WRITE但file->f_mode没有设FMODE_WRITE是错误的.不
  允许向一个只能append的文件设置FMODE_WRITE.该file不能有强制锁.
 #+BEGIN_EXAMPLE
			if ((prot&PROT_WRITE) && !(file->f_mode&FMODE_WRITE))
				return -EACCES;

			/*
			 * Make sure we don't allow writing to an append-only
			 * file..
			 */
			if (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))
				return -EACCES;

			/*
			 * Make sure there are no mandatory locks on the file.
			 */
			if (locks_verify_locked(inode))
				return -EAGAIN;
 #+END_EXAMPLE 
- 若file不为空,且设置了MAP_SHARED,那么就一定会设置VM_MAYSHARE,且如果设了FMODE_WRITE,那么就
  设置VM_SHARED,否则就不能设置VM_SHARED,还要把VM_MAYWRITE清掉.
 #+BEGIN_EXAMPLE
			vm_flags |= VM_SHARED | VM_MAYSHARE;
			if (!(file->f_mode & FMODE_WRITE))
				vm_flags &= ~(VM_MAYWRITE | VM_SHARED);
 #+END_EXAMPLE 
- 对file为空时处理MAP_SHARED就简单一点了,就是把VM_SHARED和VM_MAYSHARE给设置了,这个
  VM_MAYSHARE有什么用呢?如果是MAP_PRIVATE的做法有点不理解
 #+BEGIN_EXAMPLE
			/*
			 * Set pgoff according to addr for anon_vma.
			 */
			pgoff = addr >> PAGE_SHIFT;
 #+END_EXAMPLE 
- 不用find_vma_intersection()函数的原因是除了找到一个空区间,还想找出这个空区间的前一个
  vm_area_struct,所以没有用find_vma_intersection()因为find_vma_intersection()不能找出前一
  个_vm_area_struct.
 #+BEGIN_EXAMPLE
munmap_back:
	vma = find_vma_prepare(mm, addr, &prev, &rb_link, &rb_parent);
	if (vma && vma->vm_start < addr + len) {
		if (do_munmap(mm, addr, len))
			return -ENOMEM;
		goto munmap_back;
	}
 #+END_EXAMPLE 
- 对于为什么要做下的检查,ULK:Notice that the check is done here and not in step 1 with the
  other checks, because some memory regions could have been removed in step 4.
 #+BEGIN_EXAMPLE
	if (!may_expand_vm(mm, len >> PAGE_SHIFT))
		return -ENOMEM;
 #+END_EXAMPLE 
- ULK:VM_ACCOUNT Check whether there is enough free memory for the mapping when creating
  an IPC shared memory region (see Chapter 19). ULK:MAP_NORESERVE The function doesn't
  have to do a preliminary check on the number of free page
  frames.http://book.2cto.com/201302/16309.html : 下面介绍overcommit_memory的不同取值对应的
  不同虚拟空间分配。  OVERCOMMIT_GUESS overcommit_memory的默认值为OVERCOMMIT_GUESS。指定这
  个参数时，预测将空闲内存、页面缓存量、空闲交换区量、可回收slab（长字节）量等回收的页面数，
  虚拟空间要求分配的量比这个数小时，分配成功。  （请注意，在多个进程同时要求大量的虚拟空间
  时是无法正确预测的。下面所述的OVERCOMMIT_NEVER中就没有这种问题。） 在OVERCOMMIT_GUESS的情
  况下，可分配的虚拟空间大小基本就是物理内存大小和交换区大小的合计值。物理内存为2GB，交换区
  为2GB，当前消耗1GB时，还可以分配约3GB的虚拟空间。  OVERCOMMIT_ALWAYS 在OVERCOMMIT_ALWAYS
  的情况下，虚拟空间分配总是成功。即使对于过大的虚拟空间要求，也会分配虚拟空间。可以在与实
  际安装的物理内存量完全无关的形态下使用虚拟空间，如前面所述的散列表等。  OVERCOMMIT_NEVER
  在OVERCOMMIT_NEVER的情况下，对可分配虚拟空间量的管理更加严格。  首先，记录下整个系统内已
  分配的虚拟空间量。这个值严格由系统进行集中管理，在分配或释放虚拟空间时重新计算。这个值为
  /proc/meminfo的Committed_AS。  对于虚拟空间大小的计算也比其他参数严格。例如，在
  OVERCOMMIT_GUESS的情况下，对mmap系统调用设置了MAP_NORESERVE的虚拟空间量不添加到
  Committed_AS中。但是，在OVERCOMMIT_NEVER的情况下会添加到Committed_AS中。指定了
  MAP_NORESERVE的区域也作为可能分配物理内存的虚拟空间处理。
 #+BEGIN_EXAMPLE
	if (accountable && (!(flags & MAP_NORESERVE) ||
			    sysctl_overcommit_memory == OVERCOMMIT_NEVER)) {
		if (vm_flags & VM_SHARED) {
			/* Check memory availability in shmem_file_setup? */
			vm_flags |= VM_ACCOUNT;
		} else if (vm_flags & VM_WRITE) {
			/*
			 * Private writable mapping: check memory availability
			 */
			charged = len >> PAGE_SHIFT;
			if (security_vm_enough_memory(charged))
				return -ENOMEM;
			vm_flags |= VM_ACCOUNT;
		}
	}
 #+END_EXAMPLE 
 这个段代码最终影响的就是VM_ACCOUNT标志.在要求对页要做一个初步的检查或设置OVERCOMMIT_NEVER
 就作处理,在这种情况下若设置了VM_SHARED就设VM_ACCOUNT,表里作于IPC的内存时要检查是否有足够的
 内存,若是私有的可写的内存,就要用security_vm_enough_memory()检查,通过检查就设置
 VM_ACCOUNT.所以只有共享时或私有可写时才有可能设置VM_ACCOUNT.
- ULK:If the new interval is private (VM_SHARED not set) and it does not map a file on
  disk, it invokes vma_merge( ) to check whether the preceding memory region can be
  expanded in such a way to include the new interval.
 #+BEGIN_EXAMPLE
	/*
	 * Can we just expand an old private anonymous mapping?
	 * The VM_SHARED test is necessary because shmem_zero_setup
	 * will create the file object for a shared anonymous map below.
	 */
	if (!file && !(vm_flags & VM_SHARED) &&
	    vma_merge(mm, prev, addr, addr + len, vm_flags,
					NULL, NULL, pgoff, NULL))
		goto out;
 #+END_EXAMPLE 
  这面这段代码的注释也说了,检查VM_SHARED的必要的,因为shmem_zero_setup会为一个共享的匿名区创
  建一个文件对象,且因为映射文件的内存区是不可以被合并的.vma_merge()函数首先会检查能不能合并.
  
  到这里为止,之前的代码做的工作有:处理了vm_flags,找出了在有合适大小空闲内存区前的vm_area_struct,检查了合并.处理了pgoff
- ULK:If the MAP_SHARED flag is set (and the new memory region doesn't map a file on
  disk), the region is a shared anonymous region: invokes shmem_zero_setup( ) to
  initialize it. Shared anonymous regions are mainly used for interprocess communications;
  see the section "IPC Shared Memory" in Chapter 19.
 #+BEGIN_EXAMPLE
	} else if (vm_flags & VM_SHARED) {
		error = shmem_zero_setup(vma);
		if (error)
			goto free_vma;
	}
 #+END_EXAMPLE 
- 不知道这段代码是干什么的
 #+BEGIN_EXAMPLE
	/* We set VM_ACCOUNT in a shared mapping's vm_flags, to inform
	 * shmem_zero_setup (perhaps called through /dev/zero's ->mmap)
	 * that memory reservation must be checked; but that reservation
	 * belongs to shared memory object, not to vma: so now clear it.
	 */
	if ((vm_flags & (VM_SHARED|VM_ACCOUNT)) == (VM_SHARED|VM_ACCOUNT))
		vma->vm_flags &= ~VM_ACCOUNT;
 #+END_EXAMPLE 
- 下面一段代码也不知道干什么的
 #+BEGIN_EXAMPLE
	/* Can addr have changed??
	 *
	 * Answer: Yes, several device drivers can do it in their
	 *         f_op->mmap method. -DaveM
	 */
	addr = vma->vm_start;
	pgoff = vma->vm_pgoff;
	vm_flags = vma->vm_flags;

	if (!file || !vma_merge(mm, prev, addr, vma->vm_end,
			vma->vm_flags, NULL, file, pgoff, vma_policy(vma))) {
		file = vma->vm_file;
		vma_link(mm, vma, prev, rb_link, rb_parent);
		if (correct_wcount)
			atomic_inc(&inode->i_writecount);
	} else {
		if (file) {
			if (correct_wcount)
				atomic_inc(&inode->i_writecount);
			fput(file);
		}
		mpol_free(vma_policy(vma));
		kmem_cache_free(vm_area_cachep, vma);
	}
 #+END_EXAMPLE 
- 调用__vm_stat_account()
** void __vm_stat_account(struct mm_struct *mm, unsigned long flags, struct file *file, long pages)
*** mm/mmap.c:
- 若file不为空,就说明这个vma是映射到文件的,对于映射到文件的页要加到mm->shared_mm里去.对于设
  置了VM_GROWUP或VM_GROWSDOWN的页要加到mm->stack_vm里去.对于设置了VM_RESERVED或VM_IO的页要
  加到mm->reserved_vm里去.而mm->locked_vm已经在do_mmap_pgoff()处理了,还在一个mm->exec_vm不
  知道在哪处理了.
- VM_RESERVED:The region is special (for instance, it maps the I/O address space of a
  device), so its pages must not be swapped out.说明这个页不能被交换出去
- VM_IO: The region maps the I/O address space of a device.一个设备的I/O地址映射是不能被交
  换出去的.
** int make_pages_present(unsigned long addr, unsigned long end)
*** mm/memory.c:
- 这个函数是在do_mmap_pgoff()里设置VM_LOCKED时调用的.
- 这个函数主要是调用了get_user_pages()
- 因为get_user_pages()可以实现把所需要的页都在内存里.若只想这样,而不想得到struct page * 结
  构,那么调用get_user_pages()时的第二个参数可以为NULL.
** int get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start, int len, int write, int force, struct page **pages, struct vm_area_struct **vmas)
*** mm/memory.c:
- ULK:The get_user_pages( ) function cycles through all starting linear addresses of the
  pages between addr and addr+len ; for each of them, it invokes follow_page( ) to check
  whether there is a mapping to a physical page in the current's Page Tables. If no such
  physical page exists, get_user_pages( ) invokes handle_mm_fault( ) , which, as we'll see
  in the section "Handling a Faulty Address Inside the Address Space," allocates one page
  frame and sets its Page Table entry according to the vm_flags field of the memory region
  descriptor.这个函数还可以把在用户页的内容拷贝到内核.pages ：存放获取的struct page的指针数
  组,vms ： 返回各个页对应的struct vm_area_struct，可以传入NULL表示不获取.start ：要获取其
  页面的起始虚拟地址，它是用户空间使用的一个地址.
- write表示什么意思呢?在从make_pages_present()的调用来看,若vm_area_struct->vm_flags设置了
  VM_WRITE,那么write参数就为1.
- 从以下的代码来看,write参数使得flags设置了VM_WRITE和VM_MAYWRITE或VM_READ和VM_MAYREAD.而
  force参数会把VM_MAY给去掉,这样做有什么用呢?
- get_page()里把page->private赋给参数指针是怎么回事呢？
- 为什么要花这么大的劲来映射一个高端内存页到内核之后，好像又不怎么用呢？应该没那么简单
 #+BEGIN_EXAMPLE
			if (pg > TASK_SIZE)
				pgd = pgd_offset_k(pg);
			else
				pgd = pgd_offset_gate(mm, pg);
			BUG_ON(pgd_none(*pgd));
			pud = pud_offset(pgd, pg);
			BUG_ON(pud_none(*pud));
			pmd = pmd_offset(pud, pg);
			BUG_ON(pmd_none(*pmd));
			pte = pte_offset_map(pmd, pg);
			BUG_ON(pte_none(*pte));
			if (pages) {
				pages[i] = pte_page(*pte);
				get_page(pages[i]);
			}
			pte_unmap(pte)
 #+END_EXAMPLE 
  还是有一点不同的，因为pg这个地址是一个用户空间的地址，这样的话是不是就可以在内核态访问到
  用户态的空间了。一个pgd页表其实可以映射所有的4G内存，所以主内核页表也可以映射到用户态的地
  址，但是在映射用户态的空间时不能用pte_offset_kernel()(里面调用的pmd_offset_kernel()使用
  了__va())因为这个是映射内核空间用的，而pte_offset_map()是通过kmap_atomic()来映射用的,而
  kmap_atomic()返回的是一个pte页表的首地址,也就是说内核若想通过固定内核映射在主内核页表里映
  射一个用户态的空间，那么就要通过pte_offset_map(),在调用pte_offset_map()之前，要用
  pgd_offset(),pud_offset(),pmd_offset()建立不同级的页表，最后调用pte_offset_map().直接调用
  kmap_atomic()返回其实还只是一个pte页表的起始地址而已，pte_offset_map()还要通过一个
  pte_index()找到相应的下标来找出被映射地址的pte项.到这里可以得出，虽然内核的线性地址是物理
  地址加上PAGE_OFFSET,但这不能说明内核的线性地址不使用MMU，使用PAGE_OFFSET是因为内核有时需
  要通过线性地址知道相应的物理地址。注意pte_offset_map()和pte_offset_kernel()实现的区别，前
  者是对参数dir调用pmd_page()返回pte页表地址后再调用kmap_atomic(),再结合pte_index(),后者是
  对参数dir调用pmd_page_kernel()得到pte页表地址再结合pte_index(),所以可以看出kmap_atomic()
  通过了一次转换，但是转换后返回到东西还是一个意思，就是返回的还是pte页表的地址。
  kmap_atomic接收的page的参数是一个pmd页表项所指的包含pte页表的页框的struct page指针。再结
  合kmap_atomic()来重新理解kmap_atomic(),FIX_KMAP_BEGIN到FIX_KMAP_END =
  FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1这之间没有指定那些模块的地址，只是有一个下标而已，下
  标就在enum km_type里，要想通过这个下标找出该下标所使用的内核虚拟地址，就要通
  过__fix_to_virt()和FIXADDR_TOP这个最顶端的虚拟地址和下标来找，__fix_to_virt()返回的是一个
  页对齐的内核线性地址，且在kmap_atomic()里返回的就是__fix_to_virt()所返回的地址，由些可见
  kmap_atomic()返回的是一个页大小可用的内核线性地址，kmap_atomic()返回的这个页大小的线性地
  址是做什么用的呢？FIXADDR_TOP这个虚拟地址是与enum fixed_addresses里所有的元素相关的，不是
  单与FIX_KMAP_BEGIN相关，其实enum km_type里的所有成员都是可以插到FIX_KMAP_BEGIN和
  FIX_KMAP_END之间的，这个从kmap_atomic()里调用__fix_to_virt()时使用的是FIX_KMAP_BEGIN +
  idx可以看出.所以pte_offset_map()会把一个高端地址的所在的pte页表通过KM_PTE0窗口来映射。要
  注意若地址是高端地址，那么在get_user_pages()里用的pgd是当前进程的pgd,而不是主内核页表,(这
  个说法是错的，用的是主内核页表的，好像也不对，其实用哪个都无所谓，因为所有的pgd的第4G都是
  一样的映射的,应该是用户态的pgd,因为ULK也说过系统初始化完之后就主内核页表是没有使用的，只
  是用来拷贝第4G的页表而已,又有新发现，因为在get_user_pages()函数里有一个通过判断传入的虚拟
  地址来确定是使用主内核页表还是用户态的内核页表)所以在get_user_pages()里通过
  pud_offset(),pmd_offset()这些获得的页表都在都在用户态空间，但是可以通过pte_offset_map()把
  在用户态空间的pmd页表项所指向的整个pte页表映射到KM_PTE0所对应的pte页表项里去再通过
  pte_index()在这个被映射的整个pte页表里找出相应的页表项。所以调用pte_offset_map()之后，对
  于一个用户态空间的pte页表是被两个不同pmd页表项所映射，一个小于PAGE_OFFSET的线性地址的，另
  一个是在PTE0所指向的pte页表，在内核这一边就是通过高端地址固定映射来访问，所以有两个不同线
  性地址映射，且这两个地址互不干扰。所以调用pte_offset_map()得到的pte页表项与之前调用的
  pud_offset(),pmd_offset()得到的页表项没有关联。pmd_offset()里也会使用PAGE_OFFSET,是不是说
  用户态虚拟地址的pmd页表在内核的呢？难道是因为pte页表都在用户态空间所以要用
  pte_offset_map()才可以访问？除了pte_offset_map()就只有pte_offset_kernel()了，而
  pte_offset_kernel()里使用了PAGE_OFFSET，所以要用高端内存固定映射把用户态的pte页表映射成固
  定映射才可以找到相应的页表，接着才可以使用pte_page()找到相应的页描述符地址。
  http://bbs.chinaunix.net/thread-3691966-1-1.html 页表保存在内核态，由内核帮助进程来维护。
  到底pte_offset_map()是如何实现它的功能的呢？再说一次：dir参数要是一个pmd的页表项，且这个
  pmd页表项是映射用户态空间的，address这个地址也是用户态空间的，更重要的是address这个地址要
  在这个pmd页表项所映射的范围内，先用pmd_page(* (dir))获取dir这个pmd页表项所指向的包含pte页
  表的页框的struct page,再调用kmap_atomic(),调用完之后就会把KM_PTE0所对应的pte页表项映射到
  了pmd_page()返回的那个页框上去了，就是说KM_PTE0这个pte页表项映射的那个页其实是用户态空间
  使用的一个pte页表，又因为kmap_atomic()返回的是KM_PTE0相应的pte所对应的4k大小(一页)地址范
  围的起始地址，所以可以把它转成(pte * )的类型，因为这个起始地址就是pte页表的起始地址，这样
  就可以再结合pte_index()找出address这个用户态的虚拟地址所对应的pte页表项。但是为什么对于访
  问用户态空间地址时要这样做呢？
- 获取的vm_area_struct有点奇怪，调用的get_get_vma()总是返回相同的值，那赋给vmas还有什么用呢？
- 第一个if()成立的时候是在或即在找不到也扩展不了vma且虚拟地址不在一个范围内。
- 这下面这一句时vma一定不为空了
 #+BEGIN_EXAMPLE
		if (is_vm_hugetlb_page(vma)) {
 #+END_EXAMPLE
- 如果follow_page()(下面有介绍)失败，且地址空间不为写且有些级的页表没有就把退出循环
 #+BEGIN_EXAMPLE
			while (!(map = follow_page(mm, start, lookup_write))) {
				/*
				 * Shortcut for anonymous pages. We don't want
				 * to force the creation of pages tables for
				 * insanly big anonymously mapped areas that
				 * nobody touched so far. This is important
				 * for doing a core dump for these mappings.
				 */
				if (!lookup_write &&
				    untouched_anonymous_page(mm,vma,start)) {
					map = ZERO_PAGE(start);
					break;
				}
 #+END_EXAMPLE 
 为什么要把map赋予0页呢? 若没有跳出循环,那么以下的情况就是地址对应的pgd,pud,pmd页表项都有
 了,但是可能是pte没有或read/write的权限不对.所以接下来就是调用handle_mm_fault()了.
- 不需要从硬盘拷数据而发生的缺页叫次缺页(tsk->min_flt),需要从硬盘拷数据而发生的缺页叫主缺页
  (tsk->maj_flt)
- get_page_map()这个函数很简单,若struct page *超过了最大值就返回NULL,否则就返回参数.若发现
  有页是不正确的就释放原来已处理好的页
 #+BEGIN_EXAMPLE
			if (pages) {
				pages[i] = get_page_map(map);
				if (!pages[i]) {
					spin_unlock(&mm->page_table_lock);
					while (i--)
						page_cache_release(pages[i]);
					i = -EFAULT;
					goto out;
 #+END_EXAMPLE 
- 在这个函数的第一个if成立时是没有start地址对应的vm_area_struct的,为什么会有这种情况出现呢?在
  这种情况下只是用pte_offset_map()来找到地址对应的页框的struct page,这种情况下不会产生缺页
  异常?
** int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct * vma, unsigned long address, int write_access)
*** mm/memory.c:
- 这个函数先是用pgd_offset(),pud_offset(),pmd_offset(),pte_alloc_map()来找出相应的页表项,若
  没有就建立页表,可见页表是在存在内核里的.
- 被调用的pte_alloc_map是调用pte_alloc_one()来获取一个存放页表的页框的,从pte_alloc_one()实现来看
 #+BEGIN_EXAMPLE
#ifdef CONFIG_HIGHPTE
	pte = alloc_pages(GFP_KERNEL|__GFP_HIGHMEM|__GFP_REPEAT|__GFP_ZERO, 0);
#else
	pte = alloc_pages(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO, 0);
#endif
 #+END_EXAMPLE
 设置了CONFIG_HIGHPTE才会在就端内存里存放pte页表,不是有高端内存就一定会在高端内存里存放.因
 为可能放在高端内存,所以pte_alloc_map()会调用pte_offset_map()来获取这个地址所在的pte页表项
 的指针.在这个函数里使用了pte_offset_map()得到的pte要在handle_pte_fault()才unmap()掉.

** static inline int handle_pte_fault(struct mm_struct *mm,	struct vm_area_struct * vma, unsigned long address,	int write_access, pte_t *pte, pmd_t *pmd)
*** mm/memory.c:
- 这个函数处理请求调页和写时复制.
- ULK:An addressed page may not be present in main memory either because the page was
  never accessed by the process, or because the corresponding page frame has been
  reclaimed by the kernel.In both cases, the page fault handler must assign a new page
  frame to the process. How this page frame is initialized, however, depends on the kind
  of page and on whether the page was previously accessed by the process. In particular:
  
  1. Either the page was never accessed by the process and it does not map a disk file, or
     the page maps a disk file. The kernel can recognize these cases because the Page
     Table entry is filled with zerosi.e., the pte_none macro returns the value 1.

  2. The page belongs to a non-linear disk file mapping (see the section "Non-Linear
     Memory Mappings" in Chapter 16). The kernel can recognize this case, because the
     Present flag is cleared and the Dirty flag is seti.e., the pte_file macro returns the
     value 1.

  3. The page was already accessed by the process, but its content is temporarily saved on
     disk. The kernel can recognize this case because the Page Table entry is not filled
     with zeros, but the Present and Dirty flags are cleared.
 #+BEGIN_EXAMPLE
	entry = *pte;
	if (!pte_present(entry)) {
		/*
		 * If it truly wasn't present, we know that kswapd
		 * and the PTE updates will not touch it later. So
		 * drop the lock.
		 */
		if (pte_none(entry))
			return do_no_page(mm, vma, address, write_access, pte, pmd);
		if (pte_file(entry))
			return do_file_page(mm, vma, address, write_access, pte, pmd);
		return do_swap_page(mm, vma, address, pte, pmd, entry, write_access);
	}
 #+END_EXAMPLE
   pte_present()返回0时都包含那3种情况,执行do_no_page()是第1种情况,执行do_file_page()是第2
   种情况,执行do_swap_page()是第3种情况.
- 对于写时复制的情况就是调用do_wp_page(),是写的请求,但是pte项表示不能写
 #+BEGIN_EXAMPLE
	if (write_access) {
		if (!pte_write(entry))
			return do_wp_page(mm, vma, address, pte, pmd, entry);
 #+END_EXAMPLE 
- 还有其它的情况就是错误了,所以返回VM_FAULT_MINOR.
- 所以一共有5种情况,而这5种情况分别包含在5个return里.
- 这个函数可以看出,若页在内存里,且是写访问,且pte项表示能写,那么就有问题了,在这种情况下是不
  应该调用这个函数的.在这种情况下还是要把页表项给dirty的,还会把pte项给young了,又dirty但又
  young,那就是不合理的.但是在ULK的Figure 9-5里是没有体现这种情况的.
** static int do_no_page(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, int write_access, pte_t *page_table, pmd_t *pmd)
*** mm/memory.c:
- 这个是有两种情况的,若没有是匿名区就调用do_anonymous_page(),否则就是映射到一个文件.
- 这是用了vm_area_struct->vm_page_prot, ulk:vm_page_prot Access permissions for the page
  frames of the region.
- 
** static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma, pte_t *page_table, pmd_t *pmd, int write_access, unsigned long addr)
*** mm/memory.c:
- 要是写访问才会做事情
- anon_vma_prepare()的作用就是分配一个struct anon_vma结构体
- 调用alloc_zeroed_user_highpage()转而调用alloc_page_vma()转而直接调用alloc_pages()或独立实
  现一个函数来分配一个页框,分配时会使用__GFP_ZERO来初始化页为0
- 在ULK里说了为什么要先pte_unmap()再调用pte_offset_map(),因为alloc_zeroed_user_highpage()可
  能会休眠.
- mm->rss在这里増加的
- 以下的代码最终的作用是,得出一个pte页表项的值,权限是用vma->vm_page_prot,指向的页框是page,
  还把它dirty,还根据VM_WRITE设置write的权限。
 #+BEGIN_EXAMPLE
		entry = maybe_mkwrite(pte_mkdirty(mk_pte(page,
							 vma->vm_page_prot)),
				      vma);
 #+END_EXAMPLE 
- 设置了Page_Referenced
- 因为page_table是一个指向这个虚拟地址所映射的一个pte页表项的指针。所以调用
 #+BEGIN_EXAMPLE
	set_pte_at(mm, addr, page_table, entry);
 #+END_EXAMPLE 
  把entry赋给了page_table所指向的pte页表项.
- 返回VM_FAULT_MINOR表示成功。
- 读的时候是不分配页的。
** static int do_wp_page(struct mm_struct *mm, struct vm_area_struct * vma,	unsigned long address, pte_t *page_table, pmd_t *pmd, pte_t pte)
*** mm/memory.c:
- ULK：If only one process owns the page, Copy On Write does not apply, and the process
  should be free to write the page. Basically, the function reads the _count field of the
  page descriptor: if it is equal to 0 (a single owner), COW must not be done. Actually,
  the check is slightly more complicated, because the _count field is also increased when
  the page is inserted into the swap cache (see the section "The Swap Cache" in
  Chapter 17) and when the PG_private flag in the page descriptor is set. However, when
  COW is not to be done, the page frame is marked as writable, so that it does not cause
  further Page Fault exceptions when writes are attempted
- PG_locked这个标志的作用好像是把页临时锁定在内存不被交换出去的作用所以调用了TestSetPageLocked()
 #+BEGIN_EXAMPLE
	if (!TestSetPageLocked(old_page)) {
 #+END_EXAMPLE 
- 在handle_pte_fault()里调用的do_wp_page()所使用的参数中entry的值是pte所指向的pte页表项的值
- 如果页没有被锁且can_share_swap_page()为真，那么就把这个pte页表项的dirty,young,write标志给
  设置了
 #+BEGIN_EXAMPLE
	if (!TestSetPageLocked(old_page)) {
		int reuse = can_share_swap_page(old_page);
		unlock_page(old_page);
		if (reuse) {
			flush_cache_page(vma, address, pfn);
			entry = maybe_mkwrite(pte_mkyoung(pte_mkdirty(pte)),
					      vma);
			ptep_set_access_flags(vma, address, page_table, entry, 1);
			update_mmu_cache(vma, address, entry);
			lazy_mmu_prot_update(entry);
			pte_unmap(page_table);
			spin_unlock(&mm->page_table_lock);
			return VM_FAULT_MINOR;
		}
	}
 #+END_EXAMPLE
 如果执行上面的代码没有执行，那么就页不能共享了，且页被锁住了。所以就应该拷贝了。
- 为什么PG_reserved没有设置的要増加计数呢？是不是有设置了说明这个页不会被交换，所以就不
  用_count来计数了？ULK这样说:To avoid race conditions,get_page( ) is invoked to increase
  the usage counter of old_page before starting the copy operation
 #+BEGIN_EXAMPLE
	if (!PageReserved(old_page))
		page_cache_get(old_page);
 #+END_EXAMPLE 
- 先分配一个struct anon_vma 结构体再分配一个页框
- 分配页框时要判断是不是ZERO_PAGE页，是不是的区别是分配的页有没有用__GFP_ZERO来分，因为
  alloc_zeroed_user_highpage()就是调用alloc_page_vma()的，但是除了用GFP_HIGHUSER之外还
  用_GFP_ZERO，若不是ZERO_PAGE就要用copy_user_highpage()(下面有介绍)来拷贝页了。
- ULK:Because the allocation of a page frame can block the process, the function checks
  whether the Page Table entry has been modified since the beginning of the function (pte
  and *page_table do not have the same value). In this case, the new page frame is
  released, the usage counter of old_page is decreased (to undo the increment made
  previously), and the function terminates.
 #+BEGIN_EXAMPLE
	page_table = pte_offset_map(pmd, address);
	if (likely(pte_same(*page_table, pte))) {
		if (PageAnon(old_page))
			dec_mm_counter(mm, anon_rss);
		if (PageReserved(old_page))
			inc_mm_counter(mm, rss);
		else
			page_remove_rmap(old_page);
		flush_cache_page(vma, address, pfn);
		break_cow(vma, new_page, address, page_table);
		lru_cache_add_active(new_page);
		page_add_anon_rmap(new_page, vma, address);

		/* Free the old page.. */
		new_page = old_page;
	}
	pte_unmap(page_table);
	page_cache_release(new_page);
	page_cache_release(old_page);
 #+END_EXAMPLE 
  PageAnon()检查页是否为匿名页，低位为1时为匿名页.anao_rss:Number of page frames assigned
  to anonymous memory mappings.为什么PageAnon()为真就要减呢?为什么旧的页若是保留页就增加
  rss呢?若旧的页是保留页那么新的页也是保留的吗?新页的pte页表项是怎么被建立的呢?下面有
  page_remove_rmap()的介绍,就是说若旧页是保留的,那么page->_mapcount是不会变的,这是不是说
  page->_mapcount是对保留页无效的呢?要注意参数page_table是新页所对应的pte页表项,就是
  address所对应的pte页表项的指针,就是发生缺页异常的pte页表项的指针.所以要调用break_cow()(下
  页有介绍)来用新页重建page_table页表项.旧页是减了anon_rss,新页是在page_add_anon_rmap()加了
  anon_rss,但注意新页和旧页不是属于同一个进程的,所以vm_area_struct是不同的.
** static inline void break_cow(struct vm_area_struct * vma, struct page * new_page, unsigned long address, pte_t *page_table)
*** mm/memory.c:
- 重建的页表项是dirty的.写不写要看vm_area_struct
** void page_add_anon_rmap(struct page *page, struct vm_area_struct *vma, unsigned long address)
*** mm/rmap.c:
- 为该匿名页插入反向映射数据结构的内容.
- 增加了anon_rss
- 为什么要把vm_area_struct->anon_vma转成(void*)类型之后再加1呢?
** void page_remove_rmap(struct page *page)
*** mm/rmap.c:
- The _mapcount field stores the number of Page Table entries that refer to the page
  frame. The counter starts from -1: this value means that no Page Table entry references
  the page frame. Thus, if the counter is zero, the page is non-shared, while if it is
  greater than zero the page is shared.
- 这个函数就只是把page->_mapcount减1而已.
** #define page_cache_release(page)	put_page(page)
*** include/linux/pagemap.h:
** void put_page(struct page *page)
*** mm/swap.c:
- 若页不是保留的且页没有引用就调用__page_cache_release()
** void fastcall __page_cache_release(struct page *page)
*** mm/swap.c:
- 若页的计数不为空,那么直接把page设为NULL就是释放了.
- 若页的计数为空,那么就调用free_hot_page()来释放页.
** static inline void copy_user_highpage(struct page *to, struct page *from, unsigned long vaddr)
*** include/linux/highmem.h:
- 在内核态下从一个用户态的页框的数据拷贝到另一个用户态的页框去。
- 是使用内核的高端内存固定映射来拷贝的。使用了KM_USER0和KM_USER1这两个窗口。
- 调用了copy_user_page()，转而调用了copy_page()转而调用mmx_copy_page()或memcpy().
** struct page *follow_page(struct mm_struct *mm, unsigned long address, int write)
*** mm/memory.c:
- 这个函数调用
 #+BEGIN_EXAMPLE
	return __follow_page(mm, address, /*read*/0, write);
 #+END_EXAMPLE 
** static struct page *__follow_page(struct mm_struct *mm, unsigned long address, int read, int write)
*** mm/memory.c:
- follow_page函数是从进程的页表中搜索特定地址对应的页面对象。就是得到用户态虚拟地址
  (0-3G)address所在的页框的struct page指针，若一些权限不符合就不能获取，如要present,要
  write权限要与参数write对应，read也要对应,就这三个权限。
- 若已成功获取struct page *了，那么就要结合参数write和页表项和struct page标志来设置dirty
 #+BEGIN_EXAMPLE
			if (write && !pte_dirty(pte) && !PageDirty(page))
				set_page_dirty(page);
 #+END_EXAMPLE 
  为什么要pte_dirty()和PageDirty()同时假才可以呢？

  还要修改access位
- 所以这个函数不仅仅是获取struct page而己，还修改了东西的。
- 这个函数在某些级的页表没有对应的页表项或是read/write权限不对就会返回NULL
- 这个函数的开头与get_user_pages()第一个if一样是调用
  pgd_offset(),pud_offset(),pmd_offset(),pte_offset_map()来得到一个在用户态的pte页表里
  address所对应的pte页表项。
** struct vm_area_struct *find_extend_vma(struct mm_struct *mm, unsigned long addr)
*** mm/mmap.c:
- 在配置了CONFIG_STACK_GROWSUP时，是用find_vma_prev()来找被extend的vma_area_struct的，而设
  CONFIG_STACK_GROWSDOWN时是用find_vma()来找的。因为如果找到的vma_area_struct若不包含参数
  addr,那么调用expand_stack()时使用的参数就不一样了，CONFIG_STACK_GROWSUP是用前一个区间调用
  expand_stack()的,而CONFIG_STACK_GROWSDOWN无论是否包含addr,都会用find_vma()找到的
  vm_area_struct来调用expand_stack().就如expand_stack()的注释所写的，在
  CONFIG_STACK_GROWSUP时： vma is the first one with address > vma->vm_end.  Have to
  extend vma.否则vma is the first one with address < vma->vm_start.  Have to extend vma.
 #+BEGIN_EXAMPLE
	vma = find_vma_prev(mm, addr, &prev);
	if (vma && (vma->vm_start <= addr))
		return vma;
 #+END_EXAMPLE 
 #+BEGIN_EXAMPLE
	vma = find_vma(mm,addr);
	if (!vma)
		return NULL;
	if (vma->vm_start <= addr)
		return vma;
 #+END_EXAMPLE 
- 这个函数的功能就是先找出一个包含addr地址的区间，若没有区间包含这个地址，那么就把一个附近
  地区间的范围扩展到addr,因为addr附近有两个不同区间，至于选哪个就要看地址是向上还是向下増长
  的了。
** int expand_stack(struct vm_area_struct *vma, unsigned long address)
*** mm/mmap.c:
- 与上一个函数一样也有两个版本。
- 无论是向下还是向上增长，但是vma->vm_end还是大于vma->vm_start的
- 若是向下増长，参数address要这样对齐
 #+BEGIN_EXAMPLE
	address &= PAGE_MASK;
 #+END_EXAMPLE 
  否则是这样,但是为什么要加上一个4呢？
 #+BEGIN_EXAMPLE
	address += 4 + PAGE_SIZE - 1;
	address &= PAGE_MASK;
 #+END_EXAMPLE 
- acct_stack_growth()只是修改了统计变量，是调用之后才修改vm_area_struct->vm_start和
  vm_area_struct->vm_pgoff(向下増长)，vm_area_struct->vm_end(向上増长).
- 所以这个函数最主要是修改了vm_area_struct->vm_start或vm_area_struct->vm_end.
** static int acct_stack_growth(struct vm_area_struct * vma, unsigned long size, unsigned long grow)
*** mm/mmap.c:
- 这个函数也会做与其它一些函数类似的一系列的检查，如调用may_expand_vm()检查RLIMIT_AS是否超
  限，RLIMIT_STACK是否超限，RLIMIT_MEMLOCK是否超限。
- 作完检查之后就可以增加mm->total_vm了，虽然还没有扩展。
- 因为__vm_stat_account()只是对mm->shared_vm, mm->exec_vm, mm->stack_vm, mm->reserved_vm做
  处理。所以这个函数也只是修改了这些统计变量。而没有修改vm_area_struct->vm_start和
  vm_area_struct->vm_end.
** int do_munmap(struct mm_struct *mm, unsigned long start, size_t len)
*** mm/mmap.c:
- ULK:The function goes through two main phases. In the first phase (steps 16), it scans
  the list of memory regions owned by the process and unlinks all regions included in the
  linear address interval from the process address space. In the second phase (steps 712),
  the function updates the process Page Tables and removes the memory regions identified
  in the first phase.
** static void detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma, struct vm_area_struct *prev, unsigned long end)
*** mm/mmap.c:
- 这个函数是以参数vma这个内存区开始,一直到end这个虚拟地址所在的内存区为止,把这个范围内的
  vma都给删掉.但从代码可以看出,无论如何,都要把参数vma这个内存区给删掉.所谓的删掉就是把它从
  红黑树里删掉,并递减map_count,但是没有把内容给清掉,没有把vma的空间清掉.
- map_count Number of memory regions
- 注意要把mmap_cache给清掉
 #+BEGIN_EXAMPLE
	mm->mmap_cache = NULL;		/* Kill the cache. */
 #+END_EXAMPLE 
- 虽然没有已经把从vma开始到end的内存区从红黑树中删除,但是从vma到end的链接是没有删除的.
** unsigned long unmap_vmas(struct mmu_gather **tlbp, struct mm_struct *mm, struct vm_area_struct *vma, unsigned long start_addr, unsigned long end_addr, unsigned long *nr_accounted, struct zap_details *details)
*** mm/memory.c:
- ULK:to clear the Page Table entries covering the linear address interval and to free the
  corresponding page frames
- 这个函数不一定会把参数vma给处理了,若是start_addr大于vma->vm_start.
 #+BEGIN_EXAMPLE
	for ( ; vma && vma->vm_start < end_addr; vma = vma->vm_next) {
		unsigned long end;

		start = max(vma->vm_start, start_addr);
		if (start >= vma->vm_end)
			continue;
 #+END_EXAMPLE 
- 还要注意一种情况就是有可能start_addr和end_addr都在一个vma的里.
- VM_HUGETLB The pages in the region are handled through the extended paging mechanism 
- 
** void unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
*** arch/i386/mm/hugetlbpage.c:
- 这个函数的作用就是把从虚拟地址address到end之间的pte页表项给清掉,把被清掉的页表项所对应的
  页框给释放(put_page()在没有被引用时才会真正被释放掉)
- 这个函数看得比较容易明白,就是先用huge_pte_offset找到要处理的地址的pte页表项的指针,找到之
  后就通过ptep_get_and_clear()获取pte页表项指针的值并把pte页表项清掉,用获取到的pte页表项的
  值通过pte_page()找到对应的页框的struct page,再用找到的struct page通过put_page()把这个页框
  给释放掉.
- 在这个函数会减mm->rss
** static void unmap_page_range(struct mmu_gather *tlb, struct vm_area_struct *vma, unsigned long addr, unsigned long end, struct zap_details *details)
*** mm/memory.c:
- 把所有在addr到end这个范围内的所有pgd页表项都处理一遍，处理一遍就是调用一次zap_pud_range()
** static inline void zap_pud_range(struct mmu_gather *tlb, pgd_t *pgd, unsigned long addr, unsigned long end, struct zap_details *details)
*** mm/memory.c:
- 把所有在addr到end这个范围内的所有pud页表项都处理一遍，处理一遍就是调用一次zap_pmd_range()
** static inline void zap_pmd_range(struct mmu_gather *tlb, pud_t *pud, unsigned long addr, unsigned long end, struct zap_details *details)
*** mm/memory.c:
- 把所有在addr到end这个范围内的所有pte页表项都处理一遍，处理一遍就是调用一次zap_pte_range()
** static void zap_pte_range(struct mmu_gather *tlb, pmd_t *pmd, unsigned long addr, unsigned long end, struct zap_details *details)
*** mm/memory.c:
- 先用pte_offset_map()找到pte页表项的指针。
- 这个函数所调用的free_swap_and_cache()转而调用page_cache_release()转而调用put_page()最后释放页框。
- 如果pte_present()为真，那么释放页的地方在tlb_remove_page()
- 若页是保留页那么这个页是不会被释放的
 #+BEGIN_EXAMPLE
			if (pfn_valid(pfn)) {
				page = pfn_to_page(pfn);
				if (PageReserved(page))
					page = NULL;
			}
 #+END_EXAMPLE 
 但是还是要把pte页表给清掉的
 #+BEGIN_EXAMPLE
			ptent = ptep_get_and_clear(tlb->mm, addr, pte);
 #+END_EXAMPLE 
- 为什么pte为dirty时还要把page给dirty呢？都要回收了还有什么用呢？
 #+BEGIN_EXAMPLE
			if (pte_dirty(ptent))
				set_page_dirty(page);
 #+END_EXAMPLE 
- 若是匿名区就要减anon_rss,否则且pte为young,那么就把页给改成accessed,这是为什么呢？
 #+BEGIN_EXAMPLE
			if (PageAnon(page))
				dec_mm_counter(tlb->mm, anon_rss);
			else if (pte_young(ptent))
				mark_page_accessed(page);
 #+END_EXAMPLE 
- 如果页不present，且pte没有映射到文件那么就调用free_swap_and_cache()
** #define pgoff_to_pte(off)
*** include/asm-i386/pgtable-2level.h:
- 把一个pte页表项的内容设成这个地址所对应的文件的一个偏移量，一个偏移量表示4k大小。
- 这个宏有很多内容
- http://blog.csdn.net/dog250/article/details/5303232

** struct page * find_get_page(struct address_space *mapping, unsigned long offset)
*** mm/filemap.c:
- 这个函数的作用是在radix树里找一个页。
- 先调用radix_tree_lookup()在radix基树里找符合的页
- 再调用page_cache_get()增加页的使用计数。
** void *radix_tree_lookup(struct radix_tree_root *root, unsigned long index)
*** lib/radix-tree.c:
- 从以下的代码可以看出,radix树最多只有5层,因为RADIX_TREE_MAP_MASK是64,就是占6位，而index是
  32位的。
 #+BEGIN_EXAMPLE
			((*slot)->slots +
				((index >> shift) & RADIX_TREE_MAP_MASK));
		shift -= RADIX_TREE_MAP_SHIFT;
 #+END_EXAMPLE
** unsigned find_get_pages(struct address_space *mapping, pgoff_t start, unsigned int nr_pages, struct page **pages)
*** mm/filemap.c:
- 这个函数作用是从radix树里找一堆连续的页。start是指要找的页的第一个索引，所找的所有页的索
  引是连续的。
- 先调用radix_tree_gang_lookup()找页。
** unsigned int radix_tree_gang_lookup(struct radix_tree_root *root, void **results, unsigned long first_index, unsigned int max_items)
*** lib/radix-tree.c:
- 主要是调用__lookup()来找出连续的页,但是因为调用__lookup()一次只能找出在一个叶子结点上的连
  续的页, 所以若想要找的连续的页有跨过一个叶子节点,那么就要多次调用__lookup().
** struct page *find_lock_page(struct address_space *mapping, unsigned long offset)
*** mm/filemap.c:
- 这个函数与find_get_page()的不同就是获得页之后就把它锁住(PG_locked),锁住的作用就是只有这个
  进程可以访问该页,与PG_reserved不一样.就只是加多了以下的代码:
 #+BEGIN_EXAMPLE
		if (TestSetPageLocked(page)) {
			read_unlock_irq(&mapping->tree_lock);
			lock_page(page);
			read_lock_irq(&mapping->tree_lock);

			/* Has the page been truncated while we slept? */
			if (page->mapping != mapping || page->index != offset) {
				unlock_page(page);
				page_cache_release(page);
				goto repeat;
			}
		}
 #+END_EXAMPLE 
- 若发现页以被锁住就调用lock_page()阻塞等待把页锁住.
- 若发现页已不映射文件或不映到原来的文件或映射到原来文件但不是之前的偏移量就释放页.但不是简
  单地返回,而是重新执行radix_tree_lookup()来找页.
 #+BEGIN_EXAMPLE
			if (page->mapping != mapping || page->index != offset) {
 #+END_EXAMPLE 
** struct page *find_trylock_page(struct address_space *mapping, unsigned long offset)
*** mm/filemap.c:
- 与find_lock_page()的不同就是以下的代码:
 #+BEGIN_EXAMPLE
	if (page && TestSetPageLocked(page))
		page = NULL;
 #+END_EXAMPLE 
** struct page *find_or_create_page(struct address_space *mapping, unsigned long index, unsigned int gfp_mask)
*** mm/filemap.c:
- 这个函数的设计有点意思,为什么同时需要page和cached_page两个相同的变量呢?
** int radix_tree_preload(int gfp_mask)
*** lib/radix-tree.c:
- 这个函数的作用是把一些radix的节点先分配放到struct radix_tree_preload->nodes数组里作为缓
  冲,使得需要时不因为缺内存而分不了.
- radix_tree_preloads这个变量是PER-cpu变量
- 实现很简单,就是不断地调用kmem_cache_alloc()
** int add_to_page_cache(struct page *page, struct address_space *mapping, pgoff_t offset, int gfp_mask)
*** mm/filemap.c:
- 先调用radix_tree_preload()预先分配好radix结点
- 再调用radix_tree_insert()把页放到radix结点去.
- 也要把页锁在内存里
- 最后就是修改page->mapping,page->index,mapping->nrpages
** void remove_from_page_cache(struct page *page)
*** mm/filemap.c:
- 这个函数从radix树里删除一个页,但是不会释放这个页
- 调用__remove_from_page_cache()
** void __remove_from_page_cache(struct page *page)
*** mm/filemap.c:
- 这个函数调用radix_tree_delete()把有关page的路径的结点按需删掉
- 只是把page->mapping设为0,page->index不修改,也不把page给释放掉
** static inline struct page *__read_cache_page(struct address_space *mapping,unsigned long index,int (*filler)(void *,struct page*),void *data)
*** mm/filemap.c:
- 与find_or_create_page()基本相同,只是多了以下的代码
 #+BEGIN_EXAMPLE
		err = filler(data, page);
		if (err < 0) {
			page_cache_release(page);
			page = ERR_PTR(err);
		}
 #+END_EXAMPLE 
** void fastcall mark_page_accessed(struct page *page)
*** mm/swap.c:
- 有这样的注释
 #_BEGIN_EXAMPLE
/*
 * Mark a page as having seen activity.
 *
 * inactive,unreferenced	->	inactive,referenced
 * inactive,referenced		->	active,unreferenced
 * active,unreferenced		->	active,referenced
 */
 #+END_EXAMPLE 
 从这段注释来看,active标志和reference标志是一个二进制进位的关系,就是若是unreferenced,那么就
 先设为referenced,若是referenced,就要把referenced改为unreferenced并进一位来修改
 active.referenced标志总是取反.
** struct page *read_cache_page(struct address_space *mapping, unsigned long index, int (*filler)(void *,struct page*),void *data)
*** mm/swap.c:
- 先调用__read_cache_page(),再调用mark_page_accessed()
- 如果不是最新的说要调用filler()函数了,但是不是在__read_cache_page()有调用filler()了吗?但是
  要调用filler()就要先把页给锁住,但是锁页的过程中可能被其它的进程把页给更新了.
** static inline int grow_buffers(struct block_device *bdev, sector_t block, int size)
*** fs/buffer.c:
- In order to add a block device buffer page to the page cache, the kernel invokes the
  grow_buffers() function,
- 以下的代码算出的sizebits就是说2^sizebits个块才可以填充一个页,为什么要用幂呢?难道与页的大
  小关系一定是幂的关系吗?注意size是块的大小
  #+BEGIN_EXAMPLE
     do {
		sizebits++;
	} while ((size << sizebits) < PAGE_SIZE);
  #+END_EXAMPLE
- 以下的代码说明若是要以sizebits个块为一组来移动,那么要移index次,所以这个index就是radix的
  偏移量
  #+BEGIN_EXAMPLE
	index = block >> sizebits;
  #+END_EXAMPLE 
- 函数先算好块号,和在radix树的偏移量(index)
- 以下的代码是不是要将块号取个整呢,可以参数block不是buffer page里的第一个块,虽然它要想要找
  的块,但是因为要页内的第一个块好把这个页的所有块都填充了.
  #+BEGIN_EXAMPLE
	block = index << sizebits;
  #+END_EXAMPLE
- 到这里之后,block就是页内第一块的逻辑块号,index就是radix树的索引.
- 这里调用的page_cache_release()是为了减在grow_dev_page()转而调用的link_dev_buffer()转而调
  用的attach_page_buffers()转而调用的page_cache_get()所增加的页使用计数。
- 这里要调用unlock_page()所对应的lock_page()是在grow_dev_page()转而调用的
  find_or_create_page()转而调用的find_lock_page()转而调用的lock_page().
- 
** static struct page *grow_dev_page(struct block_device *bdev, sector_t block, pgoff_t index, int size)
*** fs/buffer.c:
- 先调用find_or_create_page()来找到可以放这个buffer的页
- page_has_buffers()这个函数就是检查PG_private有没有设置,若有设置就说明指向buffer_head的链
  表,page_has_buffers()若没有成立就是ULK说的第一种情况了:The radix tree of the block
  device does not include a page containing the data of the block: in this case a new page
  descriptor must be added to the radix tree.
- 因为调用这个函数之前已经确定radix树里没有一个页是合适要找的页,所以ULK说的三种情况都没有
  一个是找到合适的页的情况.
- 以下的代码成立时就是第二种情况:The radix tree of the block device includes a page
  containing the data of the block, but this page is not a buffer page: in this case new
  buffer heads must be allocated and linked to the page, thus transforming it into a block
  device buffer page.但是不明白怎么会出现大小不是想要的呢?一个块设备会同时存在两种不同的块
  大小吗?
  #+BEGIN_EXAMPLE
		if (bh->b_size == size) {
  #+END_EXAMPLE 
- 会调用init_page_buffers(),来处理所有的struct buffer_head,但是无论在什么时候,一个buffer
  page都会被填满吗?既使是有一些struct buffer_head的数据没有被访问?
- 执行以下的代码就是第三种情况了:The radix tree of the block device includes a buffer page
  containing the data of the block, but the page has been split in blocks of size
  different from the size of the requested block: in this case the old buffer heads must
  be released, and a new set of buffer heads must be allocated and linked to the page.
  #+BEGIN_EXAMPLE
		if (!try_to_free_buffers(page))
  #+END_EXAMPLE
- 若是第一种情况就要分配一个页了,错,不是分配一个页,而是给这个页分配buffer_head,但好像在这
  种情况下没有把这个页插入radix树
  #+BEGIN_EXAMPLE
	bh = alloc_page_buffers(page, size, 0);
  #+END_EXAMPLE 
- 还有一点不明白的就是为什么在调用link_dev_buffers()和init_page_buffers()时要加上
  inode.i_mapping.private_lock锁.ulk:private_lock Usually, spin lock used when managing
  the private_list list.private_list Usually, a list of dirty buffers of indirect blocks
  associated with the inode, The private_list field is the head of a generic list that can
  be freely used by the filesystem for its specific purposes.
  #+BEGIN_EXAMPLE
	spin_lock(&inode->i_mapping->private_lock);
	link_dev_buffers(page, bh);
	init_page_buffers(page, bdev, block, size);
	spin_unlock(&inode->i_mapping->private_lock);
  #+END_EXAMPLE 
- 这个函数有一段这样的注释
  #+BEGIN_EXAMPLE
	 * Link the page to the buffers and initialise them.  Take the
	 * lock to be atomic wrt __find_get_block(), which does not
	 * run under the page lock.
  #+END_EXAMPLE 
  这段注释与它下面的这代码有什么关系呢?
** static void init_page_buffers(struct page *page, struct block_device *bdev,sector_t block, int size)
*** fs/buffer.c:
- 这个函数所使用的buffer_XX(),set_buffer_XX()函数是在buffer_head.h文件里使用宏定义的,所以
  使用cscope是找不到的.还有test_buffer_XX()之类的也是.
- 结合grow_dev_page()和init_page_buffers()来看,进入init_page_buffers()之前在page里的buffer
  链表的struct buffer_head已经分配好了,但是不知道在哪分配的,在grow_dev_page()里调用的
  find_or_create_page()是没有分配struct buffer_head,所以在分配buffer page时没有分配struct
  buffer_head,这也就是为什么在grow_dev_page()里要调用page_has_buffers()一样.因为这个函数是
  用循环来处理所有的struct buffer_head的,且因为在grow_dev_page()里调用这个函数来看,一个页
  里的所有的struct buffer_head的b_size是一样的.原来分配struct buffer_head是在grow_dev_page
  里做的,调用了alloc_page_buffers()来分配.
- 这个函数是在grow_dev_page()因为buffer head的大小不是想要的才会调用,但是为什么在这个函数
  里并没有处理buffer_head.b_size呢?
- 还有一点就是,若是块的大小不一样了,且是变大了,那么一个页的大小可能就不能放原来那么多的
  buffer_haed了,怎么这一点也没在做处理呢?
- 这个函数处理为buffer_head的b_end_io,b_private,b_bdev,b_blocknr
- b_size是在alloc_page_buffers()（下面有介绍）初始化的。
** struct buffer_head *alloc_page_buffers(struct page *page, unsigned long size,int retry)
*** fs/buffer.c:
- 为一个buffer page分配所有的struct buffer_head结构体
- 函数一共初始化了struct buffer_head的b_this_page,b_size,b_page,b_data
- 从这个函数可以看出,一个buffer page的struct buffer_head是一下子全部分配的.
- 可以看出这个struct buffer_head链表还不是一个循环链表.这个要在link_dev_buffers
 #+BEGIN_EXAMPLE
		bh->b_this_page = head;
		bh->b_blocknr = -1;
		head = bh;
 #+END_EXAMPLE
- 调用set_bh_page()来初始化b_page和b_data
- 因为分配给struct buffer_head是从高地址开始，所以若是PAGE_SIZE不能整除size,那么就会在低地
  址区有一块不被使用的空间。
** void set_bh_page(struct buffer_head *bh,struct page *page, unsigned long offset)
*** fs/buffer.c:
- 初始化b_page和b_data
- b_page就是page
- 若是高端内存的时候就把b_data修改成该struct buffer_head所管理的块在页内的偏移量
 #+BEGIN_EXAMPLE
		bh->b_data = (char *)(0 + offset);
 #+END_EXAMPLE
 因为是指针，所以要转换为指针类型。因为不能像低端内存那样找到页框所对应的线性地址，所以高
 端内存就不能像低端内存那样在b_data那里保存struct buffer_head所管理的块的线性地址.
 #+BEGIN_EXAMPLE
		bh->b_data = page_address(page) + offset;
 #+END_EXAMPLE 
** static inline void link_dev_buffers(struct page *page, struct buffer_head *head)
*** fs/buffer.c:
- 在grow_dev_page()里调用的alloc_page_buffers()已经把分配的struct buffer_head都给链起来了，
  但还没有把它们链到page里去了.
- 把struct buffer_head弄成循环的
- 再调用attach_page_buffers()把这个链表赋给页的private.
- 这个函数还增加了页的使用计数。
** int try_to_release_page(struct page *page, int gfp_mask)
*** fs/buffer.c:
- 这个函数的作用是单单释放buffer page的吗?还是释放cache page(包含有buffer_page)，看函数名还
  真的是看不出来的
- 调用address_space->a_ops->releasepage()时释放的应该不是buffer page,而调用
  try_to_free_buffers()这个函数才是释放buffer page的.
- ulk:releasepage Used by journaling filesystems to prepare the release of a page.the
  method is usually not defined for block devices. 
- 在try_to_free_buffers()这个函数里会把页的PG_dirty给清掉,但是又没回写页的内容.这是怎么回
  事呢?
** int try_to_free_buffers(struct page *page)
*** fs/buffer.c:
- 这个函数就是释放buffer page的
- 若PG_writeback设置了就说明这个页正在被页回disk,所以不能释放.
- 在buffer_busy()这个函数里会一个介绍.
- 为什么调用两次drop_buffers()时一个要加锁,一个不用呢?因为在drop_buffers()里调用
  的__remove_assoc_queue()是操作buffer_head.b_assoc_buffers的,所以要private_lock锁
- 最后就是调用free_buffer_head()把buffer_head给释放掉.
- 所以这个函数并没有把页给释放掉,而是把page里的buffer_head给释放了,但是把页的PG_dirty给清
  了.
** static int drop_buffers(struct page *page, struct buffer_head **buffers_to_free)
*** fs/buffer.c:
- BH_Write_EIO:Set if there was an I/O error when writing this block.要设这个标志就要调用
  set_buffer_write_io_error(),而这个函数就只有在end_buffer_write_sync()里调用.
- address_space->flags是Error bits and memory allocator flags,而这个error bits只有两
  个:AS_EIO(IO error on async write),AS_ENOSPC(ENOSPC on async write).而BH_Write_EIO是相关
  的
  #+BEGIN_EXAMPLE
  		if (buffer_write_io_error(bh) && page->mapping)
			set_bit(AS_EIO, &page->mapping->flags);
  #+END_EXAMPLE 
- 若有一个buffer_head是busy的就不能把其它的buffer_head给drop
- b_assoc_buffers Pointers for the list of indirect blocks associated with an inode
- 所以函数里的那两个循环的作用就是(1)设置AS_EIO,(2)看buffer_head是不是busy,(3)把bufer_head从
  b_assoc_buffers链表那里删除,(4)再调用__clear_page_buffers()
- 所以这个函数并没有把buffer_head的空间释放掉,而是把buffer_head和page的状态改变一下而已
- 在第一个循环里可以看出,若发现有一个buffer_head是busy的,那么就不会再扫描后面的buffer_head
  了,也就是说AS_EIO有可能会设置但因为跳出循环了而没有设置.
** static void __clear_page_buffers(struct page *page)
*** fs/buffer.c:
- 这个函数只是处理了page,没有处理buffer_head,
- 处理了page的private的东西,PG_private,privte成员,还有_count成员
** static inline int buffer_busy(struct buffer_head *bh)
*** fs/buffer.c:
- The b_count field of the buffer head is a usage counter for the corresponding block
  buffer. The counter is increased right before each operation on the block buffer and
  decreased right after. The block buffers kept in the page cache are examined both
  periodically and when free memory becomes scarce, and only the block buffers having null
  usage counters may be reclaimed.若计数不为0,表示有其它的内核路经在操作这个buffer_head,这
  也算是busy,在即BH_Dirty又BH_Lock的时候也是busy,所谓的busy就是不能把这个buffer_head给drop
  掉,若仅仅是BH_Lock是可以把它给drop掉的,若仅仅是BH_Lock也是可以把它drop掉的,但是为什么
  BH_Lock也可以呢?可能是虽然把buffer_head给释放了,但是只要页的PG_dirty还在就不会把一个脏页
  给释改掉.但是奇怪的是在try_to_free_buffers()函数里调用完调用这个函数的drop_buffers()之后
  会调用clear_page_dirty()把页的PG_dirty清掉,这是为什么呢?在try_to_free_buffers()有一段这
  样的注释
  #+BEGIN_EXAMPLE
		/*
		 * If the filesystem writes its buffers by hand (eg ext3)
		 * then we can have clean buffers against a dirty page.  We
		 * clean the page here; otherwise later reattachment of buffers
		 * could encounter a non-uptodate page, which is unresolvable.
		 * This only applies in the rare case where try_to_free_buffers
		 * succeeds but the page is not freed.
		 */
  #+END_EXAMPLE 
  那调用完这个函数之后到底会不会把这个page buffer的内容写回disk呢?
** static inline struct buffer_head *lookup_bh_lru(struct block_device *bdev, sector_t block, int size)
*** fs/buffer.c:
- 注释:
  #+BEGIN_EXAMPLE
 * Look up the bh in this cpu's LRU.  If it's there, move it to the head.
  #+END_EXAMPLE 
- 每个cpu都有各自的bh_lru。
- 这个函数是用循环扫描整个lru.bhs数组,一个一个判断当前的buffer_head是不是要找的
  buffer_head,判断的条件是
  #+BEGIN_EXAMPLE
		if (bh && bh->b_bdev == bdev &&
				bh->b_blocknr == block && bh->b_size == size) 
  #+END_EXAMPLE 
  块设备要一样,块号要一样,块大小要一样.但是块大小不是在一个块设备上是固定的呢?
- 若找到了想要的buffer_head,那么就把它放到lru.bhs数组的第一个元素里,因为是数组,所以只能把
  其它的元素整体往后移
  #+BEGIN_EXAMPLE
				while (i) {
					lru->bhs[i] = lru->bhs[i - 1];
					i--;
				}
  #+END_EXAMPLE
- 为什么找到了之后要增加buffer_head的计数呢?
** static struct buffer_head * __find_get_block_slow(struct block_device *bdev, sector_t block, int unused)
*** fs/buffer.c:
- i_blkbits是块大小的2的幂次方,这也说明一个struct block_device同时只能有一个块大小.
- 下面的代码是找到该块所在radix树的page buffer的索引号,因为一个页可以有多个
  块,PAGE_CACHE_SHIFT就是PAGE_SHIFT
  #+BEGIN_EXAMPLE
	index = block >> (PAGE_CACHE_SHIFT - bd_inode->i_blkbits);
  #+END_EXAMPLE
- 会用find_get_page()找页,若找不到就退出,而不会分配页.
- 若虽然有了页在radix,但是页没有buffer head,也会退出,但是怎么可能在radix里的页没有buffer
  head呢?有这种情况就是调用try_to_release_page()的时候会把页里的buffer head给释放掉,而页还
  在radix里.
  #+BEGIN_EXAMPLE
	if (!page_has_buffers(page))
		goto out_unlock;
  #+END_EXAMPLE
- 然后用一个循环来从页的buffer_head链表里找出块号等于block的buffer_head,若找到就要增加
  buffer_head计数并返回相应的buffer_head
  #+BEGIN_EXAMPLE
		if (bh->b_blocknr == block) {
			ret = bh;
			get_bh(bh);
			goto out_unlock;
		}
  #+END_EXAMPLE 
- 若找的过程中发现有buffer_head的BH_Mapped没有设置且没有找到相应的块(可能BH_Mapped没设的块
  不是想要找的块),就会打印出一堆东西说明,这种情况是因为块设备文件io和getblk()之间有竞争,有
  注释:
  #+BEGIN_EXAMPLE
	/* we might be here because some of the buffers on this page are
	 * not mapped.  This is due to various races between
	 * file io on the block device and getblk.  It gets dealt with
	 * elsewhere, don't buffer_error if we had some unmapped buffers
	 */
  #+END_EXAMPLE
- 最后要调用page_cache_release()把页的计数减去,这次的计数是在这个函数里所调用的
  find_get_page()转而调用的page_cache_get()增加的
- 所以这个函数的结果就是通过block块号在radix树里先找到buffer page,再在这个页里找到相应的
  buffer_head返回,不会分配什么也不会释放什么
** #define touch_buffer(bh)	mark_page_accessed(bh->b_page)
*** include/linux/buffer_head.h:
** static void bh_lru_install(struct buffer_head *bh)
*** fs/buffer.c:
- 因为每一个cpu都有一个bh_lru,那会不会一个buffer_head会同时出现在多个bh_lru里呢?应该会的
- 这个函数里调用的__brelse()仅仅是调用了put_bh()来减使用计数器
- 以下的循环就使得把参数的buffer_head放到第一位,原来的整体往后移,若参数buffer_head原来就在
  数组里,那么移动的过程中就把它怱略掉
  #+BEGIN_EXAMPLE
		bhs[out++] = bh;
		for (in = 0; in < BH_LRU_SIZE; in++) {
			struct buffer_head *bh2 = lru->bhs[in];

			if (bh2 == bh) {
				__brelse(bh2);
			} else {
				if (out >= BH_LRU_SIZE) {
					BUG_ON(evictee != NULL);
					evictee = bh2;
				} else {
					bhs[out++] = bh2;
				}
			}
		}
  #+END_EXAMPLE 
  所以这个函数就是把一个buffer_head插入bh_lru.bhs数组开头的一个功能.
** #define touch_buffer(bh)	mark_page_accessed(bh->b_page)
*** include/linux/buffer_head.h:
** struct buffer_head *__find_get_block(struct block_device *bdev, sector_t block, int size)
*** fs/buffer.c:
- 这个函数先用lookup_bh_lru()在bh_lru里找有没有这个块号和大小相同的buffer_head,没有就
  用__find_get_block_slow()在radix树里找,找到了就加到bh_lru_install()里去,若找到就用
  mark_page_accessed()标记一下.
** static struct buffer_head *__getblk_slow(struct block_device *bdev, sector_t block, int size)
*** fs/buffer.c:
- 这个函数只有__getblk()调用
- 使用死循环来获得buffer head,就别说这个函数一定会返回buffer head.
- 先用__find_get_block(),若没找到就grow_buffers()来分配,若分配不成功就调用
  free_more_memory()释放内存
- 若是grow_buffers()分配成功就会进入__find_get_block(),返回会这个buffer_head已经在bh_lru里
  了,计数也增加了.
** struct buffer_head *__bread(struct block_device *bdev, sector_t block, int size)
*** fs/buffer.c:
- 这个函数会调用__getblk()来找到buffer_head,但这个buffer_head不一定是最新的内容,或是因为
  在__getblk()里调用grow_buffers()分配的,还没有内容,所以BH_Uptodate还有设置.而调
  用__bread_slow()就是从磁盘读取这个块的数据的.但是从__bread_slow()这个函数来看,返回时不一
  定是buffer_head,有可能还是null,因为虽然读了数据,但是可能还不是最新的.所以__bread()返回的
  有可能是null
** int submit_bh(int rw, struct buffer_head * bh)
*** fs/buffer.c:
- BH_Ordered:Set if the block should be written strictly after the blocks submitted before
  it (used by journaling filesystems )
- bio.bi_flags的WRITE_BARRIER是在BH_Ordered设置了,且是对该块进得写的时候才会设置的.
- BH_req:Sets the BH_Req flag of the buffer head to record that the block has been
  submitted at least one time;
- 若这个buffer_head之前写时有过io错,且这次又是写,那么就把BH_Write_io_error给清了.
  #+BEGIN_EXAMPLE
	if (test_set_buffer_req(bh) && (rw == WRITE || rw == WRITE_BARRIER))
		clear_buffer_write_io_error(bh);
  #+END_EXAMPLE 
  所以代码到这里为止就(1)确定了bio的读写方式和(2)设置BH_req,可能清BH_Write_io_error.
  
  之后的代码就是建一个bio并提交它.
- BH_Write_io_error只有调用set_buffer_write_io_error()才设置,在它又只被
  end_buffer_write_sync()调用.buffer_head.b_end_io是赋以end_buffer_write_sync()这个函兹为
  的,而end_bio_bh_io_sync()这个函数又是调用buffer_head.b_end_io()的,而bio.bi_end_io又是赋
  以end_bio_io_sync()的,所以在写磁盘时,若出现错时就会调用end_bio_bh_io_sync()转而调用
  end_buffer_write_sync(),转而设置了BH_Write_io_error.
- 所以是一个buffer_head一个buffer_head来提交的,而不是一个页一个页来提交的,但是上层来说是一
  个页一个页处理的,若一个页里有一个buffer_head是脏的,那每就认为这个页是脏的,这时对于上层来
  说这个页是要刷回disk的,但是真真在刷的时候不是把在这个页里的所有buffer_head都刷到disk,而
  是设置了BH_Dirty的buffer_head才会刷到disk里去.
** void end_buffer_write_sync(struct buffer_head *bh, int uptodate)
*** fs/buffer.c:
- 这个函数就是在写完buffer_head的数据到disk之后调用的函数.
- 若成功就只是简单设置BH_Uptodate,否则说设置BH_Write_io_error,清BH_Uptodate
** void ll_rw_block(int rw, int nr, struct buffer_head *bhs[])
*** fs/buffer.c:
- 这个函数是把nr个buffer_head给提交用的,提交的buffer_head在bhs里
- 若发现buffer_head是锁住的BH_Locked,就不能提交,若要提交,就要先把页给锁住,那么锁住的意思是
  不是其它的进程不能对这个页进行任何的操作呢?如果获得了锁那还有必要增加计数器吗?但是代码里
  是增加了计数器
  #+BEGIN_EXAMPLE
		if (test_set_buffer_locked(bh))
			continue;

		get_bh(bh);
  #+END_EXAMPLE
- 要注意设置了BH_Dirty的buffer_head才会提交,且提交前要把这个标志给清掉,但是要注意到
  submit_bh()有可能会失败,会设置BH_Write_io_error,这种情况也会清这个标志.且要设好b_end_io才
  可以提交.下次还是写提交这个buffer_head的时候就会把BH_Write_io_error给清掉.
- 读的时候要看BH_Uptodate有没有设置,没有设置才可以提交,但不能设置它,这个标志是在
  end_buffer_read_sync()里根据bio.bi_flags的BIO_UPTODATE来设置的.这点和写时不一样,因为
  BH_Uptodate可以反应buffer_head的数据是不是最新的,是否与磁盘的数据一致,而BH_Dirty并不能说
  明磁盘的数据是否与buffer_head的一致.这同时也说明dirty和uptodate的语义是有差别的,dirty是
  说明这个buffer_head有没有必要把数据回写,而不管是不是uptodate.而uptodate说明这个内存里的
  数据是否与磁盘的一致.这点也可以在end_buffer_write_sync()里看出来,在写成功时,就是
  BIO_UPTODATE设置了,就会设置BH_Uptodate,说明与磁盘的数据是一致的,而写失败时BIO_UPTODATE就
  没有设置,这里就会设置BH_Write_io_error且清BH_Uptodate说明写错了且与磁盘的数不一致,但是在
  调用submit_bh()之前已经把BH_Dirty给清了,所以BH_dirty不设置不能说明是否与磁盘的数据一致.所
  以BIO_UPTODATE应该是说明数据有没有传输成功(读/写)
** static int __pdflush(struct pdflush_work *my_work)
*** mm/pdflush.c:
- 要看这个函数就要结合pdfflush_operation()
- 先是初始化一个自已用的struct pdflush_work,
- 工作原理是这样子的,每一个回收线程都是执行pdflush函数的,而这个函数就是直接调
  用__pdflush().__pdflush()会在一开始初始化自已pdflush_work,注意pdflush_work是没有fn的,初
  始就只是把who设成自已而已,进入循环之后就把my_work插入到pdfluh_list链表里了,插入之后就体
  眠,进入体眠之后就该pdflush_operation()出场了,pdflush_operation()会把my_work拿出来,然后会
  把pdflush_operation()的参数赋给pdflush_work.fn,把参数赋给pdflush_work.arg0,之后就是唤醒
  pdflush_work.who.现在又回到了__pdflush()了,被唤醒之后就执行pdflush_work.fn函数了.由些可
  以看出,回收函数是不一样的.
- 在函数的一开始要设置进程的标志PF_FLUSHER,这个标志只有在__pdflush()设置,没有地方清,在
  current_is_pdflush()判断一下.这个标志说明这个进程是pdflush线程
- 执行完pdflush_work之后会马上处理要不要创建多一个线程的事,创建多一个线程是有条件的,就是
  pdflush_list为空有1s,且pdflush线程数少于8个
  #+BEGIN_EXAMPLE
		if (jiffies - last_empty_jifs > 1 * HZ) {
			/* unlocked list_empty() test is OK here */
			if (list_empty(&pdflush_list)) {
				/* unlocked test is OK here */
				if (nr_pdflush_threads < MAX_PDFLUSH_THREADS)
					start_one_pdflush_thread();
			}
		}
  #+END_EXAMPLE
  pdflush_list为空要注意,在pdflush_operation()有一段这样的代码
  #+BEGIN_EXAMPLE
		pdf = list_entry(pdflush_list.next, struct pdflush_work, list);
		list_del_init(&pdf->list);
		if (list_empty(&pdflush_list))
			last_empty_jifs = jiffies;
		pdf->fn = fn;
		pdf->arg0 = arg0;
		wake_up_process(pdf->who);
  #+END_EXAMPLE
  可以看出这个1s的计算是从last_empty_jifs被赋以jiffies开始,到__pdflush()被唤醒执行到判断时
  间有没有超过1s,那么这个时间超过1s能说明什么呢?而这个时间的确是pdflush_list为空超过1s,难
  道是说明了有很多进程在执行,或是说pdflush_work.fn执行了很久.
- pdflush_work.fn是只执行一次,如果__pdflush()还没有达到退出的要求,那么就把自已的
  pdflush_work重新插入pdflush_list里去,再等待pdflush_operation()的唤醒
- 什么是__pdflush()的退出条件呢,(1)若pdflush_list为空就一定不会退出,(2)若线程数少于8就一定
  不会退出,(3)若现在保存在pdflush_list里的时间最长的进程没有超过1s.如何判断这个时间有没有
  超过1s呢?因为pdflush_work在插入的时候是从链表头插进去的,且在pdflush_operation()里是从链
  表头开始删除的,所以可以通过查看最后一个结点的时间来确过保存在pdflush_list里的时间最长的
  进程有没有超过1s.
  #+BEGIN_EXAMPLE
		pdf = list_entry(pdflush_list.prev, struct pdflush_work, list);
		if (jiffies - pdf->when_i_went_to_sleep > 1 * HZ) {
			/* Limit exit rate */
			pdf->when_i_went_to_sleep = jiffies;
			break;					/* exeunt */
		}
  #+END_EXAMPLE
- 所以这个函数大概三个任务(1)执行pdflush_work.fn,(2)判断是否要建一个线程,(3)判断自已是否要
  退出.
** int wakeup_bdflush(long nr_pages)
*** mm/page-writeback.c:
- 这个函数会委托pdflush_operation()来唤醒pdflush线程来执行background_writeout()把脏页写回
  磁盘.
- nr_pages就是要求写回的页数,若是0,那么就把所有的脏页都写回去.如何确定脏页数呢?就是
  page_state的nr_dirty,但还要加上不稳定的页数nr_unstable.那么到底什么是不稳定页呢?不知道是
  不是脏的页?
- 调用这个函数的情况有
  1,sync(),2,grow_buffer(),3,free_more_memory(),4,try_to_free_pages(),5,mempoll_alloc(),5
  修改了页之后发现脏页超过了dirty background threshold阀值.
** static void background_writeout(unsigned long _min_pages)
*** mm/page-writeback.c:
- 这个函数就只是被pdflush_operation()作为参数调用而已.
- writeback_control.bdi:If not NULL, it points to a backing_dev_info structure; in this
  case, only dirty pages belonging to the underlying block device will be flushed.
- writeback_control.sync_mode:是对于锁住的inode来说的,Specifies the synchronization mode:
  WB_SYNC_ALL means that if a locked inode is encountered,it must be waited upon and not
  just skipped over; WB_SYNC_HOLD means that locked inodes are put in a list for later
  consideration; and WB_SYNC_NONE means that locked inodes are simply skipped.
- writeback_control.older_than_this:If not null, it means that inodes younger than the
  specified value should be skipped. 
- writeback_control.nr_to_write:Number of dirty pages yet to be written in this run of
  execution. 
- writeback_control.nonblocking:If this flag is set, the process cannot be blocked.
- 通过对writeback_control的设置,(1)可以看出可以回写所有的块设备,(2)要怱略所有被锁住的
  inode,(3)不管脏多久都要回写,(4)回写时不能被阻塞
- page_state.nr_writeback是什么意思呢?
- 使用死循环回写,只有在脏页没有达到某个值时且已回写足够的页(_min_pages)时或ulk:If less
  than 1,024 pages have been written or if pages have been skipped, probably the request
  queue of the block device is congested: the function puts the current process to sleep
  in a special wait queue for 100 milliseconds or until the queue becomes uncongested.这就
  有问题了,为什么writeback_control.nonblocking设成1了还要进行睡眠呢?为什么体眠之后若发现不
  是因为遇到队列阻塞造成的那么就退出,搞不明白为什么要这样设计.
  #+BEGIN_EXAMPLE
		if (wbc.nr_to_write > 0 || wbc.pages_skipped > 0) {
			/* Wrote less than expected */
			blk_congestion_wait(WRITE, HZ/10);
			if (!wbc.encountered_congestion)
				break;
		}
  #+END_EXAMPLE 
- 每一次调用writeback_inodes()回写页的数量都要求是MAX_WRITEBACK_PAGES (1024),所以
  若_min_pages不是1024的倍数时就有可能回写的页就超过_min_pages.
  #+BEGIN_EXAMPLE
		wbc.nr_to_write = MAX_WRITEBACK_PAGES;
  #+END_EXAMPLE 
- 关于下面的代码为什么要调用blk_congestion_wait()之中才判断wbc.encountered_congestion呢?
  #+BEGIN_EXAMPLE
		if (wbc.nr_to_write > 0 || wbc.pages_skipped > 0) {
			/* Wrote less than expected */
			blk_congestion_wait(WRITE, HZ/10);
			if (!wbc.encountered_congestion)
				break;
		}
  #+END_EXAMPLE 
  这个nr_to_write和pages_skipped之间应该是没有什么关系的,因为如果一个页被skip了,那么不会对
  nr_to_write做加减的.
** void writeback_inodes(struct writeback_control *wbc)
*** mm/page-writeback.c:
- wbc.nr_to_write:The nr_to_write field of this descriptor contains the number of pages to
  be flushed to disk. When the function returns, the same field contains the number of
  pages remaining to be flushed; if everything went smoothly, this field will be set to 0.
- 这个函数是被调用来刷脏页的.
- 这个函数会扫描所有的超级块.
  #+BEGIN_EXAMPLE
	for (; sb != sb_entry(&super_blocks); sb = sb_entry(sb->s_list.prev)) {
  #+END_EXAMPLE 
- s_io:List of inodes waiting to be written to disk
- s_dirty:List of modified inodes
- 在s_io的inode一定在s_dirty吗?应该不一定,因为一个inode虽然是dirty,但是它可能不想被回写,这
  也是为什么需要s_io与s_inode分开来,所以也说明调用这个函数时,就算一个dirty的inode没有放到
  s_dirty,那么它也将被回写.
  #+BEGIN_EXAMPLE
		if (!list_empty(&sb->s_dirty) || !list_empty(&sb->s_io)) {
  #+END_EXAMPLE
- 增加了superblock的使用计数
  #+BEGIN_EXAMPLE
			sb->s_count++;
  #+END_EXAMPLE
  在这里减
  #+BEGIN_EXAMPLE
			if (__put_super_and_need_restart(sb))
  #+END_EXAMPLE 
- 关于这里调用的__put_super_and_need_restart(),在函数里会判断当前扫描的superblock有没有从
  superblock链表里删除
  #+BEGIN_EXAMPLE
	if (list_empty(&sb->s_list)) {
  #+END_EXAMPLE 
  若删除了就返回1,返回1对writeback_inodes()有什么用呢?因为writeback_inodes()是扫描
  superblock链表的,而在解开保护superblock链表锁的过程中会把当前扫描的superblock给从链表里
  删掉
  #+BEGIN_EXAMPLE
			spin_unlock(&sb_lock);
			/*
			 * If we can't get the readlock, there's no sense in
			 * waiting around, most of the time the FS is going to
			 * be unmounted by the time it is released.
			 */
			if (down_read_trylock(&sb->s_umount)) {
				if (sb->s_root) {
					spin_lock(&inode_lock);
					sync_sb_inodes(sb, wbc);
					spin_unlock(&inode_lock);
				}
				up_read(&sb->s_umount);
			}
			spin_lock(&sb_lock);
  #+END_EXAMPLE
  若发现有被删除,那么就使得不能从这个superblock中找到superblock链表的下一个结点,因为它已从
  链表里删除.若还想扫描没有扫描的结点,那么就只能从新开始扫描superblock链表了
  #+BEGIN_EXAMPLE
			if (__put_super_and_need_restart(sb))
				goto restart;
  #+END_EXAMPLE 
- s_umount: Semaphore used for unmounting,为什么是try_lock,而上等待,有注释:
  #+BEGIN_EXAMPLE
			/*
			 * If we can't get the readlock, there's no sense in
			 * waiting around, most of the time the FS is going to
			 * be unmounted by the time it is released.
			 */
  #+END_EXAMPLE 
- 有文件系的根目录才可以回写
  #+BEGIN_EXAMPLE
				if (sb->s_root) {
  #+END_EXAMPLE 
- 调用sync_sb_inodes()才回写,所以这个函数的关键是:(1)判断superblock的s_dirty和s_io,(2)调用
  sync_sb_inodes()(3)s_umount有没有锁,(4)superblock有没有从链表里删除
** static void sync_sb_inodes(struct super_block *sb, struct writeback_control *wbc)
*** fs/fs-writeback.c:
- list_splice_init()这个函数就是把第一个参数所指向的表移到第二个参数所指向的链表里去,但是
  没有把第一参数结点移过去,但是为什么不需要把第一个参数给移过去呢?因为s_dirty这个成员不是
  一个指针,所以不能把这个结点给移过去
  #+BEGIN_EXAMPLE
		list_splice_init(&sb->s_dirty, &sb->s_io);
  #+END_EXAMPLE 
- 发现若是wb_kupdate()调用的这个函数时,只有在s_io为空时才会把s_dirty移到s_io,为什么要这样
  做呢?
  #+BEGIN_EXAMPLE
	if (!wbc->for_kupdate || list_empty(&sb->s_io))
		list_splice_init(&sb->s_dirty, &sb->s_io);
  #+END_EXAMPLE 
- list_move就是把第一参数结点移到第二个参数链表里去.
- bdi_cap_writeback_dirty()函数通过判断BDI_CAP_NO_ACCT_DIRTY这个标志来说明这个inode所对应
  的红黑树里的所有的页是否能回写到disk,但是若不能的话为什么要把它放回s_dirty呢?回为放到
  s_io的inode都是脏的,且BDI_CAP_NO_ACCT_DIRTY可能会在将来的某个时间被清掉.
- http://www.2cto.com/os/201204/126687.html 这个网页有说这个函数.
- backing_dev_info.BDI_write_congested:The write queue is getting full.
- 若调用这个函数的进程不想被阻塞且这个块设备的写队列已经将要满了,那么就设置
  writback_control.encountered_congestion.所以这个标志说明了是遇到了阻塞但是并没有阻塞
  #+BEGIN_EXAMPLE
		if (wbc->nonblocking && bdi_write_congested(bdi)) {
			wbc->encountered_congestion = 1;
			if (sb != blockdev_superblock)
				break;		/* Skip a congested fs */
			list_move(&inode->i_list, &sb->s_dirty);
			continue;		/* Skip a congested blockdev */
		}
  #+END_EXAMPLE
  struct backing_dev_info这个结构体应该是一个块设备一个的,因为从这个结构体的成员描述和状态
  描述来看比较像.
  #+BEGIN_EXAMPLE
enum bdi_state {
	BDI_pdflush,		/* A pdflush thread is working this device */
	BDI_write_congested,	/* The write queue is getting full */
	BDI_read_congested,	/* The read queue is getting full */
	BDI_unused,		/* Available bits start here */
};

typedef int (congested_fn)(void *, int);

struct backing_dev_info {
	unsigned long ra_pages;	/* max readahead in PAGE_CACHE_SIZE units */
	unsigned long state;	/* Always use atomic bitops on this */
	unsigned int capabilities; /* Device capabilities */
	congested_fn *congested_fn; /* Function pointer if device is md/dm */
	void *congested_data;	/* Pointer to aux data for congested func */
	void (*unplug_io_fn)(struct backing_dev_info *, struct page *);
	void *unplug_io_data;
};
  #+END_EXAMPLE 
- 若当前的inode是在调用这个函数之前dirty的,那么就不能刷这个inode链表后面的所有inode,如何确
  定当前后面的所有inode的,这就要保证插入顺序了.
  #+BEGIN_EXAMPLE
		/* Was this inode dirtied after sync_sb_inodes was called? */
		if (time_after(inode->dirtied_when, start))
			break;
  #+END_EXAMPLE 
- 若没有在规定的时间之前dirty的也不能刷inode,
  #+BEGIN_EXAMPLE
		/* Was this inode dirtied too recently? */
		if (wbc->older_than_this && time_after(inode->dirtied_when,
						*wbc->older_than_this))
			break;
  #+END_EXAMPLE 
- 该判断的都判断完了,接下来就是调用__writeback_single_inode()把单个inode刷到disk了.
- 如果是WB_SYNC_HOLD就把它插入到s_dirty里去,以便稍后再刷,但是同时要修改dirtied_when为当前
  时间,这是为什么呢?所以我dirtied_when不一定能知道什么时候dirty的.
- 如果是pdflush线程就要把它bdi.state的BDI_pdflush给清掉,
  #+BEGIN_EXAMPLE
		if (current_is_pdflush())
			writeback_release(bdi);
  #+END_EXAMPLE 
  为什么要这样呢?因为在调用__writeback_single_inode()把这个标志给设置了
  #+BEGIN_EXAMPLE
		if (current_is_pdflush() && !writeback_acquire(bdi))
			break;

		BUG_ON(inode->i_state & I_FREEING);
		__iget(inode);
		pages_skipped = wbc->pages_skipped;
		__writeback_single_inode(inode, wbc);
  #+END_EXAMPLE 
  这说明是在调用__writeback_single_inode()前不久设置的这个标志
- 这段代码什么意思
  #+BEGIN_EXAMPLE
		if (wbc->pages_skipped != pages_skipped) {
			/*
			 * writeback is not making progress due to locked
			 * buffers.  Skip this inode for now.
			 */
			list_move(&inode->i_list, &sb->s_dirty);
		}
  #+END_EXAMPLE
  现在有点明白了,因为pages_skipped的在调用__writeback_single_inode()设置了,
  #+BEGIN_EXAMPLE
		pages_skipped = wbc->pages_skipped;
		__writeback_single_inode(inode, wbc);
		if (wbc->sync_mode == WB_SYNC_HOLD) {
			inode->dirtied_when = jiffies;
			list_move(&inode->i_list, &sb->s_dirty);
		}
  #+END_EXAMPLE 
  所以若在调用__writeback_single_inode()过程中发现有页被skip,那么pages_skipped和
  wbc->page_skipped就会不相等,这说明了这个inode有页被skip,那么就要把这个inode重新放回到
  s_dirty.
- 每写完一个inode就评估一下是不是要调度,在调度的之前已增加了inode的计数器,在调度回来之后才
  会减这个计数器
  #+BEGIN_EXAMPLE
		cond_resched();
		iput(inode);
  #+END_EXAMPLE
- 调用回来之后会马上判断nr_to_write是不是不为正数了,若是就说明没有页要写了,那么就退出,那么
  会是谁去修改这个东西呢?
  #+BEGIN_EXAMPLE
		if (wbc->nr_to_write <= 0)
			break;
  #+END_EXAMPLE
- 有一个不明白的地方,
  #+BEGIN_EXAMPLE
		if (wbc->sync_mode == WB_SYNC_HOLD) {
			inode->dirtied_when = jiffies;
			list_move(&inode->i_list, &sb->s_dirty);
		}
  #+END_EXAMPLE 
  为什么这段代码不用检查这个inode有没有锁被锁呢?否则的话,若WB_SYNC_HOLD只要设置了,那么不是
  所有的inode都会又被放回到s_dirty里去了吗?我觉得执行到这里之前会有一个地方把这个标志给清
  了.
- 所以这个函数的主要过程就是,循环扫一个superblock的inode,对于每一个inode要(1)检查阻塞问题,
  检查时间问题(dirty_when,older_than_this)(3)检查pdflush线程(4)调
  用__writeback_single_inode()(5)检查WB_SYNC_HOLD,(6)检查pages_skipped,(7)调度
** static int __writeback_single_inode(struct inode *inode,struct writeback_control *wbc)
*** fs/fs-writeback.c:
- 这个函数是把单个inode回写的,有两个工作:(1)与I_LOCK相关的,(2)调用__sync_single_inode()回
  写inode.
- 若writeback_control.sync_mode不等于WB_SYNC_ALL,且这个inode被锁了,就说明现在不能刷这个
  inode,要把这个inode放回s_dirty
  #+BEGIN_EXAMPLE
	if ((wbc->sync_mode != WB_SYNC_ALL) && (inode->i_state & I_LOCK)) {
		list_move(&inode->i_list, &inode->i_sb->s_dirty);
		return 0;
	}
  #+END_EXAMPLE 
- 若是一个data-integrity sync,那么就必须等待,设了I_LOCK,就要等待,直到I_LOCK这解锁了,注意这
  个等待是位等待.在等待的过程中这个inode是被解锁的,也就是说在等待的过程中会清掉I_LOCK,所以
  要循环来判断
  #+BEGIN_EXAMPLE
		do {
			__iget(inode);
			spin_unlock(&inode_lock);
			__wait_on_bit(wqh, &wq, inode_wait,
							TASK_UNINTERRUPTIBLE);
			iput(inode);
			spin_lock(&inode_lock);
		} while (inode->i_state & I_LOCK);
  #+END_EXAMPLE 
** static int __sync_single_inode(struct inode *inode, struct writeback_control *wbc)
*** fs/fs-writeback.c:
- 这个函数把单个inode回写disk.这个函数只有__writeback_single_inode()调用
- 在回写之前要把I_LOCK设置,把I_DIRTY清掉,所以一个inode被锁住时有可能是在被回写.
- 为什么要保存I_DIRTY呢?进入这个函数时不能保证I_DIRTY是设置的吗?原来I_DIRTY是
  I_DIRTY_SYNC,I_DIRTY_DATASYNC,I_DIRTY_PAGES的结合.
- http://tracymacding.blog.163.com/blog/static/2128692992013028114014657/ 这文章写得不错.但
  是可能版本不对,没有I_SYNC这个标志,但是和I_LOCK的意思是一样的,可能是改了名字.为了数据完整
  性而进行的回写（wbc->sync_mode == WB_SYNC_ALL）.
- I_DIRTY_SYNC代表inode被弄脏，但这种弄脏并不是由文件数据被修改而导致的，典型的如文件的访问时间被修改，此时文件数据没有被修改；
- I_DIRTY_DATASYNC表示由于文件数据被修改而导致文件inode变脏；
- I_DIRTY_PAGES表示仅仅文件数据被修改，但并未导致文件inode被改动，此时是不需要同步文件元数据的。
- 其中WB_SYNC_ALL表示回写的模式，表示本次回写是文件完整性回写，不同于为了内存紧张时回收页面而进行的回写。
- filemap_fdatawait_range等待在所有上面已经发出的请求的页面上，发请求时给页面加上了
  PG_Writeback，到页面回写完成以后才会清除该标志位，而filemap_fdatawait_range等待在所有文
  件正回写页面的该标志位，直到该标志位被清除该函数才返回，意味着脏页面的回写已经完成，可以
  进行接下来的工作了。 
- do_writepages()最终是通过submit_bh()来写块的,而filemap_fdatawait()只是等待把参数所指定的
  address_space里所有的页都写回disk的时候才返回,而没有调用submit_bh()来提交,只是等待
  PG_Writback被清.所以WB_SYNC_ALL包含有同步所写的数据的意思,只要I_DIRTY_SYNC或
  I_DIRTY_DATASYNC其中一个设置了,就可以调用write_inode把inode的元数据写回disk
  #+BEGIN_EXAMPLE
	int wait = wbc->sync_mode == WB_SYNC_ALL;
  #+END_EXAMPLE
  #+BEGIN_EXAMPLE
	ret = do_writepages(mapping, wbc);

	/* Don't write the inode if only I_DIRTY_PAGES was set */
	if (dirty & (I_DIRTY_SYNC | I_DIRTY_DATASYNC)) {
		int err = write_inode(inode, wait);
		if (ret == 0)
			ret = err;
	}

	if (wait) {
		int err = filemap_fdatawait(mapping);
		if (ret == 0)
			ret = err;
	}
  #+END_EXAMPLE 
- 之前调用的do_writepages()可能没有把所有的脏页都回写到disk,这可以用mapping_tagged()的
  PAGECACHE_TAG_DIRTY来查看.
  #+BEGIN_EXAMPLE
		if (!(inode->i_state & I_DIRTY) &&
		    mapping_tagged(mapping, PAGECACHE_TAG_DIRTY)) {
  #+END_EXAMPLE
  若不是wb_kupdate()通过层层调用调用了这个函数(wbc->for_kupdate),那么除了把i_list放到s_dirty
  和设置I_DIRTY_PAGES之外,还要设置inode->dirtied_when为当前时间,不明白为什么有下面一段代码
  #+BEGIN_EXAMPLE
				/*
				 * Otherwise fully redirty the inode so that
				 * other inodes on this superblock will get some
				 * writeout.  Otherwise heavy writing to one
				 * file would indefinitely suspend writeout of
				 * all the other files.
				 */
				inode->i_state |= I_DIRTY_PAGES;
				inode->dirtied_when = jiffies;
				list_move(&inode->i_list, &sb->s_dirty);
  #+END_EXAMPLE  
- 若这时有发现I_DIRTY设置了，那么就是在正在回写这个inode的时候又有其它东西把这个inode给弄脏
  了.所发直接把这个inode放回s_dirty,如果不为脏,就要看这个inode有没有被用了,inode->i_count,
  有被用调放到inode_in_use里,没有被用就放到inode_unused里
  #+BEGIN_EXAMPLE
		} else if (inode->i_state & I_DIRTY) {
			/*
			 * Someone redirtied the inode while were writing back
			 * the pages.
			 */
			list_move(&inode->i_list, &sb->s_dirty);
		} else if (atomic_read(&inode->i_count)) {
			/*
			 * The inode is clean, inuse
			 */
			list_move(&inode->i_list, &inode_in_use);
		} else {
			/*
			 * The inode is clean, unused
			 */
			list_move(&inode->i_list, &inode_unused);
			inodes_stat.nr_unused++;
		}
  #+END_EXAMPLE
- 因为之前在需要等待的情况下调用了filemap_fdatawait()来等待PG_Writeback清除表示没有正在回
  写,所以再最后可以调用wake_up_inode()来唤醒等待这个inode的进程.
** void wake_up_inode(struct inode *inode)
*** fs/inode.c:
- 调用wake_up_bit().
  #+BEGIN_EXAMPLE
	wake_up_bit(&inode->i_state, __I_LOCK);
  #+END_EXAMPLE
** void __init page_writeback_init(void)
*** mm/page-writeback.c:
- 这个函数由start_kernel()调用,它设定一个定时器,这个定时器是调用wb_timer_fn函数,转而调用
  pdflush_operation(),转而调用wb_kupdate()
- 调用了nr_free_buffer_pages()得到可以被wb_kupdate()回写的页,这些页的特点就是buffer page,
  在内核态的.所以函数的名字也有buffer_pages()字样
- wb_timer_fn函数在调用完pdflush_operation()之后会重新注册wb_timer.定时的时间是下1s
** unsigned int nr_free_buffer_pages(void)
*** include/linux/gfp.h:
- 这个函数调用nr_free_zone_pages(),是以GFP_USER&GFP_ZONEMASK为参数.这个参数是什么意思
  呢?
  #+BEGIN_EXAMPLE
  #define GFP_ZONEMASK	0x03
  #+END_EXAMPLE
  #+BEGIN_EXAMPLE
#define ZONE_DMA		0
#define ZONE_NORMAL		1
#define ZONE_HIGHMEM		2
  #+END_EXAMPLE 
  #+BEGIN_EXAMPLE
#define GFP_USER	(__GFP_WAIT | __GFP_IO | __GFP_FS)
  #+END_EXAMPLE 
  #+BEGIN_EXAMPLE
#define __GFP_WAIT	0x10u	/* Can wait and reschedule? */
#define __GFP_HIGH	0x20u	/* Should access emergency pools? */
#define __GFP_IO	0x40u	/* Can start physical IO? */
#define __GFP_FS	0x80u	/* Can call down to low-level FS? */
  #+END_EXAMPLE 
  所以GFP_USER & GFP_ZONEMASK的结果是0,就是ZONE_DMA?好像还不是这种意思,ulk:If the
  __GFP_DMA flag is set, page frames can be taken only from the ZONE_DMA memory zone. Otherwise, if the __GFP_HIGHMEM flag is not set, page frames can be taken only
  from the ZONE_NORMAL and the ZONE_DMA memory zones, in order of preference. Otherwise
  (the __GFP_HIGHMEM flag is set), page frames can be taken from ZONE_HIGHMEM,
  ZONE_NORMAL, and ZONE_DMA memory zones, in order of preference. 又因为GFP_USER没有设
  置__GFP_HIGHMEM和__GFP_DMA,所以GFP_USER&GFP_ZONEMASK所涉汲的是ZONE_DMA和ZONE_NORMAL这两
  个区,这也符合注释所说的
  #+BEGIN_EXAMPLE
/*
 * Amount of free RAM allocatable within ZONE_DMA and ZONE_NORMAL
 */
  #+END_EXAMPLE 
  同时也说明DMA区也有buffer page
** static unsigned int nr_free_zone_pages(int offset)
*** mm/page_alloc.c:
- 这个函数要找出offset区和它的fallback区(就是分配不了时作为后援的区)的所有页.
- 因为不同的节点可能有相同的区类型,所以要搜所有的结点.
  #+BEGIN_EXAMPLE
	for_each_pgdat(pgdat) {
  #+END_EXAMPLE
- 要定位到所要找的区的zonelist,因为这个区的fallback区就在zonelist里,zonelist只有一个成员,
  它是指针数组(struct zone *为数组的成员),该offset区的所有的fallback区都在这数组里,且要分
  配页时要按照区所有数组的位置,同时第1位元素是offset自已.
  #+BEGIN_EXAMPLE
		struct zonelist *zonelist = pgdat->node_zonelists + offset;
  #+END_EXAMPLE
- zone.present_pages Total size of zone in pages, excluding holes.
- zone.pages_high High watermark for page frame reclaiming; also used by the zone
  allocator as a threshold value.为什么计算时要用present_pages减去pages_high呢?
  #+BEGIN_EXAMPLE
			unsigned long size = zone->present_pages;
			unsigned long high = zone->pages_high;
			if (size > high)
				sum += size - high;
  #+END_EXAMPLE 
** static void wb_kupdate(unsigned long arg)
*** mm/page-writeback.c:
- ulk:checks that no page in the page cache remains dirty for too long
- 所分创建的writeback_control的差别是older_than_this和for_kupdate.
  background_writeout的:
  #+BEGIN_EXAMPLE
	struct writeback_control wbc = {
		.bdi		= NULL,
		.sync_mode	= WB_SYNC_NONE,
		.older_than_this = NULL,
		.nr_to_write	= 0,
		.nonblocking	= 1,
	};
  #+END_EXAMPLE
  wb_kupdate的
  #+BEGIN_EXAMPLE
	struct writeback_control wbc = {
		.bdi		= NULL,
		.sync_mode	= WB_SYNC_NONE,
		.older_than_this = &oldest_jif,
		.nr_to_write	= 0,
		.nonblocking	= 1,
		.for_kupdate	= 1,
	};
  #+END_EXAMPLE
- 从参数看这两个函数的区别,background_writeout()为前者,wb_kupdate()为后者,前者的参数是至少
  要回写的页数,后者的参数是没有作用的.
- 这两个函数都是回写page cache的,不是回收页的.
- 这两个函数都是调用writeback_inodes()来回写页的.
- 后者退出while()的回写循环的条件是:(1)要求回写的页数已写完(2)或在调用writeback_inodes()写
  MAX_WRITEBACK_PAGES个页的时候没有把MAX_WRITEBACK_PAGES个页全都回写且没有遇到阻塞(这种情况
  说明了所有的old data都被回写了),但是没有考虑跳过的页(这点与前者不同).以上其中一个条件成立
  就会退出这个函数
  #+BEGIN_EXAMPLE
	while (nr_to_write > 0) {
		wbc.encountered_congestion = 0;
		wbc.nr_to_write = MAX_WRITEBACK_PAGES;
		writeback_inodes(&wbc);
		if (wbc.nr_to_write > 0) {
			if (wbc.encountered_congestion)
				blk_congestion_wait(WRITE, HZ/10);
			else
				break;	/* All the old data is written */
		}
		nr_to_write -= MAX_WRITEBACK_PAGES - wbc.nr_to_write;
	}
  #+END_EXAMPLE
  所以后者返回时也不一定会把MAX_WRITEBACK_PAGES个页回写,也有可能回写的页大于MAX_WRITEBACK_PAGES.

  前者退出for()的回写循环的条件是:(1)脏页比较少了且已经回写所要求的页数
  #+BEGIN_EXAMPLE
		if (wbs.nr_dirty + wbs.nr_unstable < background_thresh
				&& min_pages <= 0)
			break;
  #+END_EXAMPLE
  (2)或在调用writeback_inodes()写MAX_WRITEBACK_PAGES个页的时候没有把MAX_WRITEBACK_PAGES个
  页全都回写或有页被跳过且没有遇到阻塞(这种情况说明了所有的old data都被回写了).
  #+BEGIN_EXAMPLE
		if (wbc.nr_to_write > 0 || wbc.pages_skipped > 0) {
			/* Wrote less than expected */
			blk_congestion_wait(WRITE, HZ/10);
			if (!wbc.encountered_congestion)
				break;
		}
  #+END_EXAMPLE
  前者和后者所要求回写的页是不一样的,后者是:
  #+BEGIN_EXAMPLE
	nr_to_write = wbs.nr_dirty + wbs.nr_unstable +
			(inodes_stat.nr_inodes - inodes_stat.nr_unused);
  #+END_EXAMPLE
  前者要求回写的页数是参数值.
  注意前者不是只要所要写的页数够了就可能退出,还要求脏页数少于某个值,所以参数值如果小了也没
  有多大的用处,要大于这个脏页数阀值才会有效,不然就是多余的.
  而后者所要求的页数更猛,好像不允许有留下脏页似的,把脏的page buffer,不稳定的page buffer,在
  内存里的有在使用的inode元数的inode数都加起来,没有什么脏页数阀值且把inode的元数据也算在里
  面了.
  #+BEGIN_EXAMPLE
	nr_to_write = wbs.nr_dirty + wbs.nr_unstable +
			(inodes_stat.nr_inodes - inodes_stat.nr_unused);
  #+END_EXAMPLE
  关于调用时间的问题,前者是在被其它进程以调用background_writeout()的方式来唤醒pdflush线程
  时才会调用,而后者是用定时器来使自已重复执行的.前者每执行一次都要被通调用一次
  pdflush_operation(),后者的执行一开始也要通过pdflush_operation()但是之后的重复调用就不用
  了.而且后者是没有struct pdflush_work的概念的,后者的执行是一个定时器,所以它不会像前者那样
  要以内核线程作为执行它的一个特定的进程.因为定时器是以一个中断来执行的.
** asmlinkage long sys_sync(void)
*** fs/buffer.c:
- 这个函数调用do_sync(),这个函数是作为sync()的系统调用函数.
- sync( ) Allows a process to flush all dirty buffers to disk.

  fsync( )Allows a process to flush all blocks that belong to a specific open file to
  disk.

  fdatasync( ) Very similar to fsync( ) , but doesn't flush the inode block of the file.
- The fdatasync( ) system call is very similar to fsync( ) , but writes to disk only the
  buffers that contain the file's data, not those that contain inode information. Because
  Linux 2.6 does not have a specific file method for fdatasync( ), this system call uses
  the fsync method and is thus identical to fsync( ) .
** ssize_t generic_file_read(struct file *filp, char __user *buf, size_t count, loff_t *ppos)
*** mm/filemap.c:
- 从文件读取数据.
- 这个函数是同步操作的,调用init_sync_kiocb()在kiocb里指定了(KIOCB_SYNC_KEY)
- 主要的工作在__generic_file_aio_read()完成
** ssize_t __generic_file_aio_read(struct kiocb *iocb, const struct iovec *iov,unsigned long nr_segs, loff_t *ppos)
*** mm/filemap.c:
- 这个函数一开始就是用access_ok()来验证参数iov里所有的段的写的可访问性.
  #+BEGIN_EXAMPLE
	for (seg = 0; seg < nr_segs; seg++) {
		const struct iovec *iv = &iov[seg];

		/*
		 * If any segment has a negative length, or the cumulative
		 * length ever wraps negative then return -EINVAL.
		 */
		count += iv->iov_len;
		if (unlikely((ssize_t)(count|iv->iov_len) < 0))
			return -EINVAL;
		if (access_ok(VERIFY_WRITE, iv->iov_base, iv->iov_len))
			continue;
		if (seg == 0)
			return -EFAULT;
		nr_segs = seg;
		count -= iv->iov_len;	/* This segment is no good */
		break;
	}
  #+END_EXAMPLE
  要注意如果在iov这个数组里的第n个元素的可访问性出现了问题,那么后面元素都被认为是无效的.
- 如果是设置了O_DIRECT那么就执行丁面那一段
  #+BEGIN_EXAMPLE
	if (filp->f_flags & O_DIRECT) {
		loff_t pos = *ppos, size;
		struct address_space *mapping;
		struct inode *inode;

		mapping = filp->f_mapping;
		inode = mapping->host;
		retval = 0;
		if (!count)
			goto out; /* skip atime */
		size = i_size_read(inode);
		if (pos < size) {
			retval = generic_file_direct_IO(READ, iocb,
						iov, pos, nr_segs);
			if (retval > 0 && !is_sync_kiocb(iocb))
				retval = -EIOCBQUEUED;
			if (retval > 0)
				*ppos = pos + retval;
		}
		file_accessed(filp);
		goto out;
	}
  #+END_EXAMPLE
- 否则就是下面那一段
  #+BEGIN_EXAMPLE
	if (count) {
		for (seg = 0; seg < nr_segs; seg++) {
			read_descriptor_t desc;

			desc.written = 0;
			desc.arg.buf = iov[seg].iov_base;
			desc.count = iov[seg].iov_len;
			if (desc.count == 0)
				continue;
			desc.error = 0;
			do_generic_file_read(filp,ppos,&desc,file_read_actor);
			retval += desc.written;
			if (!retval) {
				retval = desc.error;
				break;
			}
		}
	}
  #+END_EXAMPLE
- 关于read_descriptor_t这个类型的四个成员中两个是输入(arg.buf,count)与iov相关,另外两个是输
  出,做为do_generic_file_read()的返回值.会对每个iov里的每一个元素段调用
  do_generic_file_read().
- 所以这个函数就是2件事(1)access_ok()一下用户的空间,(2)直接IO就调用
  generic_file_direct_IO()否则就是do_generic_file_read()
** static inline void do_generic_file_read(struct file * filp, loff_t *ppos, read_descriptor_t * desc, read_actor_t actor)
*** include/linux/fs.h:
- 这个函数就是直接调用do_generic_mapping_read()
** void do_generic_mapping_read(struct address_space *mapping, struct file_ra_state *_ra,struct file *filp, loff_t *ppos, read_descriptor_t *desc, read_actor_t actor)
*** mm/filemap.c:
- index是ppos以页为单位在文件里的索引,offset是ppos以页为单位在页单位里的偏移量,last_index
  是要被读的最后一个字节所在的以页为单位的索引.
- nr是可以从当前的index文件页里读多少个字节
  #+BEGIN_EXAMPLE
		/* nr is the maximum number of bytes to copy from this page */
		nr = PAGE_CACHE_SIZE;
		if (index >= end_index) {
			if (index > end_index)
				goto out;
			nr = ((isize - 1) & ~PAGE_CACHE_MASK) + 1;
			if (nr <= offset) {
				goto out;
			}
		}
		nr = nr - offset;
  #+END_EXAMPLE
- 每读一次之前都会调用cond_resched()调度一下.
  #+BEGIN_EXAMPLE
		cond_resched();
  #+END_EXAMPLE 
- 如果当前将要读的页是之前预读的页,那么就先预读,而不是先读所要读的页
  #+BEGIN_EXAMPLE
		if (index == next_index)
			next_index = page_cache_readahead(mapping, &ra, filp,
					index, last_index - index);
  #+END_EXAMPLE 
- 如果在address_space里找不到想要的页时说明预读是失败的
  #+BEGIN_EXAMPLE
		page = find_get_page(mapping, index);
		if (unlikely(page == NULL)) {
			handle_ra_miss(mapping, &ra, index);
			goto no_cached_page;
		}
  #+END_EXAMPLE 
- 若发现页不是最新的就需要从磁盘里读,现在有一个疑问了,在什么情况下会清掉PG_Uptodate的呢?
  #+BEGIN_EXAMPLE
readpage:
		/* Start the actual read. The read will unlock the page. */
		error = mapping->a_ops->readpage(filp, page);

  #+END_EXAMPLE 
- 若发现没有在address_space里找到想要的页就要分配一个并把加到radix树里,
  #+BEGIN_EXAMPLE
no_cached_page:
		/*
		 * Ok, it wasn't cached, so we need to create a new
		 * page..
		 */
		if (!cached_page) {
			cached_page = page_cache_alloc_cold(mapping);
			if (!cached_page) {
				desc->error = -ENOMEM;
				goto out;
			}
		}
		error = add_to_page_cache_lru(cached_page, mapping,
						index, GFP_KERNEL);
		if (error) {
			if (error == -EEXIST)
				goto find_page;
			desc->error = error;
			goto out;
		}
		page = cached_page;
		cached_page = NULL;
		goto readpage;
	}
  #+END_EXAMPLE
  添加到radix树的时候可能会发现这个页被其它的进程给加了,那么就回到find_page那个标签去,在这
  种情况下先不用把原来分配好的页给释放掉,到了out标签那里才释放掉,为什么要这样做呢?为了效率?因
  为在这种情况下还是找不到的可能性比较大?

- 这段代码什么意思呢
  #+BEGIN_EXAMPLE

		/* If users can be writing to this page using arbitrary
		 * virtual addresses, take care about potential aliasing
		 * before reading the page on the kernel side.
		 */
		if (mapping_writably_mapped(mapping))
			flush_dcache_page(page);
  #+END_EXAMPLE
  i_mmap_writable是在__vma_link_file()里增加的,表示VM_SHARED映射计数器的意思.
- 不明白下面的代码什么意思
  #+BEGIN_EXAMPLE

		/*
		 * When (part of) the same page is read multiple times
		 * in succession, only mark it as accessed the first time.
		 */
		if (prev_index != index)
			mark_page_accessed(page);
		prev_index = index;
  #+END_EXAMPLE 
- 最后调用参数函数actor()来把页的内容复制到用户地址空间.
  #+BEGIN_EXAMPLE
		/*
		 * Ok, we have the page, and it's up-to-date, so
		 * now we can copy it to user space...
		 *
		 * The actor routine returns how many bytes were actually used..
		 * NOTE! This may not be the same as how much of a user buffer
		 * we filled up (we may be padding etc), so we can only update
		 * "pos" here (the actor routine has to update the user buffer
		 * pointers and the remaining count).
		 */
		ret = actor(desc, page, offset, nr);
  #+END_EXAMPLE
  上面的注释说了,在这个actor()函数里要维护文件pos,在这里actor()是file_read_actor()函数
- 最后调用file_accessed()修改文件的atime访问时间.
** int file_read_actor(read_descriptor_t *desc, struct page *page, unsigned long offset, unsigned long size)
*** mm/filemap.c:
- 这个函数就是把内核里的page页从offset开始把size个字节拷贝到desc.arg.buf里去.
- 先用__copy_to_user_inatomic()进行快速的拷贝,若没有完全拷贝成功就
  用__copy_to_user(),__copy_to_user_inatomic()的返回值就是没有成功拷贝的字节数,若有一个没
  有拷都要用__copy_to_user().
- 判断是不是可以快速拷贝的方法很简单,就是调用
  fault_in_pages_writeable(),fault_in_pages_writeable()这个方法只是做简单的判断,下面有介绍.
- 为什么快速拷贝用kmap_atomic()作映射呢?而慢速用kmap()?为什么调用kmap_atomic()时不是把
  desc.arg.buf所在的页作为参数而是把参数page这个属于内核的页作为kmap_atomic()的参数?慢速拷
  贝时调用kmap()时也是这样.其实这个映射是因为__copy_to_user_inatomic()和__copy_to_user()对
  参数的要求就是这样的,第一个参数是用户态空间的地址,而第二个参数是内核态的地址空间,因为传
  入的page不知是否是高端页,所以要作kmap_atomic()映射,kmap_atomic()所关心的是页是不是高端的
  页,是高端的页就映射,不是就直接与PAGE_OFFSET作处理,而不关心这个页是内核态的还是用户态的,
  页的位置其实没有用户态和内核态之分,因为一个页可以被多个地址映射.所以就算参数page是一个低
  端的页调用一下kmap_atomic()还是有用的.
** static inline int fault_in_pages_writeable(char __user *uaddr, int size)
*** include/linux/pagemap.h:
- 这个函数只是把uaddr开始的size长度的地址空间的第一个元素和最后一个元素用__put_user()赋值,
  若都没有返回错就认为这段空间是可以写的了.
** static inline loff_t i_size_read(struct inode *inode)
*** include/linux/fs.h:
- 这个函数根据是否是64位的架构和是否是多核来确定怎么读inode.i_size(以字节为单位的文件的大
  小),因为inode.i_size是loff_t,所以是64位的类型,所以32位的架构读的时候不是原子的.
- 32位架构和多核时就用顺序锁读,32位架构和单核时就要禁止抢占.
** int mpage_readpage(struct page *page, get_block_t get_block)
*** fs/mpage.c:
** static struct bio *do_mpage_readpage(struct bio *bio, struct page *page, unsigned nr_pages, sector_t *last_block_in_bio, get_block_t get_block)
*** fs/mpage.c:
- address_space的readpage方法的作用是:The readpage method of the address_space object
  stores the address of the function that effectively activates the I/O data transfer from
  the physical disk to the page cache. 所以readpage有在do_generic_mapping_read()里调用.对
  于普通文件,readpage指向一个封装了mpage_readpage()的函数,注意是普通文件,如ext3的readpage
  #+BEGIN_EXAMPLE
  int ext3_readpage(struct file *file, struct page *page)
  {
     return mpage_readpage(page, ext3_get_block);
  }
  #+END_EXAMPLE 
- 这个函数有两种策略来处理所读的块是不是连续的情况
- http://blog.csdn.net/guogaofeng1219/article/details/5445772 如果page中的块在磁盘上连续，
  那么page的PG_private不会被置位，private字段也不会指向buffer_head的链表.但是page还是得用
  到buffer_head结构，因为它需要通过get_block()函数来获得磁盘上的逻辑块号。 
- 在一开始会判断页有没有设置PG_Private,设置了就说明这个页上的块在磁盘了不是连续的,但是在文
  件里页里的内容肯定是连续的,一个文件的所有块很多时候在磁盘上不是连续的.现在才知道
  PG_Private有这个作用的.这时会调用block_read_full_page()来一块一块地读.
  #+BEGIN_EXAMPLE
	if (page_has_buffers(page))
		goto confused;
  #+END_EXAMPLE 
- buffer_head.b_blocknr:Block number relative to the block device (logical block number)
- 原来里面调用的get_block()函数是一个参数传进来的,搞得我找了半天.
- http://blog.csdn.net/opencpu/article/details/6692556

  1.     硬件上的 block size, 应该是”sector size”，linux的扇区大小是512byte
  2.       有文件系统的分区的block size, 是”block size”，大小不一，能用工具查看
  3.       没有文件系统的分区的block size，也叫“block size”，大小指的是1024 byte
  4.       Kernel buffer cache 的block size, 就是”block size”，大部分PC是1024
  5.       磁盘分区的”cylinder size”，用fdisk 能查看。
- 执行完第一个循环的结果表,设页可以放3页块的数据(blocks_per_page)
| 0表示该buffer_head为没有被映射(BH_Mapped) | full_mapped值 | first_hole | blocks数组(0表示没有被设,1表示在循环里有设置过) |              |
|-------------------------------------------+---------------+------------+-------------------------------------------------+--------------|
|                                       000 |             0 |          0 |                                             000 |              |
|                                       111 |             1 |          3 |                                             111 |              |
|                                       100 |             0 |          1 |                                             100 | 跳到confused |
|                                       001 |             0 |          0 |                                             000 | 跳到confused |
|                                       110 |             0 |          2 |                                             110 |              |
|                                       011 |             0 |          0 |                                             000 | 跳到confused |
|                                       101 |             0 |          1 |                                             100 | 跳到confused |
从这个结果表可以看出,

(1)full_mapped表示这个页里的所有的块是不是都已经有数据了(BH_Mapped),

(2)first_hole表示页内第一个还没有被映射的buffer_head的在页内内的第first_hole个
buffer_head(就是表格第一列的第一个为0的下标),

(3)blocks数组的填充有一个规律就是在遇到第一个(第一列从左到右)为0的时候就不填充了,所以
first_hole就是指blocks数组被填充的元素个数.

(4)什么情况下会跳到confused呢?出现一个没有被映射的buffer_head时紧接着出现一个被映射的
buffer_head.

在这个for循环里只有设置过了blocks数组才会设置bdev变量.

- 从以下的代码可以看出,(1)会把从页内第一个没有被映射的buffer_head开始(包括该buffer_head)到
  该页的最后范围内的空间都清零,就算这个区间有buffer是被映射的了,如上一个表的最后一种情况时
  会把第三个buffer_head也给清掉.(2)若第一个buffer_head就没有被映射,那么就会设置
  PG_Uptodate,为什么要这样做呢?明明是清了数据的.(3)若页里的空间都映射了块,那么就设置PG_Mappedtodisk,
  #+BEGIN_EXAMPLE
	if (first_hole != blocks_per_page) {
		char *kaddr = kmap_atomic(page, KM_USER0);
		memset(kaddr + (first_hole << blkbits), 0,
				PAGE_CACHE_SIZE - (first_hole << blkbits));
		flush_dcache_page(page);
		kunmap_atomic(kaddr, KM_USER0);
		if (first_hole == 0) {
			SetPageUptodate(page);
			unlock_page(page);
			goto out;
		}
	} else if (fully_mapped) {
		SetPageMappedToDisk(page);
	}
  #+END_EXAMPLE 
  注意执行到这里为止,页的PG_Private都是没有设置的,都是没在buffer_head结构体链表与这个页关
  联的.

- 若是通过mpage_readpages()来调用这个函数的时候,传入的bio是NULL的,所以在这个函数里个分配一
  个新的bio
- min_t(type,x,y)这个个宏好像实现了泛型的比较似的.
- 到alloc_new标签这里已经说明blocks数组至少第一个元素是没有空的,有可能不止第一个元素为空,
  但是第n个元素为空那么第n-1个元素就不为空(从个一个表可以看出),若不只一个元素为空,那么它们
  就是在disk上是连续的.
- 为什么在分配bio的时候指定的块号不是直接使用blocks数组的值呢?
  #+BEGIN_EXAMPLE
		bio = mpage_alloc(bdev, blocks[0] << (blkbits - 9),
			  	min_t(int, nr_pages, bio_get_nr_vecs(bdev)),
				GFP_KERNEL);
  #+END_EXAMPLE
  可能与逻辑块号和物理块号和扇区之间有关系,逻辑块号是对于VFS来说的,物理块号是对于具体文件
  系统来说的(Ext2),扇区是对于磁盘来说的.i_blkbits应该是对于逻辑块号来说的,因为写时是一个扇
  区写的,扇区的大小是512byte(2的9次方),所以要用i_blkbits减去9再给逻辑块号移位.
- http://hi.baidu.com/zhangliulin/item/eabc7738edcf2ac11a969640
  逻辑块作为一个抽象的概念，它必然要映射到具体的物理块上去，因此，逻辑块的大小必须是物理块
  大小的整数倍，一般说来，两者是一样大的。

  通常，一个文件占用的多个物理块在磁盘上是不连续存储的，因为如果连续存储，则经过频繁的删除、
  建立、移动文件等操作，最后磁盘上将形成大量的空洞，很快磁盘上将无空间可供使用。

  如果物理块定的比较大，比如一个柱面大小，这时，即使是1个字节的文件都要占用整个一个柱面，
  假设Linux环境下文件的平均大小为1K，那么分配32K的柱面将浪费97%的磁盘空间，也就是说，大的
  存取单位将带来严重的磁盘空间浪费。另一方面，如果物理块过小，则意味着对一个文件的操作将进
  行更多次的寻道延迟和旋转延迟，因而读取由小的物理块组成的文件将非常缓慢！

  具体文件系统管理的是一个逻辑空间，这个逻辑空间就象一个大的数组，数组的每个元素就是文件系
  统操作的基本单位——逻辑块，逻辑块是从0开始编号的，而且，逻辑块是连续的。与逻辑块相对的是
  物理块，物理块是数据在磁盘上的存取单位，也就是每进行一次I/O操作，最小传输的数据大小。

  在Ext2中，还有一个重要的概念：片（fragment）.
- mpage_readpages()调用的do_mpage_readpage()的nr_pages不一定是1,而这个函数mpage_readpage()
  调用do_mpage_readpage()的nr_pages是1.
- 到底要读磁盘的那一块数据,最原始的根据是page.index.
- 如果页内的块不是连续的,且不是最新的,那么就调用block_read_full_page()

- 这个函数结合mpage_readpages()讲解可以看
  http://blog.csdn.net/dog250/article/details/5303564
- 原来块寻址也有直接和间接寻址的
- 对于bio,在mpage_readpages()里会用循环来处理参数传进来的所有页,有一个原则,就是尽可能地用
  最少的bio来转输数据,要想用最少的bio,那么就要在bio里尽最大可能地放入连续的块,若传入的
  nr_pages个页的块都是连续的,那么就可以用一个bio就可以了
- 在mpage_readpages()这个函数里调用do_mpage_readpage()返回的bio若是NULL,那么说明在
  do_mpage_readpage()函数里因为调用了mpage_bio_submit()来传输.mpage_bio_submit()这个函数是
  一定返回NULL的.
- 在什么情况下会调用mpage_bio_submit()呢?这个情况还不好判断

  (1)发现在bio里的最后一个块不与blocks数组里的第一个块是连续的,那么就要先调用
  mpage_bio_submit()来把bio提交了,因为不连续,所以不能把blocks数组里的块放到,要先提交.
  #+BEGIN_EXAMPLE
	if (bio && (*last_block_in_bio != blocks[0] - 1))
		bio = mpage_bio_submit(READ, bio);
  #+END_EXAMPLE 

  (2)如果bio_add_page()没有成功把页添加bio里去,bio_add_page()返回的不是0就是length
  #+BEGIN_EXAMPLE
	length = first_hole << blkbits;
	if (bio_add_page(bio, page, length, 0) < length) {
		bio = mpage_bio_submit(READ, bio);
		goto alloc_new;
	}
  #+END_EXAMPLE 

  http://blog.chinaunix.net/uid-24774106-id-3266816.html 这个文章讲了ext2文件系统在磁盘上
  的结构.讲了整个ext2文件系统的结构图,超级块的分布,超级块里的数据和超级块结构体的对应,超级
  块的备份,块组描述符的分布,块组描述符的备份,块组里块的个数的原因.

  Inode中有个长度为15的数组指向文件的数据存储的块。前12个为直接指针，指向了数据所在的block
  块。第13 个数组元素是1级间接指针，第14个数组元素为二级间接指针，第15个元素为三级间接指针，
  通过间接指针，ext2支持的文件最大长度获得了极大的扩展。
  http://blog.chinaunix.net/uid-24774106-id-3267190.html

  其实关于ext2在ulk的第18章里有介绍,可以看看ext2_block_to_path()是如何得到间接寻址的深度并
  且填充offsets索引数组的,在ext2_block_to_path()里设置的boundary就会影响BH_Boundary的设置.关
  于boundary:比如当前的i_block为11，那么此时的boundary就会被设置为1，表示下面就要间接寻址
  了，正如它的名字所谓，final是一个边界，如果i_block到达了一个边界，那么就将这个值设置
  为1.

  现在再看下面这两段话就明白为什么要设置BH_Boundary了(先要明白一点就是若一个文件块是要通过
  间接寻址才可以读取的话,那么它先要获取间接指针块,所谓的间接指针块也是存储在磁盘上的一个
  块,所以当使用get_block()这个函数来获取间接块的块号时,就要先获取存储这个间接块号的存储间接指
  针的块,所以get_block()函数会作一次磁盘的io操作):如果bh的boundary被设置了，那么说明马
  上就要进行额外的io了，这个额外的io就是 间接寻址，这个间接寻址要在下一次进入
  do_mpage_readpage函数以后的第一次get_block回调函数里面进行，其实就是读一个 block而已。

  设置boundary是因为下一次get_block就要间接寻址了，因此下一次 get_block的时候就要进行一次额
  外的io了，这次额外的io将会使磁盘莫名其妙的移动磁头到这个间接寻址块并且开始读取数据，我们
  知道mpage 利用bio的聚集方式操作磁盘很有效，因为它可以最大限度减少磁头的移动，因为操作的块
  连续嘛，但是具体的操作是bio被提交以后才进行的，我们假设读取 一个大文件的前16块，并且为了
  方便讨论特定文件的块号页面内连续，这样只用一个bio就可以了，如果在提交所有的page的bio之前
  的 get_block中有io的话，那么磁头会移动到间接寻址块开始读取数据，然后等块号读回来以后将页
  面加入bio，最终提交bio的时候磁头会拐回来到 第一个直接寻址块，mpage的优势将部分失效，正如
  注释中说，io的顺序将是：12 0 1 2 3 4 5 6 7 8 9 10 11 13 14 15 16，这样会很不好，如果可以
  做到连续操作磁盘就好了，虽然直接寻址指针11指向的块和间接寻址指针指向的块不一定连续，但是
  很多时候都是连续的，就算不 连续也不会离很远，于是就有了boundary一说，也就是说当映射完第
  11块以后，发现下一块就要间接寻址了，那么就设置该bh的boundary标 志，这样返回的时候，一旦发
  现设定了boundary标志就知道该间接寻址，对于我们的例子在下一次的get_block的时候就要操作第
  12块间接指针 了，为了使得磁头的移动最小化，那么首先将积累的bio提交，这样磁头在移动到12块
  之前就完成了以前的从0到11块的io操作，磁头再也不会是莫名其妙 的移动到12块了，而是很有意义
  的移动到了12块。
  
  
  
  (3)


** static int blkdev_readpage(struct file * file, struct page * page)
*** fs/block_dev.c:
- 这个函数就是块设备文件的address_space_operations.readpage方法.
- 这个函数直接调用block_read_full_page()而普通文件的readpage方法是
  ext3_readpage(),ext3_readpage()调用mpage_readpage(),调用mpage_readpage()时传的
  get_block()是ext3_get_block(),而blkdev_readpage()调用的block_read_full_page（）时传的
  get_block()是blkdev_get_block()
** int block_read_full_page(struct page *page, get_block_t *get_block)
*** fs/buffer.c:
- 这个函数是把page里的buffer head里所指定的块给读到这个page里去,所以这个函数在该页的块在
  disk里不是连续的时候也会调用,读disk文件时也会调用.
- 首先会看这个pge里有没有buffer_head链表,若没有就会创建
  #+BEGIN_EXAMPLE
	blocksize = 1 << inode->i_blkbits;
	if (!page_has_buffers(page))
		create_empty_buffers(page, blocksize, 0);
  #+END_EXAMPLE 
- 这样就算出了page里的第一个逻辑块的块号
  #+BEGIN_EXAMPLE
	iblock = (sector_t)page->index << (PAGE_CACHE_SHIFT - inode->i_blkbits);
  #+END_EXAMPLE
  这样就算出了该文件的最大逻辑块号
  #+BEGIN_EXAMPLE
	lblock = (i_size_read(inode)+blocksize-1) >> inode->i_blkbits;
  #+END_EXAMPLE
- 注意传给这个函数的get_block()方法不仅仅是blkdev_get_block()
- 第一个dowhile循环的执行和get_block()有很大的关系,总之执行完之后是对BH_Mapped和
  BH_Uptodate有影响,修改了buffer_head.b_blocknr.有一些文件系统指定的get_block()函数是会进
  行I/O操作的,所以要第二次判断BH_Uptodate.
- 还有一种情况比较有意思,就是出现BH_Mapped没设置,BH_Uptodate设置了,且页的内容被清零,
  #+BEGIN_EXAMPLE
			if (!buffer_mapped(bh)) {
				void *kaddr = kmap_atomic(page, KM_USER0);
				memset(kaddr + i * blocksize, 0, blocksize);
				flush_dcache_page(page);
				kunmap_atomic(kaddr, KM_USER0);
				if (!err)
					set_buffer_uptodate(bh);
				continue;
			}
  #+END_EXAMPLE 
- dowhile循环对下的代码的执行的影响就是fully_mapped变量,和arr这个buffer_head数组,这个数组
  包含了要作I/O传输的buffer_head,所以在
- 在do_mpage_readpage()里若块是连续的,那么最终会调用submit_bio()进行I/O传输,若块不是连续的,那
  么会调用block_read_full_page()转而调用submit_bh()来对页里的没有设置BH_Uptodate的
  buffer_head进行I/O传输.
- 若页里的所有的buffer_head都是设了BH_Uptodate的,那么就设置PG_Uptodate
  #+BEGIN_EXAMPLE
	if (!nr) {
		/*
		 * All buffers are uptodate - we can set the page uptodate
		 * as well. But not if get_block() returned an error.
		 */
		if (!PageError(page))
			SetPageUptodate(page);
		unlock_page(page);
		return 0;
	}
  #+END_EXAMPLE
- 这个函数的所要提交的buffer_head都是要异步的传输的
  #+BEGIN_EXAMPLE

	/* Stage two: lock the buffers */
	for (i = 0; i < nr; i++) {
		bh = arr[i];
		lock_buffer(bh);
		mark_buffer_async_read(bh);
	}
  #+END_EXAMPLE 
- 接下来就是一个buffer_head一个buffer_head地来提交了,但是有可能在锁住buffer_head之后把
  BH_Uptodate给设置了,在此之前一直没有把buffer_head给锁住过,包括在dowhile循环里,这时直接调
  用end_buffer_async_read(),为什么还要调用end_buffer_async_read()?因为之前调用了
  mark_buffer_async_read()时设置了BH_Async_read,在end_buffer_async_read()会清掉.
  #+BEGIN_EXAMPLE
	for (i = 0; i < nr; i++) {
		bh = arr[i];
		if (buffer_uptodate(bh))
			end_buffer_async_read(bh, 1);
		else
			submit_bh(READ, bh);
	}
  #+END_EXAMPLE 
** static void end_buffer_async_read(struct buffer_head *bh, int uptodate)
*** fs/buffer.c:
- 这个函数有两个大的任务,(1)和传入的buffer_head有关(2)和buffer_head所在的页有关
- 与buffer_head相关的就是(1)根据参数uptodate来设置BH_Uptodate(2)清BH_Async_read
- 与page相关的就是(1)根据参数uptodate来设置PG_error(2)根据页内的所有的buffer_head的
  BH_Uptodate来设置PG_updodate
** void create_empty_buffers(struct page *page, unsigned long blocksize, unsigned long b_state)
*** fs/buffer.c:
- 这个函数会给没有buffer_head链表的page创建buffer_head链表,还要初始化这个buffer_head链表里
  的buffer_head,要根据PG_dirty,PG_uptodate来设置所有buffer_head相应的BH_Dirty和
  BH_Uptodate,就那么简单.
** static int blkdev_get_block(struct inode *inode, sector_t iblock, struct buffer_head *bh, int create)
*** fs/ext3/inode.c:
- 这个函数是blkdev_readpage()调用block_read_full_page()传的get_block()函数用来获取逻辑块号的.
- 很简单,就下面两句重点,和ext3_get_block()不同
  #+BEGIN_EXAMPLE
	bh->b_bdev = I_BDEV(inode);
	bh->b_blocknr = iblock;
	set_buffer_mapped(bh);
  #+END_EXAMPLE
  为什么要设置BH_Mapped呢?
- 这个函数是把文件块号直接当作是设备的逻辑块号,与普通文件的get_block()不一样,不用把文件块
  号转为设备的块号
- 关于这个函数的create参数的作用,有这样的注释
  #+BEGIN_EXAMPLE
		/*
		 * for reads, we're just trying to fill a partial page.
		 * return a hole, they will have to call get_block again
		 * before they can fill it, and they will get -EIO at that
		 * time
		 */
  #+END_EXAMPLE
  还知道在block_read_full_page()调用get_block()(就是blkdev_get_block())时create参数是0,而
  在block_write_full_page()调用get_block()时create参数是1,就是在读块的时候create是0写的时
  候是1.通过注释也可以看出,读时若块号超过最大的块号,那么就认为这是一个文件洞,返回0表示,写
  时若块号超过最大的块号就会返回错.
** ssize_t generic_file_write(struct file *file, const char __user *buf,size_t count, loff_t *ppos)
*** mm/filemap.c:
- ULK:Many filesystems (including Ext2 or JFS ) implement the write method of the file
  object by means of the generic_file_write( ) function
- 该函数先调用__generic_file_write_nolock()再做一些同步的工作.
** ssize_t __generic_file_write_nolock(struct file *file, const struct iovec *iov,unsigned long nr_segs, loff_t *ppos)
*** mm/filemap.c:
- 这个函数就是调用init_sync_kiocb()初始化kiocb,再调用__generic_file_aio_write_nolock(),再
  调用wait_on_sync_kiocb()
** ssize_t __generic_file_aio_write_nolock(struct kiocb *iocb, const struct iovec *iov, unsigned long nr_segs, loff_t *ppos)
*** mm/filemap.c:
- 这个函数开始是检查iov里的用户态空间有没有不可访问的(access_ok),若有那么就标明有问题的段
  在哪里,下面在操作时就从所标明的那个地方开始都不要理了.
- 为什么要设置current.backing_dev_info呢?ulk:this setting allows the current process to
  write back the dirty pages owned by file->f_mapping even if the corresponding request
  queue is congested;
- 接着调用__generic_write_checks()作一些检查,下面有介绍,inode.i_mode:file type and access
  rights
- 接下来就是调用remove_suid()清掉原来设置的S_ISUID和S_ISGID,为什么呢?是不是因为现在是写一
  个可执行程序,就是说这个可执行程序被改了,那么它原来它的SUID和SGID就无效了,只有在没有
  CAP_FSETID权限时才会还有效.
- 接下来是调用inode_update_time来修改文件的ctime和mtime,但是没有修改atime,难道修改文件不会
  修改atime.在之前有一个关于I_DIRTY_SYNC的解释:I_DIRTY_SYNC代表inode被弄脏，但这种弄脏并不
  是由文件数据被修改而导致的.inode_update_time()在这里被调用就是因为修改了文件的内容时调用
  的,而且inode_update_time()转而调用mark_inode_dirty_sync()转而以I_DIRTY_SYNC调
  用__mark_inode_dirty(),那么那个解释就不对了.
- 文件的 Access time，atime 是在读取文件或者执行文件时更改的。

  文件的 Modified time，mtime 是在写入文件时随文件内容的更改而更改的。

  文件的 Create time，ctime 是在写入文件、更改所有者、权限或链接设置时随 Inode 的内容更改而更改的。
- 若是直接I/O操作,那么就调用generic_file_direct_write(),如果generic_file_direct_write()没
  有把所有的数据写回disk,那么是因为
  #+BEGIN_EXAMPLE
		/*
		 * direct-io write to a hole: fall through to buffered I/O
		 * for completing the rest of the request.
		 */
  #+END_EXAMPLE
  这时就要调用generic_file_buffered_write()用普通的方式来写.如果不是直接I/O,也要用
  generic_file_buffered_write()写.
- 所以整个函数的任务就是:(1)验证用户空间的有效性,(2)检查写文件的大小限制(3)删除
  suid/sgid(4)更新inode的mtime/ctime(5)调用generic_file_direct_write()直接I/O(6)调用
  generic_file_buffered_write()
** ssize_t generic_file_buffered_write(struct kiocb *iocb, const struct iovec *iov, unsigned long nr_segs, loff_t pos, loff_t *ppos, size_t count, ssize_t written)
*** mm/filemap.c:
- 函数一开始是计算应该从用户空间的哪个位置开始把数据写入磁盘,根据的参数是iov(存放用户空间
  起始地址和长度的地方),nr_segs(有多少个iov)和written(已写了多少个字节),传入iov时,iov应该
  是一个数组的首地址,而不是首地址的某个偏移的iov,所以written就应该是从传入的iov的第1个元素
  开始算起已写了多少个字节.
  #+BEGIN_EXAMPLE
	if (likely(nr_segs == 1))
		buf = iov->iov_base + written;
	else {
		filemap_set_next_iovec(&cur_iov, &iov_base, written);
		buf = cur_iov->iov_base + iov_base;
	}
  #+END_EXAMPLE 
- offset变量:当前要写的文件的位置以页为单位的页内偏移量
- index变量:当前要写的文件的位置以页为单位的文件页号
- bytes变量:从当前要写的文件的位置开始到该位置所在的文件页的结束位置为止还有多少个字节.但
  是后来又成了在当前位置不超过文件页的结束位置情况下可以写多少个字节,因为可能不用写满
  #+BEGIN_EXAMPLE
		bytes = PAGE_CACHE_SIZE - offset;
		if (bytes > count)
			bytes = count;
  #+END_EXAMPLE 
- maxlen变量:与bytes类似,但是不是文件页的,而是iov的
  #+BEGIN_EXAMPLE
		maxlen = cur_iov->iov_len - iov_base;
		if (maxlen > bytes)
			maxlen = bytes;
  #+END_EXAMPLE 
- fault_in_pages_readable()与fault_in_pages_writeable()类似
- 调用__grab_cache_page()从文件的radix树里找出要写的页,在这个函数里,若找不到,那么就创建一
  个page cache并放到radix树里(add_to_page_cache()),且放到pagevec里.
- 调用prepare_write(),下面有介绍
- prepare_write()之后就可以写页了,如果是一个段就调用filemap_copy_user()把用户的buf拷贝到
  page里去,如果是多个段,那么就调用filemap_copy_from_user_iovec()把所有的段一起拷贝,其中也
  只是连续调用filemap_copy_from_user()
- 把用户空间的数据拷贝到页之后就调用commit_write(),对于普通文件的commit_write()方法是
  generic_commit_write()
** int generic_commit_write(struct file *file, struct page *page,
*** fs/buffer.c:
- 这个函数调用__block_commit_write()把在page页的从from到to之间的buffer_head的都设置
  BH_Uptodate和设置BH_Dirty,同时设置这两个标志并不予盾,因为BH_Uptodate并不表示磁盘的数据与
  buffer_head里的数据是一致的,而且在这种情况下buffer_head的数据是比磁盘还要新的.下面有介绍
** static int __block_commit_write(struct inode *inode, struct page *page, unsigned from, unsigned to)
*** fs/buffer.c:
- 这个函数 的作用的对在page页里的所有的buffer_head都扫描一次,若发现有buffer_head的空间部分
  或全部是落在from到to之间的,那么就设置BH_Uptodate,注意是部分落在那个区间也会设置,那么没有
  落在这个区间里的数据也是最新的吗?除了设置BH_Uptodate还要调用mark_buffer_dirty(),下面有介
  绍.
- 到最后若所有的buffer_head都已是最新的时候,就设置PG_Uptodate.
- 所以这个函数大概就是设置了buffer_head的一些标志,page的标志,radix树,移动inode.而这些就是
  commit的全部意思. 
** static int blkdev_prepare_write(struct file *file, struct page *page, unsigned from, unsigned to)
*** fs/block_dev.c:
- 这个函数是块设备文件的prepare_write()函数,与普通文件一样都是调用block_prepare()只是
  传的get_block()函数是blkdev_get_block()
** static int blkdev_commit_write(struct file *file, struct page *page, unsigned from, unsigned to)
*** fs/block_dev.c:
- 这个函数是块设备的commit_write()方法,里面封装了block_commit_write(),与
  generic_commit_write()不同是如果扩大了普通文件,那么就要修改inode.i_size且把inode弄成脏.
  #+BEGIN_EXAMPLE
	if (pos > inode->i_size) {
		i_size_write(inode, pos);
		mark_inode_dirty(inode);
	}
  #+END_EXAMPLE 
** void fastcall mark_buffer_dirty(struct buffer_head *bh)
*** fs/buffer.c:
- (1)这个函数先设置参数buffer_head为BH_Dirty,
  
  (2)然后设置它所在的页的PG_Dirty,
  
  (3)然后增加page_state.nr_dirty
  
  (4)然后设置radix树的PAGECACHE_TAG_DIRTY,
  
  (5)然后以I_DIRTY_PAGES调用__mark_inode_dirty()

- 为什么调用 __mark_inode_dirty()时要判断address_space.host呢?而调用radix_tree_tag_set()这
  些就不呢?注释有说这个判断相当于
  #+BEGIN_EXAMPLE
				/* !PageAnon && !swapper_space */
  #+END_EXAMPLE
  那么就是说PageAnon或swapper_space时都有radix树吗?都有address_space吗?也就是说radix树不是
  只针对普通文件的吗?

** int block_prepare_write(struct page *page, unsigned from, unsigned to, get_block_t *get_block)
*** fs/buffer.c:
- 普通文件的prepare_write()函数一般会封装这个函数
  #+BEGIN_EXAMPLE
int ext2_prepare_write(struct file *file, struct page *page,
unsigned from, unsigned to)
{
return block_prepare_write(page, from, to, ext2_get_block);
}
  #+END_EXAMPLE 
- 这个函数调用一次只能处理一个页.
- BH_new:BH_New Set if the corresponding block has just been allocated and has never been
  accessed
- 参数from,to是指在page这个页的页内偏移量
- 这个函数的第一个for循环的作用是处理page里所有的buffer_head的
  BH_new,BH_Mapped,BH_Uptodate,且有可能从disk读数据到buffer_head.因为参数from,to是指在page
  这个页的页内偏移量,且循环是从第一个buffer_head开始的,所以block_start变量一开始是为0的.
  #+BEGIN_EXAMPLE
	for(bh = head, block_start = 0; bh != head || !block_start;
	    block++, block_start=block_end, bh = bh->b_this_page)
  #+END_EXAMPLE

  如果至少一个buffer_head的范围不与from到to这个范围相交,那么就至少执行下面的if里的语句,
  #+BEGIN_EXAMPLE
		if (block_end <= from || block_start >= to) {
			if (PageUptodate(page)) {
				if (!buffer_uptodate(bh))
					set_buffer_uptodate(bh);
			}
			continue;
		}
  #+END_EXAMPLE
  如果这个页是uptodate的,那么buffer_head也要设置uptodate,接着就跳过这个buffer_head.

  接下来是清掉BH_New表示这个buffer_head已经不是还没有被访问过的了.
  #+BEGIN_EXAMPLE
		if (buffer_new(bh))
			clear_buffer_new(bh);
  #+END_EXAMPLE

  接下来是判buffer_head有没有已经被指定映射到某个块上去了,
  #+BEGIN_EXAMPLE
		if (!buffer_mapped(bh)) {
  #+END_EXAMPLE
  若没有被映射,那么就要调用参数get_block()来获取相应的块号来初始化buffer_head.b_blocknr.因
  为在一些get_block()函数里可能会创建给文件创建一个新的块,ulk: The get_block function
  could allocate a new physical block for the file (for instance, if the accessed block
  falls inside a "hole" of the regular file; see the section "File Holes" in Chapter
  18). In this case, it sets the BH_New flag.所以调用完get_block()之后要再次检查BH_New
  #+BEGIN_EXAMPLE
			err = get_block(inode, block, bh, 1);
			if (err)
				break;
			if (buffer_new(bh)) {
				clear_buffer_new(bh);
  #+END_EXAMPLE 

  突然想到一个问题,就是一个块既以普通文件的形式被读到一个page cache里,又以块设备文件的方式
  被读到了一个page buffer里,那么如果要写相同的块,那么是如何同步的呢?难道是写时都会把另一个
  给写回disk?"另一个"在内核注释里叫aliase.真的是要把另一个给写回磁盘
  #+BEGIN_EXAMPLE
			if (buffer_new(bh)) {
				clear_buffer_new(bh);
				unmap_underlying_metadata(bh->b_bdev,
							bh->b_blocknr);
  #+END_EXAMPLE
  unmap_underlying_metadata()就是被调用来把aliase用等待的方式给回写到disk,

  为什么老是要检查页的PG_Uptodate,而不把页给锁住呢?

  下面这一句还有必要吗?不是已经在之前已经确定过成立了吗?
  #+BEGIN_EXAMPLE
				if (block_end > to || block_start < from) {
  #+END_EXAMPLE

  为什么要把BH_New没有设置的块的里的不在from到to这个范围内的空间清0呢?难道是因为这个块是一
  个文件洞?还有既然写disk是写一个块的,那么这些被清0的数据不是也被写到了disk了吗?ulk只有一
  句这样的话:if the write operation does not rewrite the whole buffer in the page, it
  fills the unwritten portion with zero's. 在这种情况下是不会从磁盘读数据的,而是考虑下一个
  块,因为这个块本来就没有在磁盘上,而是通过get_block()函数创建的,可能是一个洞,但是要追加一
  个文件时是不是也会新建一个块呢?
  #+BEGIN_EXAMPLE
				if (block_end > to || block_start < from) {
					void *kaddr;

					kaddr = kmap_atomic(page, KM_USER0);
					if (block_end > to)
						memset(kaddr+to, 0,
							block_end-to);
					if (block_start < from)
						memset(kaddr+block_start,
							0, from-block_start);
					flush_dcache_page(page);
					kunmap_atomic(kaddr, KM_USER0);
				}
  #+END_EXAMPLE

  BH_Delay Set if the buffer is not yet allocated on disk

  在循环的最后会调用ll_rw_block()来提交buffer_head.
- 退出循环之后,要对每个提交的buffer_head进行等待,直到所有提交的buffer_head都已经读取完成.原
  来这个prepare函数还会从disk里读数据的,还挺繁重的.
- 
** static inline void filemap_set_next_iovec(const struct iovec **iovp, size_t *basep, size_t bytes)
*** mm/filemap.c:
- 这个函数被generic_file_buffered_write()调用用来找出从参数iovp这个iov的base偏移量开始增加
  bytes个字节的偏移量所在的iov的偏移量,iovp是一个指针数组.所以这个函数的输出是一个iov和在该iov的偏移量.
- 这个函数可以做一个函数库.

** void inode_update_time(struct inode *inode, int ctime_too)
*** fs/inode.c:
- 这个函数修改mtime和看ctime_too参数修改ctime.若真的修改了,那么就调用mark_inode_dirty_sync()
- 不明白里面调用的current_fs_time()是什么原理.
** int remove_suid(struct dentry *dentry)
*** mm/filemap.c:
- 如果一个程序设置了SUID，则euid和egid变成被运行的程序的所有者的uid和gid.SUID的作用就是这样：
  让本来没有相应权限的用户运行这个程序时，可以访问他没有权限访问的资源。passwd就是一个很鲜
  明的例子。
- 关于清S_ISUID,若原来设置S_ISUID,且有权限清掉它(CAP_FSETID),那么就清掉它.
- 关于清S_ISGID,若原来设置S_ISGID且S_IXGRP,且有权限清掉它(CAP_FSETID),那么就清掉它.
** inline int generic_write_checks(struct file *file, loff_t *pos, size_t *count, int isblk)
*** mm/filemap.c:
- 这个函数写文件的操作进行一些检查(1)file.f_error有设就返回这个f_error.
  #+BEGIN_EXAMPLE
        if (unlikely(file->f_error)) {
                int err = file->f_error;
                file->f_error = 0;
                return err;
        }
  #+END_EXAMPLE
  是不是说明有设file.f_error时就不能写这个文件了呢?

  (2)这里都是在文件不是块设备的情况下,<1>若设置O_APPEND,那么pos就设为文件尾,文件尾就是
  i_size_read().<2>若pos超过了文件的最大范围,那么就所SIGXFSZ信号且返回,表示不能写.<3>若要
  写的字节数超过了文件最大的范围,那么就杷要写的字节改小到最大的范围.
  #+BEGIN_EXAMPLE
	if (!isblk) {
		/* FIXME: this is for backwards compatibility with 2.4 */
		if (file->f_flags & O_APPEND)
                        *pos = i_size_read(inode);

		if (limit != RLIM_INFINITY) {
			if (*pos >= limit) {
				send_sig(SIGXFSZ, current, 0);
				return -EFBIG;
			}
			if (*count > limit - (typeof(limit))*pos) {
				*count = limit - (typeof(limit))*pos;
			}
		}
	}
  #+END_EXAMPLE
  也不能超过文件系统的最大限制inode.i_sb.s_maxbytes
  #+BEGIN_EXAMPLE
	if (likely(!isblk)) {
		if (unlikely(*pos >= inode->i_sb->s_maxbytes)) {
			if (*count || *pos > inode->i_sb->s_maxbytes) {
				send_sig(SIGXFSZ, current, 0);
				return -EFBIG;
			}
			/* zero-length writes at ->s_maxbytes are OK */
		}

		if (unlikely(*pos + *count > inode->i_sb->s_maxbytes))
			*count = inode->i_sb->s_maxbytes - *pos;
	}
  #+END_EXAMPLE 
- 如果文件是一个块设备文件,要判断这个块设备是不是只读的
  #+BEGIN_EXAMPLE
		if (bdev_read_only(I_BDEV(inode)))
			return -EPERM;
  #+END_EXAMPLE
  只读就要返回EPERM

  还有当前的位置和要写的字节数不能超过设备的限制
  #+BEGIN_EXAMPLE
		if (*pos >= isize) {
			if (*count || *pos > isize)
				return -ENOSPC;
		}

		if (*pos + *count > isize)
			*count = isize - *pos;
  #+END_EXAMPLE 
** int mpage_writepages(struct address_space *mapping, struct writeback_control *wbc, get_block_t get_block)
*** fs/mpage.c:
- ulk:When the kernel wants to effectively start the I/O data transfer, it ends up
  invoking the writepages method of the file's address_space object, which searches for
  dirty pages in the radix-tree and flushes them to disk. 所以要真的传一个块的时候就会调用
  address的writepages方法.所以调用这个函数一定会把数据写回磁盘,只是看它在写时会不会被阻塞.
- ext2的writepages方法就是用这个函数实现的
  #+BEGIN_EXAMPLE

static int
ext2_writepages(struct address_space *mapping, struct writeback_control *wbc)
{
	return mpage_writepages(mapping, wbc, ext2_get_block);
}
  #+END_EXAMPLE
- if a filesystem does not define the writepages method, the kernel invokes directly
  mpage_writepages( ) passing NULL as third argument.
- 如果不想写时阻塞,且现在的写队列是阻塞的,那么就退出
  #+BEGIN_EXAMPLE
	if (wbc->nonblocking && bdi_write_congested(bdi)) {
		wbc->encountered_congestion = 1;
		return 0;
	}
  #+END_EXAMPLE
- 那个scanned局部量是什么意思呢?
- 关于index的初始化:为什么设置了WB_SYNC_NONE就从address_space.writeback_index开始回写页呢?
  #+BEGIN_EXAMPLE
	if (wbc->sync_mode == WB_SYNC_NONE) {
		index = mapping->writeback_index; /* Start from prev offset */
	} else {
		index = 0;			  /* whole-file sweep */
		scanned = 1;
	}
  #+END_EXAMPLE
  为什么没有设置WB_SYNC_NONE就要设置scanned呢?
  这还没有完,还要根据writeback_control.start来设置index
  #+BEGIN_EXAMPLE
	if (wbc->start || wbc->end) {
		index = wbc->start >> PAGE_CACHE_SHIFT;
		end = wbc->end >> PAGE_CACHE_SHIFT;
		is_range = 1;
		scanned = 1;
	}
  #+END_EXAMPLE
  这时也会设置scanned,也会设置is_range,那么这个is_range的意思就是指这次的回写是被指定的范
  围的,若是指定了范围,那么就要设置address_space.writeback_index为最后一个回写的页的index
  #+BEGIN_EXAMPLE
	if (!is_range)
		mapping->writeback_index = index;
  #+END_EXAMPLE
- pagevec_lookup_tag()的介绍下面有.
- 因为end的初始值是-1,所以就算writeback_control里没有要求回写的页的范围,那个while循环的判
  断也不会有问题
  #+BEGIN_EXAMPLE
	while (!done && (index <= end) &&
			(nr_pages = pagevec_lookup_tag(&pvec, mapping, &index,
			PAGECACHE_TAG_DIRTY,
			min(end - index, (pgoff_t)PAGEVEC_SIZE-1) + 1)))
  #+END_EXAMPLE
- 一进入while()就会设置scanned,且在里面不再会有改变,所以结合上面可以看出,在
  writeback_control.sync_mode为WB_SYNC_NONE且writeback_contro.start和writeback_control.end
  没有设置且在radix树里找到了脏页,那么scanned还是会设置为0的,为什么要这么做呢?而在没有设置
  WB_SYNC_NONE时不管有没有脏页,都会设置scanned,不管有没有设置writeback_control.start和end
  都会设置scanned.
- 关于done变量,是在while()里的for()循环里被修改以告诉外面的while()循环等这次的for()循环把
  所有的页全都循环一次之后就不用循环了(for()里是没有break的只有continue).在什么情况下会设
  置呢?(1)在writeback_control设置了范围,且从当前在开始已超过了设置的范围
  #+BEGIN_EXAMPLE

			if (unlikely(is_range) && page->index > end) {
				done = 1;
				unlock_page(page);
				continue;
			}
  #+END_EXAMPLE
  觉得没有必要用continue了,应该是break.从pagevec_lookup_tag()返回的页是有可能会超出范围的,
  因为这个页只是找出指定个数的脏页,而不管这些页是否在某一个范围内.
  
  (2)在写一个页时有错或writeback_control.nr_to_write要求写的页数已经被够了,那么就设置done
  #+BEGIN_EXAMPLE
			if (ret || (--(wbc->nr_to_write) <= 0))
				done = 1;
  #+END_EXAMPLE 
  要求被写的页数和要求被写的页所在的范围并不冲突.
  (3)writeback_control.nonblocking设置了且遇到了阻塞
  #+BEGIN_EXAMPLE
			if (wbc->nonblocking && bdi_write_congested(bdi)) {
				wbc->encountered_congestion = 1;
				done = 1;
			}
  #+END_EXAMPLE
- 在for()循环里若发现将要被写的页是正在回写的且设置了WB_SYNC_NONE,那么就要等待
  #+BEGIN_EXAMPLE
			if (wbc->sync_mode != WB_SYNC_NONE)
				wait_on_page_writeback(page);
  #+END_EXAMPLE
- 现在在for()里剩下的就只有调用函数写页了.若参数get_block()为空,就说明调用这个函数的函数不
  是address_space.a_ops.writepages,ulk:If the get_block parameter is NULL (no writepages
  method defined), it invokes the mapping->writepage method of the address_space object of
  the file to flush the page to disk. Otherwise, if the get_block parameter is not NULL,
  it invokes the mpage_writepage()function.
  #+BEGIN_EXAMPLE

	writepage = NULL;
	if (get_block == NULL)
		writepage = mapping->a_ops->writepage;
  #+END_EXAMPLE
- 
** static int ext2_writepage(struct page *page, struct writeback_control *wbc)
*** fs/ext2/inode.c:
- 这个函数就是直接调用block_write_full_page(),但是调用block_write_full_page()时也会用到
  get_block()函数

** unsigned pagevec_lookup_tag(struct pagevec *pvec, struct address_space *mapping,pgoff_t *index, int tag, unsigned nr_pages)
*** mm/swap.c:
- 这个函数的最终结果是把在mapping这个address_space里从index开始打算找出nr_pages个在radix树
  里有tag标签的页,并把这些页的描述符按顺序放到pvec.page数组里,但真正找到的页数是
  pvec.nr,pvec.nr也等于返回值.同时还把*index修改成pvec.page数组里的最后一个页的page.index.
** static struct bio *__mpage_writepage(struct bio *bio, struct page *page, get_block_t get_block,sector_t *last_block_in_bio, int *ret, struct writeback_control *wbc,	writepage_t writepage_fn)
*** fs/mpage.c:
- 这个函数被mpage_writepages()和mpage_writepage()调用
- 先看看注释
  #+BEGIN_EXAMPLE
/*
 * Writing is not so simple.
 *
 * If the page has buffers then they will be used for obtaining the disk
 * mapping.  We only support pages which are fully mapped-and-dirty, with a
 * special case for pages which are unmapped at the end: end-of-file.
 *
 * If the page has no buffers (preferred) then the page is mapped here.
 *
 * If all blocks are found to be contiguous then the page can go into the
 * BIO.  Otherwise fall back to the mapping's writepage().
 * 
 * FIXME: This code wants an estimate of how many pages are still to be
 * written, so it can intelligently allocate a suitably-sized BIO.  For now,
 * just allocate full-size (16-page) BIOs.
 */
  #+END_EXAMPLE
  可以看出这个函数是使用page里的buffer_head来从disk里读数据的,只支持全部都是
  mapped-and-dirty的页和没有映射到文件尾的页.若页没有buffer_head那么就在这个函数里作映射.
  
  若发现这个页里的所有的buffer_head之间是连续的,就进入BIO,否则就调用
  address_space.a_ops.writepage()
- 若这个页是有buffer_head的,那么要么就跳到confused,要么就跳到page_is_mapped.因为在这里还有
  bio的处理,所以从里跳到confused时是不会执行mpage_bio_submit()的,这时若是
  mpage_writepages()而不是mpage_writepage()调用__mpage_writepage(),那么就会执行参数
  writepage_fn函数,这个函数就是address_space.a_ops.writepage.在ext2里
  address_space.a_ops.writepage是由把block_write_full_page()封装起来的ext2_writepage()实现
  的.
- 从下面这段代码可以看出,只要发现有一个buffer_head是没有设置BH_Mapped的,那么就一定会跳到
  confused的,且在接下来循环处理其它的buffer_head时是没有什么意义的,但是为什么还要做呢?
  #+BEGIN_EXAMPLE
			if (!buffer_mapped(bh)) {
				/*
				 * unmapped dirty buffers are created by
				 * __set_page_dirty_buffers -> mmapped data
				 */
				if (buffer_dirty(bh))
					goto confused;
				if (first_unmapped == blocks_per_page)
					first_unmapped = page_block;
				continue;
			}

			if (first_unmapped != blocks_per_page)
				goto confused;	/* hole -> non-hole */
  #+END_EXAMPLE 
  那个hole -> non-hole的注释是什么意思呢?
  
  在没有设置BH_Mapped但是设了BH_Dirty时是什么情况下做呢?有注释
  #+BEGIN_EXAMPLE
				/*
				 * unmapped dirty buffers are created by
				 * __set_page_dirty_buffers -> mmapped data
				 */
  #+END_EXAMPLE 
  
  可以看出first_unmapped指的是第一个没有设置BH_Mapped且没有设置BH_Dirty的buffer_head的页内
  块号.
- 不为脏或为脏但不是Uptodated就跳到confused，这是为什么呢?
- 从下面的代码可以看出放在blocks数组里的块要是连续的
  #+BEGIN_EXAMPLE
			if (page_block) {
				if (bh->b_blocknr != blocks[page_block-1] + 1)
					goto confused;
			}
  #+END_EXAMPLE 
- 所以blocks数组是存放的是设置了BH_Mapped的块，且是脏的和Uptodated的,且块是连续的
- 不知道下面的代码有什么用
  #+BEGIN_EXAMPLE
			if (boundary) {
				boundary_block = bh->b_blocknr;
				boundary_bdev = bh->b_bdev;
			}
  #+END_EXAMPLE
- first_unmapped为0表里blocks数组里没有有效的元素,那么它就跳到confused,若是里有有效的元素,
  那么它就跳到page_is_mapped.那么什么情况下会出现跳到page_is_mapped呢?
  
  在做么情况下会while()循环后面的goto confused呢?有注释
  #+BEGIN_EXAMPLE
		} while ((bh = bh->b_this_page) != head);

		if (first_unmapped)
			goto page_is_mapped;

		/*
		 * Page has buffers, but they are all unmapped. The page was
		 * created by pagein or read over a hole which was handled by
		 * block_read_full_page().  If this address_space is also
		 * using mpage_readpages then this can rarely happen.
		 */
		goto confused;
  #+END_EXAMPLE 
  所以while循环里的continue是有用的,如果对于所有的buffer_head都执行到了continue,那么就会最
  后就会执行goto confused.
  
  注释里的pagein是指换入页的意思吗?
  
  在while循环里能把所有的buffer_head都处理一次,那么first_unmapped要么是0要么是
  blocks_per_page.所以跳到page_is_mapped是指页里的所有的buffer_head都是被映射的,且都符合存
  放在blocks数组里的条件,因为都已放到blocks数组里了.
- 第二个for循环比较简单,在页没有buffer_head映射的情况下才会执行到这里,到这里也没有必要给页
  分配buffer_head,这个for循环的目的就是把该页里所映射的块的块号按顺序放到blocks数组里,注意
  要么执行第一个while循环要么执行第二个for循环,不可能执行完第一个while循环又执行第二个for
  循环.
  
  如果在块设备里找不到相应的块号,那么就跳到confused,不再找后面的块了.
  #+BEGIN_EXAMPLE
		map_bh.b_state = 0;
		if (get_block(inode, block_in_file, &map_bh, 1))
			goto confused;
  #+END_EXAMPLE
  
  若buffer_head是新的,那么就把它的aliase块回写,但是为什么要检查BH_New呢?不是都会设置的吗?
  #+BEGIN_EXAMPLE
		if (buffer_new(&map_bh))
			unmap_underlying_metadata(map_bh.b_bdev,
						map_bh.b_blocknr);
  #+END_EXAMPLE 
  
  不知道下面是什么意思
  #+BEGIN_EXAMPLE
		if (buffer_boundary(&map_bh)) {
			boundary_block = map_bh.b_blocknr;
			boundary_bdev = map_bh.b_bdev;
		}
  #+END_EXAMPLE
  
  若发现当前块不与上一个块连续,就跳到confused,怎么可能会不连续,在一个页里的块不是都是连续
  的吗?
  
  #+BEGIN_EXAMPLE
		if (page_block) {
			if (map_bh.b_blocknr != blocks[page_block-1] + 1)
				goto confused;
		}
  #+END_EXAMPLE 
  所以没有把这个页的所有的块放到blocks数组都会跳到confused,若页里的所有的块都放到了blocks
  数组里了,那么就会往下执行到page_is_mapped处,注意是有可能page_block不与blocks_per_page的,
  因为这个页映射到了文件尾.
- 到page_is_mapped标签这里主要是看这个页是不是文件页的最后一页,若是最后一页那么就要处理映
  射文件结尾处之后的空间,要把没有用的页空间给清0.是不是page.index是从1开始计数的,不然为什
  么在page.index大于end_index时就跳到confused而不清0呢?
- 下面的这段代码中,若bio不为空的情况是在mpage_writepaegs()里调用这个函数时出现
  的,__mpage_writepage()趴有mpage_writepages()和mpage_writepage()调用,代码里的那个判断是什
  么意思吗?
  #+BEGIN_EXAMPLE

	/*
	 * This page will go to BIO.  Do we need to send this BIO off first?
	 */
	if (bio && *last_block_in_bio != blocks[0] - 1)
		bio = mpage_bio_submit(WRITE, bio);
  #+END_EXAMPLE 
- 接下来就是分配bio了,现在分配bio这一段还是看不懂,
  #+BEGIN_EXAMPLE
  	if (bio == NULL) {
		bio = mpage_alloc(bdev, blocks[0] << (blkbits - 9),
				bio_get_nr_vecs(bdev), GFP_NOFS|__GFP_HIGH);
		if (bio == NULL)
			goto confused;
	}

	/*
	 * Must try to add the page before marking the buffer clean or
	 * the confused fail path above (OOM) will be very confused when
	 * it finds all bh marked clean (i.e. it will not write anything)
	 */
	length = first_unmapped << blkbits;
	if (bio_add_page(bio, page, length, 0) < length) {
		bio = mpage_bio_submit(WRITE, bio);
		goto alloc_new;
	}
  #+END_EXAMPLE

  注意执行到alloc_new这里之后就不会再有可能执行confused了,除非分配不到bio.

  http://blog.csdn.net/yunsongice/article/details/6171260
  对于普通文件，假设没有遇到文件的洞，那么一个页面所包含的4个块总是连续的，mpage_alloc调用
  的bio_alloc_bioset函数就只分配一个bio_vec结构。随后do_mpage_readpage通过调用bio_add_page
  将这个结构的bv_page指向对于的页描述符，bv_len设置为4个块的大小4096字节，bv_offset为0。
  bio_add_page还有一些合并段的工作，即不同的段在RAM中相应的页框正好是连续的并且在磁盘上相应
  的数据块也是相邻的，那么就合并它们。

  如果blocks数组里有一个块是设置了BH_Boundary的了,那么bio_add_page()这个函数会不会也把这个
  块给认为与后面的块不是连续的而把它们加到同一个bio里呢?
  
- 现在开始清掉已经加到blocks数组里的buffer_head的BH_Dirty,
  #+BEGIN_EXAMPLE

		do {
			if (buffer_counter++ == first_unmapped)
				break;
			clear_buffer_dirty(bh);
			bh = bh->b_this_page;
		} while (bh != head);
  #+END_EXAMPLE

		
- 不明白下面的代码什么意思
  #+BEGIN_EXAMPLE
		/*
		 * we cannot drop the bh if the page is not uptodate
		 * or a concurrent readpage would fail to serialize with the bh
		 * and it would read from disk before we reach the platter.
		 */
		if (buffer_heads_over_limit && PageUptodate(page))
			try_to_free_buffers(page);
  #+END_EXAMPLE




** not function
*** 17CHARPT
- 什么是映射的页:a page is said to be mapped if it maps a portion of a file. For instance,
  all pages in the User Mode address spaces belonging to file memory mappings are mapped,
  as well as any other page included in the page cache. In almost all cases, mapped pages
  are syncable: in order to reclaim the page frame, the kernel must check whether the page
  is dirty and, if necessary, write the page contents in the corresponding disk file.用户
  态地址空间中的文件内存映射页是mapped,page cache里的页是mapped,要回收这种页就可能要回写磁
  盘
- 什么是匿名的页:a page is said to be anonymous if it belongs to an anonymous memory
  region of a process (for instance, all pages in the User Mode heap or stack of a process
  are anonymous). In order to reclaim the page frame, the kernel must save the page
  contents in a dedicated disk partition or disk file called "swap area" (see the later
  section "Swapping"); therefore, all anonymous pages are swappable.

** int try_to_unmap(struct page *page)
*** mm/rmap.c:
- 这个函数是给撤销内存映射用的,在反向映射这一节介绍.
- 注释:try to remove all page table mappings to a page
- 从返回值都是以SWAP开头的,所以这个函数应该是在页交换的时候调用的.
  #+BEGIN_EXAMPLE
 * SWAP_SUCCESS	- we succeeded in removing all mappings
 * SWAP_AGAIN	- we missed a mapping, try again later
 * SWAP_FAIL	- the page is unswappable
 */
  #+END_EXAMPLE
- 这些页表的映射是在哪些函数建立的呢?
- ulk:First of all, the PFRA must have a way to determine whether the page to be reclaimed
  is shared or non-shared, and whether it is mapped or anonymous.因为这个函数会处理匿名页
  和映射页的撤销映射,所以这个函数都能处理无论是共享的还是非共享的,还是匿名页还是映射页.
- 关于PageAnon的原理,ulk:The mapping field of the page descriptor determines whether the
  page is mapped or anonymous, as follows:

  If the mapping field is NULL, the page belongs to the swap cache (see the section "The
  Swap Cache" later in this chapter).

  If the mapping field is not NULL and its least significant bit is 1, it means the page
  is anonymous and the mapping field encodes the pointer to an anon_vma descriptor (see
  the next section, "Reverse Mapping for Anonymous Pages").

  If the mapping field is non-NULL and its least significant bit is 0, the page is mapped;
  the mapping field points to the address_space object of the corresponding file (see the
  section "The address_space Object" in Chapter 15).

  Every address_space object used by Linux is aligned in RAM so that its starting linear
  address is a multiple of four. Therefore, the least significant bit of the mapping field
  can be used as a flag denoting whether the field contains a pointer to an address_space
  object or to an anon_vma descriptor. This is a dirty programming trick, but the kernel
  uses a lot of page descriptors, thusthese data structures should be as small as
  possible. The PageAnon( ) function receives as its parameter the address of a page
  descriptor and returns 1 if the least significant bit of the mapping field is set, 0
  otherwise. 
** static int try_to_unmap_anon(struct page *page)
*** mm/rmap.c:
- 这个函数是给try_to_unmap()调用来unmap一个匿名区的.
- ulk有一句话even if an anonymous memory region includes different pages, there always is
  just one reverse mapping list for all the page frames in the region.
- ulk:When the kernel assigns the first page frame to an anonymous region, it creates a
  new anon_vma data structure, which includes just two fields: lock, a spin lock for
  protecting the list against race conditions, and head, the head of the doubly linked
  circular list of memory region descriptors. Then, the kernel inserts the vm_area_struct
  descriptor of the anonymous memory region in the anon_vma 's list; to that end, the
  vm_area_struct data structure includes two fields related to this list: anon_vma_node
  stores the pointers to the next and previous elements in the list, while anon_vma points
  to the anon_vma data structure. Finally, the kernel stores the address of the anon_vma
  data structure in the mapping field of the descriptor of the anonymous page, as
  described previously.

  ulk说了,是在给一个匿名区分配 _第一个_ 页框时 _就_ (_才_)会分配一个anan_vma结构体,为了以
  后给这个匿名区的页的共享做反向映射的打算.

  anon_vma.head, vm_area_struct.anon_vma_node, vm_area_struct.anon_vma, page.mapping这三个
  成员是如何组成一个完整的反向映射链表的.anon_vma.head是该链表的链表头,这个链表是把
  vm_area_struct这个结构体给链起来的,要想把这个结构体给链起来,就要把
  vm_area_struct.anon_vma_node这个结点插到该链表里去,有时想通过vm_area_struct这个结构体找
  到被链到那个反向映射链表的表头,所以就要vm_area_struct.anon_vma指向链表头了,要想知道匿名
  区里哪个页使用了反向映射,那么这个页就要用page.mapping来标明了,page.mapping指向被链到的链
  表的表头,就是anon_vma结构体.只要看明白ulk的图17-1就可以了.

  有一个问题,就是若一个匿名区里共享了其它不同的匿名区的页框,但是一个匿名区只能链到一个
  vma_anon链表里去,那么这种情况该如何处理呢?可能是会把这个匿名区拆分开来.
- 这个函数就是对每个在page.mapping里所存放的anon_vma链表里的每一个vm_area_struct调用一次
  try_to_unmap_one().
** static int try_to_unmap_one(struct page *page, struct vm_area_struct *vma)
*** mm/rmap.c:
- 用vma_address()来计算page在内存区所对应的线性地址,下面有介绍
- 调用page_check_address()检查address这个线性地址是不是与page这个页是对应的,若是对应就返回
  指向该页的pte页表项.但是为什么会出现用address找到的pte项所指向的页框不是page页框呢?ulk有
  说:
  (1)The Page Table entry refers to a page frame assigned with COW , but the anonymous
  memory region identified by vma still belongs to the anon_vma list of the original page
  frame.

  (2)The mremap( ) system call may remap memory regions and move the pages into the User
  Mode address space by directly modifying the page table entries. In this particular
  case, object-based reverse mapping does not work, because the index field of the page
  descriptor cannot be used to determine the actual linear address of the page.
  
  (3)The file memory mapping is non-linear (see the section "Non-Linear Memory Mappings"
  in Chapter 16).

  第(1)种情况是因为之前是共享page来读的,但是因为要写了,所以要做COW操作来创建一个新的页框来
  写,所以就把指向原来共享page的pte页表项给修改以指向新的页框,但还没有来得及把这个
  vm_area_struct从因共享原来page而链到anon_vma链表里删除.

  第(2)(3)还不太懂
- 锁住和保留的匿名区是不可以被交换出去的,而调用这个函数就是在想交换页时调用的,所以发现这样
  的页就不可以了.
- 若这个页的Accessed位设置了,那么就说明这个页在使用,也就说明这个页不可以被交换,但是为什么
  要清掉这个标志呢?要返回SWAP_FAIL.ulk:Checks that the Accessed bit inside the Page Table
  entry is cleared; if not, the function clears the bit and returns SWAP_FAIL. If the
  Accessed bit is set, the page is considered in-use, thus it should not be reclaimed.
  #+BEGIN_EXAMPLE
	if ((vma->vm_flags & (VM_LOCKED|VM_RESERVED)) ||
			ptep_clear_flush_young(vma, address, pte)) {
		ret = SWAP_FAIL;
		goto out_unmap;
	}
  #+END_EXAMPLE
- ulk:Checks whether the page belongs to the swap cache (see the section "The Swap Cache"
  later in this chapter) and it is currently being handled by get_user_pages( ) (see the
  section "Allocating a Linear Address Interval" in Chapter 9); in this case, to avoid a
  nasty race condition, the function returns SWAP_FAIL.

  上面的这段话对应的代码是
  #+BEGIN_EXAMPLE

	/*
	 * Don't pull an anonymous page out from under get_user_pages.
	 * GUP carefully breaks COW and raises page count (while holding
	 * page_table_lock, as we have here) to make sure that the page
	 * cannot be freed.  If we unmap that page here, a user write
	 * access to the virtual address will bring back the page, but
	 * its raised count will (ironically) be taken to mean it's not
	 * an exclusive swap page, do_wp_page will replace it by a copy
	 * page, and the user never get to see the data GUP was holding
	 * the original page for.
	 *
	 * This test is also useful for when swapoff (unuse_process) has
	 * to drop page lock: its reference to the page stops existing
	 * ptes from being unmapped, so swapoff can make progress.
	 */
	if (PageSwapCache(page) &&
	    page_count(page) != page_mapcount(page) + 2) {
		ret = SWAP_FAIL;
		goto out_unmap;
	}
  #+END_EXAMPLE
  为什么要加2呢?
- 下面的代码,页表项的dirty位和PG_Dirty是什么关系呢?
  #+BEGIN_EXAMPLE

	/* Move the dirty bit to the physical page now the pte is gone. */
	if (pte_dirty(pteval))
		set_page_dirty(page);
  #+END_EXAMPLE 


 
** static inline unsigned long vma_address(struct page *page, struct vm_area_struct *vma)
*** mm/rmap.c:
- 好像vm_area_struct.start是页的大小的倍数,是吗?
- 在传入的page是一个文件映射页而不是一个匿名映页时,page.index是该页在以页大小为单位时的文件
  的索引,vm_area_struct.vm_pgoff是vm_area_struct.vm_start这个地址在以页大小为单位的文件的
  索引.所以要想知道page在线性地址是多少,只要先用这两个索引得出vm_start和页的起始线性地址的
  差值
  #+BEGIN_EXAMPLE
  ((pgoff - vma->vm_pgoff) << PAGE_SHIFT)
  #+END_EXAMPLE 
  再用vm_start来加上差值.
- 但对于page是一个匿名映页的时又是怎么回事呢?ulk:For anonymous pages, the vma->vm_pgoff
  field is either zero or equal to vm_start/PAGE_SIZE ; correspondingly,the page->index
  field is either the index of the page inside the region or the linear address of the
  page divided by PAGE_SIZE.

  所以(1)在vm_pgoff为0时,那么page.index就是相对于vm_start这个线性地址的偏移量除以PAGE_SIZE的
  值,(2)若vm_pgoff是vm_start/PAGE_SIZE,那么page.index就是页的线性地址除以PAGE_SIZE的值.为
  什么会有这两种情况呢?
- 对于为什么要做以下的检查
  #+BEGIN_EXAMPLE
	if (unlikely(address < vma->vm_start || address >= vma->vm_end)) {
		/* page should be within any vma from prio_tree_next */
		BUG_ON(!PageAnon(page));
		return -EFAULT;
	}
  #+END_EXAMPLE
  ulk有解释:If the target page is anonymous, it checks whether its linear address falls
  inside the memory region; if not, it terminates by returning SWAP_AGAIN. (As explained
  when introducing reverse mapping for anonymous pages, the anon_vma 's list may include
  memory regions that do not contain the target page.)

  那么又为什么会出现anon_vma链表里的内存区不会包含目标页呢?可能是因为一个页里即有被其它匿名
  区共享的页,也有主动去共享其它匿名区的页.比如匿名区A有X1和Y1页,其中X1是与匿名区B共享的,所
  以anon_vma把A和B两个链起来做成了一个循环链表了,而这时A用Y1是共享了C的Y3页,对于A来说是主动
  共享了别人的页,但是要想把A和C链起来,有可能会把原来的anon_vma结构体删掉,而把A和B链接到因C
  创建第一个页框时而建立的anon_vma链表里去,这样A,B和C就链到了同一个anon_vma里去了,但是A,B
  和C并没有共享同一个页框. 
- 之前有看过一个文章说一个页表项的指向的页框被交换出去时页表项存放的是swap cache的索引.下
  面的代码就是做这个工作的
  #+BEGIN_EXAMPLE

	if (PageAnon(page)) {
		swp_entry_t entry = { .val = page->private };
		/*
		 * Store the swap location in the pte.
		 * See handle_pte_fault() ...
		 */
		BUG_ON(!PageSwapCache(page));
		swap_duplicate(entry);
		if (list_empty(&mm->mmlist)) {
			spin_lock(&mmlist_lock);
			list_add(&mm->mmlist, &init_mm.mmlist);
			spin_unlock(&mmlist_lock);
		}
		set_pte_at(mm, address, pte, swp_entry_to_pte(entry));
		BUG_ON(pte_file(*pte));
		dec_mm_counter(mm, anon_rss);
	}
  #+END_EXAMPLE
  这个是在匿名区才有的.

  关于swap_duplicate()

  http://blog.csdn.net/cxylaf/article/details/1626534
  swp_type() 和swp_offset()函数根据页槽索引和交换区号得到type和offset值，函数
  swp_entry(type,offset)得到交换槽。最后一位总是清0表示页不在RAM上。槽最大224(64G)。第一个
  可用槽索引为1。槽索引不能全为0。一个页面可能被多个进程共用，它可能被从一个进程地址空间换
  出但仍然在物理内存上，因此一个页面可能被多次换出。但物理上仅第一次被换出并存储在交换区上，
  接下来的换出操作只是增加swap_map的引用计数。swap_duplicate(swp_entry_t entry)的功能正是用
  户尝试换出一个已经换出的页面。

  swap_duplicate()主要就是增加swap_map的计数而已.

  page.private是什么时候保存了swap cache的index了呢?

  对于文件映射的unmap并没有使用swap cache.

  对匿名页并没有因为换出而做磁盘操作.
- 调用page_remove_rmap()减page->_mapcount计数,表示少一个pte引用这个页框.page->_mapcount为
  负数的时候表示没有pte引用这个页框时就会减page_state.nr_mapped,但是有一段注释看不懂
  #+BEGIN_EXAMPLE
		/*
		 * It would be tidy to reset the PageAnon mapping here,
		 * but that might overwrite a racing page_add_anon_rmap
		 * which increments mapcount after us but sets mapping
		 * before us: so leave the reset to free_hot_cold_page,
		 * and remember that it's only reliable while mapped.
		 * Leaving it set also helps swapoff to reinstate ptes
		 * faster for those pages still in swapcache.
		 */
  #+END_EXAMPLE 
- 最后减page的计数(_count),用的是page_cache_release(),所以_count为0时会释放页.

** not function
*** 17.2.2
- 为什么文件映射页要用优先搜索树呢?而匿名页不用类似的方法提高效率呢?ulk:We have seen in
  the previous section that descriptors for anonymous memory regions are collected in
  doubly linked circular lists; retrieving all page table entries referencing a given page
  frame involves a linear scanning of the elements in the list. The number of shared
  anonymous page frames is never very large, hence this approach works well. Contrary to
  anonymous pages, mapped pages are frequently shared, because many different processes
  may share the same pages of code. For instance, consider that nearly all processes in
  the system share the pages containing the code of the standard C library (see the
  section "Libraries" in Chapter 20). For this reason, Linux 2.6 relies on special search
  trees, called "priority search trees ," to quickly locate all the memory regions that
  refer to the same page frame. 

** static int try_to_unmap_file(struct page *page)
*** mm/rmap.c:
- 先找出页在文件里的索引号
  #+BEGIN_EXAMPLE
	pgoff_t pgoff = page->index << (PAGE_CACHE_SHIFT - PAGE_SHIFT);
  #+END_EXAMPLE 
- 接着就是搜索搜索树对找到的所有vm_area_struct来调用try_to_unmap_one()这一段与
  try_to_unmap_anon()类似
  #+BEGIN_EXAMPLE
	vma_prio_tree_foreach(vma, &iter, &mapping->i_mmap, pgoff, pgoff) {
		ret = try_to_unmap_one(page, vma);
		if (ret == SWAP_FAIL || !page_mapped(page))
			goto out;
	}
  #+END_EXAMPLE

  #+BEGIN_EXAMPLE
	list_for_each_entry(vma, &anon_vma->head, anon_vma_node) {
		ret = try_to_unmap_one(page, vma);
		if (ret == SWAP_FAIL || !page_mapped(page))
			break;
	}
  #+END_EXAMPLE 
- 下面就是搜索非线性链表的优先搜索树了.这个要结合优先搜索树和非线性映射来看.
** static inline void add_page_to_active_list(struct zone *zone, struct page *page)
** static inline void add_page_to_inactive_list(struct zone *zone, struct page *page)
** static inline void del_page_from_active_list(struct zone *zone, struct page *page)
** static inline void del_page_from_inactive_list(struct zone *zone, struct page *page)
** static inline void del_page_from_lru(struct zone *zone, struct page *page)
*** include/linux/mm_inline.h:
- 前4个从zone.active_list或zone.inactive_list里添加或删除page
- 最后一个函数先查看page是放在active_list还是inactive_list,然后再删除.还多一点就是清掉
  PG_active.
** void fastcall activate_page(struct page *page)
*** mm/swap.c:
- 若页已设了PG_lru(页在lru里),且页没有设置(PG_active),那么就从incative_list里删除,设置
  PG_active,把页加到active_list里.
** void fastcall lru_cache_add(struct page *page)
*** mm/swap.c:
- 这个函数是把一个页放到lru里去,所谓的lru就是zone里的active_lru和inactive_lru这两个成员和
  每个CPU都有的lru_add_pvecs数组,这三者的关系是若想把一个页放到lru里去,那么就先放到
  lru_add_pvecs里去,等到lru_add_pvecs数组满了,就放到inactive_list里去
- 先调用pagevec_add()把页放到lru_add_pvecs里去,返回值就是lru_add_pvecs还可以放多少个的page
- 若lru_add_pvecs数组满了,那么就调用__pagevec_lru_add()把lru_add_pvecs里的页用
  add_page_to_inactive_list()放到inactive_list里去.
- 放到inactive_list之后就会调用release_pages()来释放页,所以可以得出放到inactive_list之后可
  能会出现该页没有被引用而释放.release_page()下面有介绍
- 这就是为什么要lru的原因,ulk:The PFRA collects the pages that were recently accessed in
  the active list so that it will not scan them when looking for a page frame to
  reclaim. Conversely, the PFRA collects the pages that have not been accessed for a long
  time in the inactive list. Of course, pages should move from the inactive list to the
  active list and back, according to whether they are being accessed.
** void release_pages(struct page **pages, int nr, int cold)
*** mm/swap.c:
- 该函数释放存放在pages数组里的页.
- 先把要释放的页放到pages_to_free里去.最后再调用__pagevec_free()或pagevec_free()来释放
  pages_to_free里的页.
- 若该页是保留的或这个页的计数没为0,就跳过这个页
  #+BEGIN_EXAMPLE
		if (PageReserved(page) || !put_page_testzero(page))
			continue;
  #+END_EXAMPLE
  为什么不检查这个页是不是被锁住了呢?
- 接着就是把PG_lru给去掉,所以在page._count为0时不表明这个页不在lru里
  #+BEGIN_EXAMPLE
		if (TestClearPageLRU(page))
			del_page_from_lru(zone, page);
  #+END_EXAMPLE
- 为什么还要检查一次page._count呢?执行到这里不是已经确过是0了吗?
  #+BEGIN_EXAMPLE
		if (page_count(page) == 0) {
			if (!pagevec_add(&pages_to_free, page)) {
				spin_unlock_irq(&zone->lru_lock);
				__pagevec_free(&pages_to_free);
				pagevec_reinit(&pages_to_free);
				zone = NULL;	/* No lock is held */
			}
		}
  #+END_EXAMPLE 
** void fastcall lru_cache_add_active(struct page *page)
*** mm/swap.c:
- 与lru_cache_add()不同的是用__pagevec_lru_add_active()替代__pagevec_lru_add(),所以也可以
  看出这个函数与lru_cache_add()不同的是把页加到了active_list里了
- 还有一个地方是不同的,就是不是用lru_add_pvecs里的页加到active_list,而是使用
  lru_add_active_pvecs里的页.所以lru_add_pvecs是对的inactive的页,lru_add_active_pvecs是对
  于active里的面.
** void __pagevec_lru_add_active(struct pagevec *pvec)
*** mm/swap.c:
- 与__pagevec_lru_add()不同的是(1)多设置了PG_active,(2)调用add_page_to_active_list()而不是
  add_page_to_inactive_list().可以看出这个函数是把页放到active_list
- 这个函数也会把加入到zone.active_list里的页调用release_pages()来减计数,但是有可能会被释放
  吗?
** void lru_add_drain(void)
*** mm/swap.c:
- 这个函数的作用其实就是lru_cache_add()和lru_cache_add_active()函数的结合,但与它们不同的是
  不用等到lru_add_pvecs和lru_add_active_pvecs满,只要它们里有页就放到zone.inactive_list和
  zone.active_list里去.
** int page_referenced(struct page *page, int is_locked, int ignore_token)
*** mm/rmap.c:
- 这个函数就是(1)把PG_referenced给清了,(2)计算有多少个pte引用这个页,那这个函数是不是与反向
  映射有关呢?要想知道有多少个pte引用这个页,page._mapcount不就是了吗?返回的值一般会比
  page._mapcount的大.
- 这个函数就只有shrink_list使用来获取有多少个pte引用该页.
- ulk说了这个函数的作用:The page_referenced( ) function, which is invoked once for every
  page scanned by the PFRA, returns 1 if either the PG_referenced flag or some of the
  Accessed bits in the Page Table entries was set; it returns 0 otherwise. This function
  first checks the PG_referenced flag of the page descriptor; if the flag is set, it
  clears it. Next, it makes use of the object-based reverse mapping mechanism to check and
  clear the Accessed bits in all User Mode Page Table entries that refer to the page
  frame. To do this, the function makes use of three ancillary functions;
  page_referenced_anon( ) , page_referenced_file( ) , and page_referenced_one( ), which
  are analogous to the try_to_unmap_xxx( ) functions described in the section "Reverse
  Mapping" earlier in this chapter. The page_referenced( ) function also honors the swap
  token; see the section "The Swap Token" later in this chapter.

  所以这个函数的结果是(1)把page的PG_referenced给清了,(2)用反向映射来找出映射这个页的pte页
  表项并把这个页的页表项的accessed位给清掉.

- 清PG_referenced是在page_referenced里直接完成的
  #+BEGIN_EXAMPLE
	if (TestClearPageReferenced(page))
		referenced++;
  #+END_EXAMPLE 

- 清pte的accessed位是在page_referenced()调用的page_referenced_anon()调用的
  page_referenced_one()里清和page_referenced()调用的page_referenced_file()调用的
  page_referenced_one()里清掉.
** static int page_referenced_anon(struct page *page, int ignore_token)
*** mm/rmap.c:
- 在page_referenced()里对于匿名页就调用该函数来得出有多少到匿名区reference这个页.
- 这个函数就是遍历所有的在page.mapping这个anon_vma链表里的匿名区,对链表里的每个匿名区调用
  page_referenced_one(),因为page_referenced_one()的返回值可能是0,1或2.
** static int page_referenced_one(struct page *page, struct vm_area_struct *vma, unsigned int *mapcount, int ignore_token)
*** mm/rmap.c:
- 这个函数是被page_referenced_anon()和page_referenced_file()调用的.
- 通过vma_address()找出page在vma里的线性地址,再用这个地址来调用page_check_address()得到pte
  页表项.
- 若页表项的accessed位设置了,就清掉这个位,并且返回值加1.所以page_referenced()函数会把引用
  这个页的accessed位清掉,清一个加1次计数.
- 若vma的mm不是当前进程的mm,且不能忽略swap token,且mm里有swap_token,返回值还加1.这个是什么
  意思呢?
** static int page_referenced_file(struct page *page, int ignore_token)
*** mm/rmap.c:
- 这个函数是被page_referenced_anon()调用用来看文件映射页的引用情况.
- 文件映射页的反向映射用page.mapping.i_mmap这个搜索树来查找的.这点与page_referenced_anon()
  不同.
- 一旦发现有一个内存区是同时设置了VM_LOCKED和VM_MAYSHARE,那么就给返回值加1并退出搜索了,为
  什么要这样做呢?那为什么page_referenced_anon()不作这种检查呢?
- 对每个内存区调用page_referenced_one()

** not function
*** 17.3.1.4
- ulk:The LRU lists include two kinds of pages: those belonging to the User Mode address
  spaces, and those included in the page cache that do not belong to any User Mode
  process. 所以lru链表有两种类型.

** static void refill_inactive_zone(struct zone *zone, struct scan_control *sc)
*** mm/vmscan.c:
- 这个函数就是把zone.active_list里的页放到zone.inactive_list.
- 先调用lru_add_drain()这个函数把lru_add_pvecs和lru_add_active_pvecs里的页放到
  zone.inactive和zone.active
- 再调用isolate_lru_pages()(下面有介绍)把zone.active_list里的nr_pages个页放到
  l_hold里
- pgscanned就是在isolate_lru_pages()里扫了个页.这个数是要加到zone.pages_scanned的,这个成员
  还有且只有shrink_cache()里被加,只有在balance_pgdat()里读取.
  #+BEGIN_EXAMPLE
	zone->pages_scanned += pgscanned;
  #+END_EXAMPLE 
- isolate_lru_pages()的返回值是移了多少个页,所以要从zone.nr_active里减去
  #+BEGIN_EXAMPLE
	zone->nr_active -= pgmoved;
  #+END_EXAMPLE 
- 下面是计算swap_tendency,小于100就个page cache里收页,否则就是从用户态里收页.
  #+BEGIN_EXAMPLE

	/*
	 * `distress' is a measure of how much trouble we're having reclaiming
	 * pages.  0 -> no problems.  100 -> great trouble.
	 */
	distress = 100 >> zone->prev_priority;

	/*
	 * The point of this algorithm is to decide when to start reclaiming
	 * mapped memory instead of just pagecache.  Work out how much memory
	 * is mapped.
	 */
	mapped_ratio = (sc->nr_mapped * 100) / total_memory;

	/*
	 * Now decide how much we really want to unmap some pages.  The mapped
	 * ratio is downgraded - just because there's a lot of mapped memory
	 * doesn't necessarily mean that page reclaim isn't succeeding.
	 *
	 * The distress ratio is important - we don't want to start going oom.
	 *
	 * A 100% value of vm_swappiness overrides this algorithm altogether.
	 */
	swap_tendency = mapped_ratio / 2 + distress + vm_swappiness;
  #+END_EXAMPLE 
- 接下来就是把l_hold里的页分配到l_inactive和l_active里了.
  #+BEGIN_EXAMPLE

	while (!list_empty(&l_hold)) {
		cond_resched();
		page = lru_to_page(&l_hold);
		list_del(&page->lru);
		if (page_mapped(page)) {
			if (!reclaim_mapped ||
			    (total_swap_pages == 0 && PageAnon(page)) ||
			    page_referenced(page, 0, sc->priority <= 0)) {
				list_add(&page->lru, &l_active);
				continue;
			}
		}
		list_add(&page->lru, &l_inactive);
	}
  #+END_EXAMPLE

  ulk:total_swap_pages variable contains the total number of nondefective page slots.

  在什么情况下会放到l_active里呢?首先必须是有pte页表项引用了这个页(page_mapped()大于0).为什
  么首先要有这个条件呢?难道会出现这个条件不成立时下面的条件会成立的情况吗?

  (1)不可以回收用户态的页,(2)total_swap_pages为0,且页是匿名页(3)页的page_referenced()返回1

  ulk:A page belonging to the User Mode address space of some processthat is, a page whose
  page->_mapcount is nonnegative.

  ulk:If the page is in the User Mode address space of some process (the _mapcount field
  in the page descriptor is greater than or equal to zero)但是为什么在try_to_unmap_one()这
  个函数里时,无论是用户态的页还是page cache的页都会调用page_remove_rmap()来减
  page._mapcount呢?还是说放到zone.active_list里的页若是用户态的那么page._mapcount就是不为0
  的,若是page cache的页就是为0的?

  第(2)种情况就是ulk:if the page is anonymous but no swap area is active.为什么
  total_swap_pages为0就表示没有swap area激活呢?难道是page cache页的时候就算在swap area没有
  被激活也可以回收吗?这个是没问题的,因为在page cache的内容是不会用来交换的.所以若swap area
  没有激活了,且页是匿名页,那么就要放到l_active里去.

  在反向映射中涉及的是匿名页和映射页(mapped page),这里的映射页是page cache吗?

  ulk:the page_referenced( )function applied to the page returns a positive value, which
  means that the page has been recently accessed.

  
  就是说一个页的page->_mapcount不为0就表示这个页是用户态下的页,这有点不对吧,但是可能有前提的,因为在这里出现的页要么是page cache的页要么是用户态的
  页,所以可以用page._count判断,但是这个判断不能确定页是匿名的页,所以还要有一个PageAnon的判
  断.所以是不是也可以确定


  为什么不可以回收用户态页时且有pte引用这个页就把它放回active呢?难道说因为有pte引用就说明
  这个不是page cahce页了吗?要判断是不是匿名页还是要用PageAnon()这个来判断的.
** static int isolate_lru_pages(int nr_to_scan, struct list_head *src, struct list_head *dst, int *scanned)
*** include/linux/mm.h:
- 这个函数就是把src链表里的页放到dst链表里,打算放nr_to_scan个,但实际放了scanned个.
- src 是lru链表来的
- page._count:_count A usage reference counter for the page. If it is set to -1, the
  corresponding page frame is free and can be assigned to any process or to the kernel
  itself. If it is set to a value greater than or equal to 0, the page frame is assigned
  to one or more processes or is used to store some kernel data structures.
- lru_to_page()就是通过链表里的第一个页结点来获取struct page*
- prefetchw_prev_lru_page()这个函数是提高执行效率的,因为这个函数会被经常调用.注释有说
  #+BEGIN_EXAMPLE
 * For pagecache intensive workloads, this function is the hottest
 * spot in the kernel (apart from copy_*_user functions).
  #+END_EXAMPLE
- 为什么page._count为0就表示这个页在其它地放正在回收呢?这句的意思是不是说还有其它的进程在
  调用了这个函数?但是zone.lru_lock不是已经都获取了吗?
  #+BEGIN_EXAMPLE
		if (get_page_testone(page)) {
			/*
			 * It is being freed elsewhere
			 */
			__put_page(page);
			SetPageLRU(page);
			list_add(&page->lru, src);
			continue;
  #+END_EXAMPLE 
- 接下来又是执行一个循环把l_inactive里的页移到zone.inactive_list里去,在移的过程中,发现
  lru_add_pvecs里有页满了就也把这些页放到zone.inactive_list里去,接着再移.
  #+BEGIN_EXAMPLE
	pagevec_init(&pvec, 1);
	pgmoved = 0;
	spin_lock_irq(&zone->lru_lock);
	while (!list_empty(&l_inactive)) {
		page = lru_to_page(&l_inactive);
		prefetchw_prev_lru_page(page, &l_inactive, flags);
		if (TestSetPageLRU(page))
			BUG();
		if (!TestClearPageActive(page))
			BUG();
		list_move(&page->lru, &zone->inactive_list);
		pgmoved++;
		if (!pagevec_add(&pvec, page)) {
			zone->nr_inactive += pgmoved;
			spin_unlock_irq(&zone->lru_lock);
			pgdeactivate += pgmoved;
			pgmoved = 0;
			if (buffer_heads_over_limit)
				pagevec_strip(&pvec);
			__pagevec_release(&pvec);
			spin_lock_irq(&zone->lru_lock);
		}
	}
	zone->nr_inactive += pgmoved;
	pgdeactivate += pgmoved;
  #+END_EXAMPLE
  可以看出,从l_inactive移到了zone.inactive_list里的页数加到了zone.nr_inactive去了.

  pgdeactivate也表示从l_inactive移到zone.inactive_list里的页数.但是为什么还要有pgmoved这个
  变量呢?因为在释放zone.lru_lock这个锁的时候要保证zone.nr_inactive表示的页数要与在
  zone.inactive_list里的页数一致.

  inactive_list链接的是最近有一段时间没有访问了的页.而active_list链连接的是最近刚访问过的页.
  
  每把一定的页数(pvec.nr)放到zone.inactive_list里,且发现buffer_head超限,就会调用
  pagevec_strip()来把放在pvec里的页的buffer_head给释放掉,不用保证页是有buffer_head的,因为在
  pagevec_strip()(下面有介绍)里有检查。要想在这释放页的buffer_head,就要有buffer_head超限的
  条件.
  
  调用__pagevec_release()来释放pvec的页.在__pagevec_release()里又会调用lru_add_drain()移页
  到zone.active/inactive,且会调用release_pages()来释放页,现在有一个疑问就是若没有出现
  buffer_head超限的情况下调用release_pages()释放有buffer_head的页是怎么做的?会不会因为要给
  页添加buffer_head链表而增加页的计数器呢?若有,那么在release_pages()就不会释放了.通过查看
  源码,可以得出删除页里的buffer_head时是会减计数
  的:pagevec_strip()->try_to_release_page()->try_to_free_buffers() -> drop_buffers() ->
  __clear_page_buffer() -> page_cache_release() -> put_page() -> put_page_testzero().
  所以这里的__pagevec_release()不一定会释放pvec里的所有页.现在又发现计数器的一个特点:对于
  释放buffer_head时,是先减计数器再释放,但是可能还有其它的东西在使用页而还不能释放页,那么就
  会出现虽然把buffer_head的那个计数值减了,但是不一定会把buffer_head给释放了,所以页的计数可
  能会出现小于-1的计数值.
  
  从这里可以看出页是一批一批释放的,而不是等收集到所有的页之后在这个函数的最后一下子全部释
  放掉.我觉得这可以尽快地释放出一些页好让其它程序使用.
  
  从这里也可以看出被释放的页也会在zone->inactive_list里,真的是这样子的吗?不敢相信.
- 第三个循环是把l_active里的页移到zone.active_list里去.与第二个循环基本相同,只是少了
  pgdeactivate这样的一个变量,少了释放buffer_head的语句.但是也调用了__pagevec_release(),所
  以我觉得调用__pagevec_release()是不可能释放掉页的,只是起到一个减计数的效果,但是这个计数
  为谁减的呢?难道是因为放到了zone.inactive/active而减的.好像知道为谁减的了,是为了在前面调
  用的isolate_lru_pages()而减的,因为在isolate_lru_pages()里加了一个计数.

** void pagevec_strip(struct pagevec *pvec)
*** mm/swap.c:
- 这个函数释放pvec里的页的里的buffer_head结构体.
- 调用try_to_release_page()来释放里的buffer_head,所以要先看这个页有没有buffer_head链表.
  #+BEGIN_EXAMPLE
  		if (PagePrivate(page) && !TestSetPageLocked(page)) {
			try_to_release_page(page, 0);
			unlock_page(page);
		}
  #+END_EXAMPLE
** static void free_more_memory(void)
*** fs/buffer.c:
- 这个函数先调用wakeup_bdflush()来把1024个页回写到磁盘.
- 这个函是用来释放所有结点的ZONE_NORMAL区的内存
- 调用try_to_free_pages()来释放内存.
** int try_to_free_pages(struct zone **zones, unsigned int gfp_mask, unsigned int order)
*** mm/vmscan.c:
- 这个函数是直接回收页的主要入口.
  #+BEGIN_EXAMPLE
  This is the main entry point to direct page reclaim.
  #+END_EXAMPLE
  
  调用者没有用__GFP_FS时若失败可以推出是因为zone可能充满了脏页或正在写的页.
  #+BEGIN_EXAMPLE
  If the caller is !__GFP_FS then the probability of a failure is reasonably
  high - the zone may be full of dirty or under-writeback pages, which this
  caller can't do much about.  We kick pdflush and take explicit naps in the
  hope that some of these pages can be written.  But if the allocating task
  holds filesystem locks which prevent writeout this might not work, and the
  allocation attempt will fail.
  #+END_EXAMPLE

- 在page_state里有一个调用该函数的次数计数allocstall
  #+BEGIN_EXAMPLE
	inc_page_state(allocstall);
  #+END_EXAMPLE
- 第一个循环就是把临时的优先级存到zone.temp_priority和收集所有区的active和inactive链表里的
  页
  #+BEGIN_EXAMPLE
		zone->temp_priority = DEF_PRIORITY;
		lru_pages += zone->nr_active + zone->nr_inactive;
  #+END_EXAMPLE
- 关于scan_control.nr_mapped的解释:Number of pages referenced in the User Mode address
  spaces.注意是用户态的空间,而代码是
  #+BEGIN_EXAMPLE
		sc.nr_mapped = read_page_state(nr_mapped);
  #+END_EXAMPLE
  所以可以看出其实对于page_state.nr_mapped的解释应该也是一样的.
  
  page_state.nr_mapped只有在page_add_file_rmap()和page_add_anon_rmap()才加,且是只有
  在该页已是至少有两个pte引用时才加.
  
  用户态下的页也有map成file的?
** static void shrink_caches(struct zone **zones, struct scan_control *sc)
*** include/linux/mmzone.h:
- 这个函数只被try_to_free_pages()调用.
- 这个函数主要是对传入的每一个zone调用shrink_zone(),在调用之前要做一些检查:
  (1)检查这个区是不是有页在
  #+BEGIN_EXAMPLE
		if (zone->present_pages == 0)
			continue;
  #+END_EXAMPLE
  (2)看这个cpu可以不可以访问这个区
  #+BEGIN_EXAMPLE
		if (!cpuset_zone_allowed(zone))
			continue;
  #+END_EXAMPLE
- 接下来就是处理zone.temp_prioprity和zone.prev_prioprity.
  #+BEGIN_EXAMPLE
		zone->temp_priority = sc->priority;
		if (zone->prev_priority > sc->priority)
			zone->prev_priority = sc->priority;
  #+END_EXAMPLE 
  关于zone.prev_prioprity的注释:
  #+BEGIN_EXAMPLE
	  prev_priority holds the scanning priority for this zone.  It is
	  defined as the scanning priority at which we achieved our reclaim
	  target at the previous try_to_free_pages() or balance_pgdat()
	  invokation.
	 
	  We use prev_priority as a measure of how much stress page reclaim is
	  under - it drives the swappiness decision: whether to unmap mapped
	  pages.
  #+END_EXAMPLE 
  就是(1)上次达到回收目标调用try_to_free_pages()或balance_pgdat(),(2)该明这个区的页有难回
  收(3)用来给交换确定是否unmap映射页.
  关于zone.temp_prioprity的注释:
  #+BEGIN_EXAMPLE
	  temp_priority is used to remember the scanning priority at which
	  this zone was successfully refilled to free_pages == pages_high.
  #+END_EXAMPLE 
  temp_priority是用来成功使得free_pages == pages_high时扫描用的prioprity.但是看代码看不出来
  啊.pages_high是页框回收的最高阀值.
  
  原来有present_pages这个成员,那么nr_active加nr_inactive不是这个区的总页数咯?
- 为什么没有可回收的页时且当前的优先级不等于DEF_PRIORITY时就不调用shrink_zone()呢?按道理是
  优先级等于DEF_PRIOPRITY时更难收页的啊.
  #+BEGIN_EXAMPLE
		if (zone->all_unreclaimable && sc->priority != DEF_PRIORITY)
			continue;	/* Let kswapd poll it */
  #+END_EXAMPLE 
** static void shrink_zone(struct zone *zone, struct scan_control *sc)
*** mm/vmscan.c:
- 这个函数其实对于active的页和inactive的页是分开处理的.而前面大部分处理是相同的(1)把页数放
  到zone.nr_scan_active/nr_scan_inactive(2)初始化nr_active/nr_inactive(3)给
  scan_control.nr_to_scan赋值.不同的是active的页是调用refill_inactive_zone()把用active填充
  inactive而inactive调用的是shrink_cache().
- 关于zone.nr_scan_active/nr_scan_inactive,ulk:The nr_scan_active and nr_scan_inactive
  fields of the zone descriptor play a special role here. To be efficient, the function
  works on batches of 32 pages. Thus, if the function is running at a low privilege level
  (high value of sc->priority) and one of the LRU lists does not contain enough pages, the
  function skips the scanning on that list. However, the number of active or inactive
  pages thus skipped is recorded in nr_scan_active or nr_scan_inactive, so that the
  skipped pages will be considered in the next invocation of the function.
  
  代码是
  #+BEGIN_EXAMPLE

	/*
	 * Add one to `nr_to_scan' just to make sure that the kernel will
	 * slowly sift through the active list.
	 */
	zone->nr_scan_active += (zone->nr_active >> sc->priority) + 1;
	nr_active = zone->nr_scan_active;
	if (nr_active >= sc->swap_cluster_max)
		zone->nr_scan_active = 0;
	else
		nr_active = 0;

	zone->nr_scan_inactive += (zone->nr_inactive >> sc->priority) + 1;
	nr_inactive = zone->nr_scan_inactive;
	if (nr_inactive >= sc->swap_cluster_max)
		zone->nr_scan_inactive = 0;
	else
		nr_inactive = 0;
  #+END_EXAMPLE
  
  所以这个代码的意思就是先按照优先级计算出要从active/inactive链表里回收多少页,若页数大于
  32(scan_control.swap_cluster_max(就是32,就是SWAP_CLUSTER_MAX),注释有说明),那么就把页数保
  存到nr_active/nr_inactive里把zone.nr_scan_active/nr_scan_inactive清0,否则就把页数保存到
  nr_active/nr_inactive里把清0,这样就达到了下次再进入这个函数时就会把上一次没有扫描而保存
  在zone.nr_scan_active/nr_scan_inactive里的页数在这次扫描里处理(因为计算出页数之后是自加
  到zone.nr_scan_active/nr_scan_inactive里的.).
- 每调用这个函数一次就打算回收32页
  #+BEGIN_EXAMPLE
	sc->nr_to_reclaim = sc->swap_cluster_max;
  #+END_EXAMPLE
- 若nr_active或nr_inactive没有为0,那就在事干了.对于nr_active的页说调用
  refill_inactive_zone(),每调用一次refill_inactive_zone()最多处理32页,若nr_active没有被处
  理完就再循环一次.
  
  而nr_inactive的处理就是调用shrink_cache().
** static void shrink_cache(struct zone *zone, struct scan_control *sc)
*** mm/vmscan.c:
- max_scan最大是32,因为scan.nr_to_scan在shrink_zone里计算时最大是32,结合shrink_zone()来看,
  在shrink_zone()里的nr_inactive就是指在shrink_cache()里要扫描多少页.在shrink_cache()里最
  外层的while()循环是以所扫描的页数为条件的,而不管有多少页被释放(scan.nr_to_reclaim),而在
  shrink_zone()里初始化scan.nr_to_reclaim是用32的,而nr_inactive至少是32,所以到最后
  scan.nr_to_reclaim可能在shrink_cache()被减成负数.
- 函数先用isolate_lru_pages()把打算32个页实际是返回值的数的页从zone.inactive_list移到
  page_lists
  #+BEGIN_EXAMPLE
		nr_taken = isolate_lru_pages(sc->swap_cluster_max,
					     &zone->inactive_list,
					     &page_list, &nr_scan);
  #+END_EXAMPLE 
  因为从zone.inactive_list里移出了页,所以在解锁之前要从zone.nr_inactive里减相应的页
  #+BEGIN_EXAMPLE
		zone->nr_inactive -= nr_taken;
  #+END_EXAMPLE 
  关于zone.pages_scanned的注释是:since last reclaim.所以这个成员会的释放页时清零.清零是在
  free_pages_bulk()里,shrink_list()->pvecs_release()....->free_pages_bulk().

  zone.nr_inactive还有一个地方被加就是在refill_inactive_zone()里,同时也是在调用
  isolate_lru_pages(),但是在refill_inactive_zone()里调用的isolate_lru_pages()是从
  zone.active_list里移出页的,而不是zone.inactive_list.而且用isolate_lru_pages()移出的页可
  能在之后的处理中又被移回去,但是没有把zone.pages_scanned给减回去.
- 接下来就是调用shrink_list()来收调用isolate_lru_pages()获取的页了.
** static int shrink_list(struct list_head *page_list, struct scan_control *sc)
*** mm/vmscan.c:
- 每回收一个页就调度一次
  #+BEGIN_EXAMPLE

	pagevec_init(&freed_pvec, 1);
	while (!list_empty(page_list)) {
		struct address_space *mapping;
		struct page *page;
		int may_enter_fs;
		int referenced;

		cond_resched();

		page = lru_to_page(page_list);
		list_del(&page->lru);
  #+END_EXAMPLE 
- 若发现若是被锁住的话,那么就把它放回原来的链表.
  #+BEGIN_EXAMPLE

		if (TestSetPageLocked(page))
			goto keep;
  #+END_EXAMPLE
  #+BEGIN_EXAMPLE
keep:
		list_add(&page->lru, &ret_pages);
  #+END_EXAMPLE
- 关于scan_control.nr_scanned,它是在shrink_caches()里初始化为0,然后通过
  shrink_zone()->shrink_cache()->shrink_list()过程中都没在动过.
  所以scan_control.nr_scanned只有在这个函数里加
  #+BEGIN_EXAMPLE
		sc->nr_scanned++;
		/* Double the slab pressure for mapped and swapcache pages */
		if (page_mapped(page) || PageSwapCache(page))
			sc->nr_scanned++;
  #+END_EXAMPLE
  只要这个页不被锁,那么就必定会增加nr_scanned,但是为什么在有pte引用这个页或PG_swapcache设置
  时就会再加一次呢?那么对于scan_control.nr_scanned的解释是不是有点不对
  呢,scan_control.nr_scanned:Number of inactive pages scanned in the current iteration?
- 如果发现这个页正在被回写(PG_writeback),那么就清在循环一开始就设置的PG_locked
  #+BEGIN_EXAMPLE
		if (PageWriteback(page))
			goto keep_locked;
  #+END_EXAMPLE 
  #+BEGIN_EXAMPLE
keep_locked:
		unlock_page(page);
  #+END_EXAMPLE
  unlock_page()就是清PG_locked
- 若这个页有被引用且这个页正在使用(page_mapping_inuse()(下面有介绍))就会设置页的PG_active并把页返回去
  #+BEGIN_EXAMPLE

		referenced = page_referenced(page, 1, sc->priority <= 0);
		/* In active use or really unfreeable?  Activate it. */
		if (referenced && page_mapping_inuse(page))
			goto activate_locked;
  #+END_EXAMPLE
  #+BEGIN_EXAMPLE
activate_locked:
		SetPageActive(page);
		pgactivate++;
keep_locked:
		unlock_page(page);
keep:
		list_add(&page->lru, &ret_pages);
  #+END_EXAMPLE
  若这个页只是被referenced了但是没有正在使用,那会怎么样呢?若被referenced且页是脏的,那么也把
  它放到返回去,但是没有设置PG_active,因为要设置PG_active的时候要这个页被正在使用才可以.
- 在什么情况下回收页时会引起磁盘操作呢?(1)若是因为分配页时不够页而引起的回收页操作时指定了
  分配页标志有__GFP_FS时是允许磁盘操作的(2)若是该页是swap cache里的页且分配标志指定
  了__GFP_IO也允许磁盘操作.

  但是若这个页是映射文件的页且脏时可不可以引起磁盘操作呢?
- 若页有被pte引用且页是swap cache的页或是文件映射页,那么就调用try_to_unmap()来取消映射.
  若不能取消映射就不能释放这个页,若是返回SWAP_FAIL就放到PG_active.

  为什么不考虑匿名页来调用try_to_unmap()呢?

  关于匿名区共享的情况:ulk:Anonymous pages are often shared among several processes. The
  most common case occurs when forking a new process: as explained in the section "Copy On
  Write" in Chapter 9, all page frames owned by the parentincluding the anonymous pagesare
  assigned also to the child. Another (quite unusual) case occurs when a process creates a
  memory region specifying both the MAP_ANONYMOUS and MAP_SHARED flag: the pages of such a
  region will be shared among the future descendants of the process.那进程之间的共享内存(shmem)是
  不是也是一种情况呢?按照上面的ulk的说法,回收共享的匿名页是不是效率不高,因为回收了可能很快
  又被访问了?
- 为什么在这个页是脏时且有被referenced或不能操作磁盘或在laptop模式但scan_control不允许在
  laptop下回写时就不回收这个页呢?
  #+BEGIN_EXAMPLE
		if (PageDirty(page)) {
			if (referenced)
				goto keep_locked;
			if (!may_enter_fs)
				goto keep_locked;
			if (laptop_mode && !sc->may_writepage)
				goto keep_locked;
  #+END_EXAMPLE 
- 如果这个页是dirty的且可允许回写,那么就调用pageout()回写.
  #+BEGIN_EXAMPLE
  #+END_EXAMPLE 
- 若收到的是PAGE_SUCCESS,那么页正在被回写,那么就不能释放这个页,要返回去.
  #+BEGIN_EXAMPLE
			case PAGE_SUCCESS:
				if (PageWriteback(page) || PageDirty(page))
					goto keep;
  #+END_EXAMPLE
  不明白为什么还要检测PG_dirty,从pageout()里看,能返回PAGE_SUCCESS的情况之前都会把PG_dirty给
  清的.除非writepage()会又设置.通过看__block_write_full_page()的代码可以确定是有可能的,因为
  若发现有页里有一buffer_head是锁住的,就有可以调用redirty_page_for_writepage()来设置
  PG_dirty.
- 在writepage()是用ramdisk_writepage()实现的时候,是把PG_dirty设置的.所以返回PAGE_SUCCESS时
  也有可能还设置PG_dirty.但是这就不符合注释所说的了
  #+BEGIN_EXAMPLE
			case PAGE_SUCCESS:
				if (PageWriteback(page) || PageDirty(page))
					goto keep;
				/*
				 * A synchronous write - probably a ramdisk.  Go
				 * ahead and try to reclaim the page.
				 */
				if (TestSetPageLocked(page))
					goto keep;
				if (PageDirty(page) || PageWriteback(page))
					goto keep_locked;
				mapping = page_mapping(page);
  #+END_EXAMPLE
  能执行到TestSetPageLocked()这里不应该是ramdisk.但有一点是可以理解的,就是若这个页没设
  PG_writeback(说明是同步写)且没有dirty,且这个页没有被锁,那么这个页是可以被回收的了,不用再
  跳到keep把页返回给shrink_cache().
- 无论进不进入下面的判断,要想往下执行来释放这个页,那么这个页就一定是清PG_dirty的
  #+BEGIN_EXAMPLE
		if (PageDirty(page)) {
  #+END_EXAMPLE
  但是下面有注释说还是有可能设置PG_dirty的.
  #+BEGIN_EXAMPLE
		 * We do this even if the page is PageDirty().
		 * try_to_release_page() does not perform I/O, but it is
		 * possible for a page to have PageDirty set, but it is actually
		 * clean (all its buffers are clean).  This happens if the
		 * buffers were written out directly, with submit_bh(). ext3
		 * will do this, as well as the blockdev mapping. 
		 * try_to_release_page() will discover that cleanness and will
		 * drop the buffers and mark the page clean - it can be freed.
		 *
  #+END_EXAMPLE
  又不对了,注释好像是说执行到这里时PG_dirty是清掉的.在ext3和blockdev mapping里就会直接把脏
  页给回写到disk,但是没有把buffer_head给释放,所以下面要想回收这个页时就要先释放这个页的
  buffer_head
  #+BEGIN_EXAMPLE
		if (PagePrivate(page)) {
			if (!try_to_release_page(page, sc->gfp_mask))
				goto activate_locked;
			if (!mapping && page_count(page) == 1)
				goto lfree_it;
		}
  #+END_EXAMPLE 
  如果返回try_to_release_page()返回0就表示没有成功释放buffer_head,那么就把页放回active链表.
- 为什么下面还要检测页是不是脏的呢?之前不是已经确定不为脏了吗?难道设置PG_locked之后不能保
  证PG_dirty不能修改吗?
  #+BEGIN_EXAMPLE
		write_lock_irq(&mapping->tree_lock);

		/*
		 * The non-racy check for busy page.  It is critical to check
		 * PageDirty _after_ making sure that the page is freeable and
		 * not in use by anybody. 	(pagecache + us == 2)
		 */
		if (page_count(page) != 2 || PageDirty(page)) {
			write_unlock_irq(&mapping->tree_lock);
			goto keep_locked;
		}
  #+END_EXAMPLE
  注释说明了page._count为2的情况是page cache的一个计数和回收这个页时加的计数.

  要注意到了这里这个页要么是page cache里的,要么是swap cache的,因为只有这两种情况下mapping
  才不会为空.mapping为空是不会执行到这里的.
- 下面就该把页放到freed_pvec这个链表里以在最后释放了,这时要分两种情况

  (1)swap cache的页
  #+BEGIN_EXAMPLE

#ifdef CONFIG_SWAP
		if (PageSwapCache(page)) {
			swp_entry_t swap = { .val = page->private };
			__delete_from_swap_cache(page);
			write_unlock_irq(&mapping->tree_lock);
			swap_free(swap);
			__put_page(page);	/* The pagecache ref */
			goto free_it;
		}
#endif /* CONFIG_SWAP */
  #+END_EXAMPLE
  (2)page cache的页
  #+BEGIN_EXAMPLE
		__remove_from_page_cache(page);
		write_unlock_irq(&mapping->tree_lock);
		__put_page(page);

free_it:
		unlock_page(page);
		reclaimed++;
		if (!pagevec_add(&freed_pvec, page))
			__pagevec_release_nonlru(&freed_pvec);
		continue;
  #+END_EXAMPLE

  那么匿名页最后是怎么给释放的呢?这种页应该是先放到swap cache里.
- ulk里17-5那个图可以记住.
- 通过看17-5图知道了匿名页是怎么被释放的了,若是匿名页,那么就调用add_to_swap()把这个页加到
  swap cache里,那么匿名页就变成了swap cache的页了,那么这个被添加到swap cache的页就有可能回
  收了.
- ulk:If the page is in the User Mode address space of some process (the _mapcount field
  in the page descriptor is greater than or equal to zero), shrink_list( ) invokes the
  try_to_unmap( ) function to locate all User Mode Page Table entries that refer to the
  page frame.可以看出_mapcount增加都是与用户态的进程有关的.
** static inline int page_mapping_inuse(struct page *page)
*** mm/vmscan.c:
- 调用时不加锁所以这个函数的返回值不稳定
  #+BEGIN_EXAMPLE
  /\* Called without lock on whether page is mapped, so answer is unstable */
  #+END_EXAMPLE 
- 注意page_mapped()返回的是page._mapcount是不是不为负.page_mapping()返回是的swapper_space
  或页被映射文件时的page.mapping,所以在page.mapping为NULL或页是匿名页时就会返回NULL.
- 若这个页真的是映射到了一个匿名区,那么就会有pte引用它,所以page_mapped()返回真,且若
  pageSwapCache()也返回真,那么还进入page_mapping()函数,那么只有一种page_mapping()的返回值
  为NULL,就是在page.mapping为NULL.
- 若page_mapping()返回的不是NULL,那么这情况下这个页的状态是没有被pte映射,也不是swap cache,
  因为page_mapping()的返回值不是NULL,所以可能推出页不是匿名页且page.mapping不为NULL,所以这
  个页要么是文件映射的页要么是swap cache的页,又因为调用page_mapping()已确定页不是swap
  cache的页,所以最后的推断是这个页是文件映射的页.注释也是这样说的
  #+BEGIN_EXAMPLE
	mapping = page_mapping(page);
	if (!mapping)
		return 0;

	/* File is mmap'd by somebody? */
	return mapping_mapped(mapping);
  #+END_EXAMPLE
  但是若页映射为文件,也不一定会返回1,这个还要看mapping_mapped()的返回值.在mapping_mapped()
  这个函数里主要是判断address_space.i_mmap和address_space.i_mmap_nonlinear是否为空,只要一
  个不为空就会返回1.那是不是说page.mapping指向了address_space但是address_space.i_mmap和
  address_space.i_mmap_nonlinear可以为空.address_space.i_mmap是反向查找的优先树,若是非线性
  映射的话在i_mmap里可能找不到这个页,所以要用i_mmap_nonlinear这个链表.所以若一个页是文件映
  射,那么就可以用address_space.i_mmap和address_space.i_mmap_nonlinear来判断这个映射页是不
  是正在使用.

  还可以得到一个信息:就是该页在没有被任何pte引用的情况下但page.mapping.i_mmap或
  page.mapping.i_mmap_nonlinear可以不为空.

  以上的说法有错误,因为page.mapping.i_mmap这个优先搜索树里不一定会包含这个页.且
  i_mmap_nonlinear也不一定包含这个页.那么是不是若这个页在i_mmap或i_mmap_nonlinear里就一定
  会有pte引用这个页是吗?为什么要有一个这么粗慥的判断呢?为了效率,因为在优先搜索树里找出这个
  页或在i_mmap_nonlinear找出一个页是费时的.真的是这样子的吗?

  所以若一个页没有被pte引用且这个页是匿名页或page.mapping为NULL那么就一定是unused的.还有一
  个不一定的情况就是没有pte引用且还是映射页,这时要判断是不是unused就要看i_mmap和
  i_mmap_nonlinear了.
** static pageout_t pageout(struct page *page, struct address_space *mapping)
*** mm/vmscan.c:
- ulk:The pageout( ) function is invoked by shrink_list( ) when a dirty page must be
  written to disk.这个函数只有shrink_list()调用.
- 只有在回写时不阻塞才会回写,注释:
  #+BEGIN_EXAMPLE
	 * If the page is dirty, only perform writeback if that write
	 * will be non-blocking.
  #+END_EXAMPLE
  但是若当前进程若正在generic_file_write()里against this page's queue时,就算会被阻塞也可以
  回写.
  #+BEGIN_EXAMPLE
	 * If this process is currently in generic_file_write() against
	 * this page's queue, we can perform writeback even if that
	 * will block.
  #+END_EXAMPLE 

  还有一种阻塞时可以回写的就是页是swap cahce页,注释有说:
  #+BEGIN_EXAMPLE
	 * If the page is swapcache, write it back even if that would
	 * block, for some throttling. This happens by accident, because
	 * swap_backing_dev_info is bust: it doesn't reflect the
	 * congestion state of the swapdevs.  Easy to fix, if needed.
	 * See swapfile.c:page_queue_congested().
  #+END_EXAMPLE 
- is_page_cache_freeable()在ulvmm的解释是:is page cache freeable() will return true if it
  is not mapped by any process and has no buffers.但从代码上还是看不出来
  #+BEGIN_EXAMPLE
	return page_count(page) - !!PagePrivate(page) == 2;
  #+END_EXAMPLE
  是不是说没有进程引用但是有buffer时因为PagePrivate()不是0所以page_count() - 1 == 1?

  在调用is_page_cache_freeable()时是不是在确保页是文件映射页呢?

  要看有没有进程引用不是通过page._mapcount的?因为_mapcount指的是有pte指向这个页,进程会访问
  这个页的内容,而_count是指进程对这个页的struct page结构体有多少个引用.
- 若映射页有进程引用或有buffer_head在,就不能回写
  #+BEGIN_EXAMPLE
	if (!is_page_cache_freeable(page))
		return PAGE_KEEP;
  #+END_EXAMPLE 
- 通过在shrink_list()分析的page_mapping()这个函数,可以确定若mapping不为NULL,那么就可以确定
  page要么是在swap cache里的,要么是文件映射页.
- 因为这个页是要回写页的内容成磁盘,所以要保证mapping不为NULL的,所以
  #+BEGIN_EXAMPLE
	if (!mapping) {
		/*
		 * Some data journaling orphaned pages can have
		 * page->mapping == NULL while being dirty with clean buffers.
		 */
		if (PagePrivate(page)) {
			if (try_to_free_buffers(page)) {
				ClearPageDirty(page);
				printk("%s: orphaned page\n", __FUNCTION__);
				return PAGE_CLEAN;
			}
		}
		return PAGE_KEEP;
	}
  #+END_EXAMPLE
  若mapping为NULL就不会回写,要么是PAGE_CLEAN表示这个是Some data journaling orphaned pages,
  要么返回PAGE_KEEP.

  返回PAGE_KEEP给shrink_list()后就会跳到keep_locked.
- 若mapping.a_ops.writepage为NULL,那么给shrink_list()返回PAGE_ACTIVATE让它跳到
  activate_locked来设置页的PG_actived.

  在什么情况下会出现writepage为NULL的情况呢?
- 在什么情况下可以从入队列呢?may_write_to_queue()
  (1) 当前进程是kswapd,不是再看下面
  (2) 当前进程是pdflush,不是再看下面,注释说这不可能,为什么呢?
  (3) 队列不拥挤,不行再看下面.为什么队列拥挤就不行呢?所以当进程是kswapd或pdflush时,就算拥
  挤也可以插入,所以队列拥挤不代表队列已满.
  (4) 为什么bdi是当前进程的bdi就可以呢?因为是当前进程的bdi阻塞了,所以再添加一个也不要紧.
  其它的情况都不可以.所以对于一个普通的进程只有在队列不拥挤时才可以回写.

  不可以时要返回PAGE_KEEP给shrink_list()
- 下面有关于clear_page_dirty_for_io()
- 若PG_dirty设置了,那么就可以调用address_space.a_ops.writepage()了.调用之前要先设置
  PG_reclaim,PG_reclaim只有在这个函数设置和清除.PG_reclaim:ulk:The page has been marked to
  be written to disk in order to reclaim memory.
- 调用writepage()有错就要调用handle_write_error()来设置page.flags的AS_ENOSPC或AS_EIO.这个
  东西是不会在writepage()里设置的.

  还有一点要注意的就是调用这个函数是要设置PG_locked的,又因为在调用pageout()之前已经在
  shrink_list()设置PG_locked了,但是到这里又设置PG_locked时不会死锁呢?因为在writepage()函数
  里已释清了PG_locked了,writepage()一般由block_write_full_page()实
  现,block_write_full_page()又调用__block_write_full_page(),__block_write_full_page()在调
  用完submit_bh()之后就是解锁了.真是一个整体啊.
- 关于PG_reclaim,从代码上看调用完writepage()之后不一定会清掉PG_reclaim的,不清掉会有什么影
  响吗?writepage()是同步写吗?
- 调用完writepage()之后若没有返回WRITEPAGE_ACTIVATE来要求把页放回active_list都会返回
  PAGE_SUCCESS表示写成功了,就算有error出现,为什么要WRITEPAGE_ACTIVATE那么特别呢?

  WRITEPAGE_ACTIVATE这个值只有在shmem_writepage()返回,而shmem_writepage()有什么用呢?ulk:The
  shmem_writepage( ) function, which implements the writepage method for IPC shared memory
  regions' pages, essentially allocates a new page slot in a swap area, and moves the page
  from the page cache to the swap cache.所以这个函数的功能就是把page cache的页放到swap
  cache里.从注释上也可以看出来
  #+BEGIN_EXAMPLE
/*
 * ->writepage() return values (make these much larger than a pagesize, in
 * case some fs is returning number-of-bytes-written from writepage)
 */
#define WRITEPAGE_ACTIVATE	0x80000	/* IO was not started: activate page */
  #+END_EXAMPLE
  发现还有一个地方设置了WRITEPAGE_ACTIVATE,就是randisk_writepage(),这个回写也是不用磁盘操
  作的.
- 返回PAGE_CLEAN就说明这个页的PG_dirty已清掉,所以也没有必要回写.但是不是在shrink_list()里
  设置了PG_locked了且发现设了PG_dirty了才调用pageout()的吗?

  返回PAGE_SUCCESS就说明这个页也清了PG_dirty,且是也回写disk了.

  返回PAGE_KEEP说明这个页是回写 _失败_ 的

  返回PAGE_ACTIVATE说明这个页不是回写 _失败_ 的,是因为writepage()的问题,要么是为空,要么是
  shmem_writepage()

  shrink_list()收到pageout()的返回值之后就分别对上面的4个返回值做处理.
** int clear_page_dirty_for_io(struct page *page)
*** mm/page-writeback.c:
- 这个函数清除页的PG_dirty的,还返回没清之前的状态,说白了就是TestClearPageDirty()的功能,但是
  若页是文件映射页且PG_dirty也设置了且bdi有计数page_state.nr_dirty的能力,那么就减小
  page_state.nr_dirty.
- 这个函数只是清PG_dirty而已,没有清掉radix树的PAGECACHE_TAG_DIRTY,为什么可以这样呢?
  #+BEGIN_EXAMPLE
 * This is for preparing to put the page under writeout.  We leave the page
 * tagged as dirty in the radix tree so that a concurrent write-for-sync
 * can discover it via a PAGECACHE_TAG_DIRTY walk.  The ->writepage
 * implementation will run either set_page_writeback() or set_page_dirty(),
 * at which stage we bring the page's dirty flag and radix-tree dirty tag
 * back into sync.
  #+END_EXAMPLE
  注释说了这个函数是为了回写页而调用的(回写单个页是一定调用address.a_ops.writepage的吗?),
  不清PAGECACHE_TAG_DIRTY是为了可能有进程来扫描radix树找PAGECACHE_TAG_DIRTY的页,所以不能清.但
  是等调用完writepage之后PAGECACHE_TAG_DIRTY和PG_dirty就可以又同步了(调用
  set_page_writeback()或set_page_dirty()).

  为什么PAGECACHE_TAG_DIRTY和PG_dirty不同步也可以出现呢?因为设置了PG_locked,所以这个页的
  struct page不能并发访问.
  #+BEGIN_EXAMPLE
 * This incoherency between the page's dirty flag and radix-tree tag is
 * unfortunate, but it only exists while the page is locked.
  #+END_EXAMPLE 
** not function
*** 17.3.3
- When the PFRA tries to reclaim page frames, it should also check whether some of these
  disk caches can be shrunk.
  
  Every disk cache that is considered by the PFRA must have a shrinker function registered
  at initialization time. The shrinker function expects two parameters: the target number
  of page frames to be reclaimed, and a set of GFP allocation flags; the function does
  what is required to reclaim the pages from the disk cache, then it returns the number of
  reclaimable pages remaining in the cache.
  
  The set_shrinker( ) function registers a shrinker function with the PFRA. This function
  allocates a descriptor of type shrinker , stores the address of the shrinker function in
  the descriptor, and then inserts the descriptor in a global list rooted at the
  shrinker_list global variable. The set_shrinker( ) function also initializes the seeks
  field of the shrinker descriptor: informally, it is a parameter that indicates how much
  it costs to re-create one item of the cache once it is removed.

  In Linux 2.6.11 there are few disk caches registered with the PFRA: besides the dentry
  cache and the inode   cache, only the disk quota layer, the filesystem meta information
  block cache (mainly   used for filesystems' extended attributes), and the XFS journaling
  filesystem register   shrinker functions .

  The PFRA's function that reclaims pages from the shrinkable disk caches is called
  shrink_slab( ) (the name is a bit misleading, because the function has   little to do
  with the slab allocator caches).  This function is invoked by   TRy_to_free_pages( ), as
  explained in the earlier section "Low On Memory Reclaiming,"   and by balance_pgdat( ),
  which is described in the later section "Periodic Reclaiming." 
- 以上就说明了shrink_slab()的由来,就是回收disk cache的(dentry cache, inode cache等),而
  shrink_caches()是回收LRU里的页的.

** static int shrink_slab(unsigned long scanned, unsigned int gfp_mask,unsigned long lru_pages)
*** mm/vmscan.c:
- scanned参数是调用者的scan_control.nr_scanned:Number of inactive pages scanned in the
  current iteration.
- 这里用到do_div()处理64位的数除以32位的数.
  http://blog.csdn.net/lanmanck/article/details/7622861
- 这个函数的原理是这样的,扫描注册到shrinker_list链表里的所有struct shrinker结构体,扫描到的
  每一个struct shrinker都要调用shrinker.shrinker()两次,第一次调用是为了得到可以从该
  shrinker对应的某个disk cache里回收多少个页,并根据这个数计算出shrinker.nr表示最终可以回收
  多少个页;第二次调用就是回收之前算出的最终应该收的页数.为什么shrinker.shrinker()可以有两
  种功能呢?因为使用了第一个参数来表示,为0就返回多少个页可以回收.
- 每第二次调用shrinker.shrinker()就调度一次.
** static int shrink_dcache_memory(int nr, unsigned int gfp_mask)
*** fs/dcache.c:
- ulk:The shrink_dcache_memory( ) function is the shrinker function for the dentry cache;
- 主要是调用prune_dcache()来释放cache
- 若设置了__GFP_FS,那么就不能回收dcache页.为什么呢?注释说这会引起死锁
  #+BEGIN_EXAMPLE
 * We need to avoid reentering the filesystem if the caller is performing a
 * GFP_NOFS allocation attempt.  One example deadlock is:
 *
 * ext2_new_block->getblk->GFP->shrink_dcache_memory->prune_dcache->
 * prune_one_dentry->dput->dentry_iput->iput->inode->i_sb->s_op->put_inode->
 * ext2_discard_prealloc->ext2_free_blocks->lock_super->DEADLOCK.
  #+END_EXAMPLE
** static int __init kswapd_init(void)
*** mm/vmscan.c:
- 这个函数给每一个内存结点注册一个执行相同的kswapd()的内核线程来回收内存.
- 每个结点的kswapd成员都保存结点对应的kswapd线程.
** static int kswapd(void *p)
*** mm/vmscan.c:
- 为什么要用线程来回收内存呢?ulk:Some memory allocation requests are performed by
  interrupt and exception handlers, which cannot block the current process waiting for a
  page frame to be freed; moreover, some memory allocation requests are done by kernel
  control paths that have already acquired exclusive access to critical resources and
  that, therefore, cannot activate I/O data transfers. In the infrequent case in which all
  memory allocation requests are done by such sorts of kernel control paths, the kernel is
  never able to free memory. The kswapd kernel threads also have a beneficial effect on
  system performance by keeping memory free in what would otherwise be idle time for the
  machine; processes can thus get their pages much faster.

  总结就两点:1,一些情况不允许调度和休眠, 2,可以在idle时回收提高效率.
- __alloc_pages()在发现内存少的时候会唤醒kswapd线程.用zone.pages_low和zone.pages_min来判断.
- kswap线程的reclaim_state要把reclaimed_slab给清掉.
- task.flags要设置PF_KSWAP说明这个线程是kswapd线程,设置PF_MEMALLOC说明这个线程可以使用所有
  的保留的内存.
- pg_dat_t.kswapd_max_order:kswapd线程要释放内存块大小取对数后的值。
- kswapd线程只有__alloc_pages()会唤醒.
- 关于kswapd_max_order,__alloc_pages()调用wakeup_kswapd()来唤醒kswapd线程,wakeup_kswapd()的
  第二参数是order,第二个参数order在wakeup_kswapd()里在order到于kswapd_max_order的时候赋给
  kswapd_max_order.kswapd()执行后就判断上一次执行这个函数时的kswapd_max_order是否有比现在
  的kswapd_max_order大,若大的话就不用休眠,而是直接调用balance_pgdat()来回收内存,接着再休眠.为
  什么要这样做呢?
** static int balance_pgdat(pg_data_t *pgdat, int nr_pages, int order)
*** mm/vmscan.c:
- 这个函数的最外层循环是遍历12个优先级的.
- 以下的这段代码的作用是找出所在位置最高的且空闲页低于阀值的zone(这是在参数nr_pages为0的时
  候,若是nr_pages不为0就不用找这样的zone了,而是就从最后一个zone开始.)
  #+BEGIN_EXAMPLE
		if (nr_pages == 0) {
			/*
			 * Scan in the highmem->dma direction for the highest
			 * zone which needs scanning
			 */
			for (i = pgdat->nr_zones - 1; i >= 0; i--) {
				struct zone *zone = pgdat->node_zones + i;

				if (zone->present_pages == 0)
					continue;

				if (zone->all_unreclaimable &&
						priority != DEF_PRIORITY)
					continue;

				if (!zone_watermark_ok(zone, order,
						zone->pages_high, 0, 0, 0)) {
					end_zone = i;
					goto scan;
				}
			}
			goto out;
		} else {
			end_zone = pgdat->nr_zones - 1;
		}
  #+END_EXAMPLE
  若zone.present_pages是0表示这个zone没有页,那么就不可能有页被回收了.
- 为什么要从最高位置的内存区开始扫描内存区呢?因为分配内存的时候是从高位置的内存区往低位置
  的内存区分配,所以若先扫描低位置的zone时,且同时又有很多地方在分配内存,且所有的zone都缺内
  存,那么就会把低位置的zone刚释放的内存给分配出去了,而紧接着释放的高端内存反而没有被分配,
  而分配的原则是从高端内存开始.有注释说:
  #+BEGIN_EXAMPLE
		/*
		 * Now scan the zone in the dma->highmem direction, stopping
		 * at the last zone which needs scanning.
		 *
		 * We do this because the page allocator works in the opposite
		 * direction.  This prevents the page allocator from allocating
		 * pages behind kswapd's direction of progress, which would
		 * cause too much scanning of the lower zones.
		 */
  #+END_EXAMPLE 
- nr_pages只有在kswapd()调用时才是0,
- balance_pgdat()只有两个函数调用,一个是kswapd()一个是
  shrink_all_memory(),shrink_all_memory()调用时的nr_pages不是0,shrink_all_memory()只有被
  free_some_memory()调用,free_some_memory()只有被prepare_processes()调
  用,prepare_processes()被kernel/power/disk.c里的pm_suspend_disk()和software_resume()调用,
  所以可以看出nr_pages不为0就在系统体眠到disk时调用用来释放页的.在prepare_processes()里调
  用的free_some_memory()时的注释是
  #+BEGIN_EXAMPLE
	/* Free memory before shutting down devices. */
  #+END_EXAMPLE 
- scan_control.swap_cluster_max会根据nr_pages来赋值.接着就是调用shrink_zone()来回收页
- 关于struct reclaim_state和reclaim_state.reclaimed_slab,ulk:If the current process is
  performing memory reclaiming (current->reclaim_state field not NULL), the reclaimed_slab
  field of the reclaim_state structure is properly increased, so that the pages just freed
  can be accounted for by the page frame reclaiming algorithm.

  reclaim_state.reclaim_slab只有在kmem_freepages()里增加,kmem_freepages()是用来释改slab的.
- 设置reclaim_state.reclaim_slab之后再调用shrink_slab().要把reclaim_state.reclaimed_slab的
  页加到scan_control.nr_reclaimed
  #+BEGIN_EXAMPLE
			reclaim_state->reclaimed_slab = 0;
			shrink_slab(sc.nr_scanned, GFP_KERNEL, lru_pages);
			sc.nr_reclaimed += reclaim_state->reclaimed_slab;
  #+END_EXAMPLE
- 关于zone.all_unreclaimable. 只在free_pages_bulk()里清零,只在balance_pgdat()里设置1.
- 在回收页的过程中有可能会设置zone.all_unreclaimable
  #+BEGIN_EXAMPLE
			if (zone->all_unreclaimable && priority != DEF_PRIORITY)
				continue;

			if (nr_pages == 0) {	/* Not software suspend */
				if (!zone_watermark_ok(zone, order,
						zone->pages_high, end_zone, 0, 0))
					all_zones_ok = 0;
			}
			zone->temp_priority = priority;
			if (zone->prev_priority > priority)
				zone->prev_priority = priority;
			sc.nr_scanned = 0;
			sc.nr_reclaimed = 0;
			sc.priority = priority;
			sc.swap_cluster_max = nr_pages? nr_pages : SWAP_CLUSTER_MAX;
			shrink_zone(zone, &sc);
			reclaim_state->reclaimed_slab = 0;
			shrink_slab(sc.nr_scanned, GFP_KERNEL, lru_pages);
			sc.nr_reclaimed += reclaim_state->reclaimed_slab;
			total_reclaimed += sc.nr_reclaimed;
			total_scanned += sc.nr_scanned;
			if (zone->all_unreclaimable)
				continue;
  #+END_EXAMPLE 
  但是又只有一个地方设置all_unreclaimable,难道是zone会嵌套被扫描.
- 要在回收时扫描的页数大于或等于nr_active和nr_inactive的和的4倍时才会设置zone.all_unreclaimable.
- 关于scan_control.may_writepage:ulk:may_writepage If set, writing a dirty page to disk is
  allowed (only for laptop mode).
- 接下来要设置scan_control.may_writepage,设置是有条件的
  #+BEGIN_EXAMPLE
			/*
			 * If we've done a decent amount of scanning and
			 * the reclaim ratio is low, start doing writepage
			 * even in laptop mode
			 */
			if (total_scanned > SWAP_CLUSTER_MAX * 2 &&
			    total_scanned > total_reclaimed+total_reclaimed/2)
				sc.may_writepage = 1;
  #+END_EXAMPLE
  什么情况是做了大量的扫描呢?
  #+BEGIN_EXAMPLE
  			if (total_scanned > SWAP_CLUSTER_MAX * 2 &&
  #+END_EXAMPLE
  什么情况是回收率很低呢?
  #+BEGIN_EXAMPLE
			    total_scanned > total_reclaimed+total_reclaimed/2)
  #+END_EXAMPLE 
- scan_control.nr_reclaimed会一直在循环里累加的,不是循环一次就清0一次,
- 对以下的注释不清楚:
  #+BEGIN_EXAMPLE
		/*
		 * We do this so kswapd doesn't build up large priorities for
		 * example when it is freeing in parallel with allocators. It
		 * matches the direct reclaim path behaviour in terms of impact
		 * on zone->*_priority.
		 */
		if ((total_reclaimed >= SWAP_CLUSTER_MAX) && (!nr_pages))
			break;
  #+END_EXAMPLE
  nr_pages为0时是kswapd()调用的,在回收的页数大于32时就要退出这个优先级的循环.
- 如果结点里所有的区有少空闲页的情况,那么就回到开始再回一次页.
  #+BEGIN_EXAMPLE
	if (!all_zones_ok) {
		cond_resched();
		goto loop_again;
	}
  #+END_EXAMPLE
  all_zones_ok是在回收页的那个循环清的,表示还有少空闲页的区
  #+BEGIN_EXAMPLE
			if (nr_pages == 0) {	/* Not software suspend */
				if (!zone_watermark_ok(zone, order,
						zone->pages_high, end_zone, 0, 0))
					all_zones_ok = 0;
			}
  #+END_EXAMPLE
  只有在kswapd()调用的时候才会有回到开头,因为nr_pages为0时才会清all_zones_ok.

  在重新回到开始再回收页的时候已保存了上一次扫描的优先级.
  #+BEGIN_EXAMPLE
	for (i = 0; i < pgdat->nr_zones; i++) {
		struct zone *zone = pgdat->node_zones + i;

		zone->prev_priority = zone->temp_priority;
	}
  #+END_EXAMPLE 
** static void cache_reap(void *unused)
*** mm/slab.c:
- cache_reap()这个函数是属于PFRA的一部分,但是这个函数只是一个工作队列的回调函
  数.cache_reap()这个工作是放在keventd_wq这个工作队列里的.
- 这个函数是用来回收slab allocate cache的.
- 这个函数会在最后又重新注册自已到工作队列里.
- 一开始先获取把所有cache链在一起的链表的信号量,这里只是trylock,所以可以知道这个函数不想体
  眠,但是工作队列的回调函数是可以体眠的.

  如果不成功获取就重新插入到工作队列.
  #+BEGIN_EXAMPLE
	if (down_trylock(&cache_chain_sem)) {
		/* Give up. Setup the next iteration. */
		schedule_delayed_work(&__get_cpu_var(reap_work), REAPTIMEOUT_CPUC + smp_processor_id());
		return;
	}
  #+END_EXAMPLE
- 接着就循环所有的在cache_chain里的kmem_cache_t
- 若kmem_cache_t.flags的SLAB_NO_REAP设置了,那么就不能回收这个cache的slab
- 先调用drain_array_locked()把array_cache的slab放回cache,同时修改了array_cache.avail的值.这
  样就可以在回收cache的页时可以多回收一点页.
- kmem_cache_t里有一个kmem_list3,kmem_list3里有一个next_reap,这个next_reap就是说若要回收这
  个kmem_cache_t里的slab,那么回收的时间就要晚于next_reap里的时间,这个时间是jiffies值.
  #+BEGIN_EXAMPLE
		if(time_after(searchp->lists.next_reap, jiffies))
			goto next_unlock;
  #+END_EXAMPLE

  若回收时发现时间已过了,那么就要重新设置往后REAPTIMEOUT_LIST3,就是4s.要注意调用cache_reap
  的定时时间是REAPTIMEOUT_CPUC,说是2s
  #+BEGIN_EXAMPLE
		searchp->lists.next_reap = jiffies + REAPTIMEOUT_LIST3;
  #+END_EXAMPLE 
- 除了把本地cache里的一些slab放回到cache之外,还要把共享cache里slab放回到cache.
  #+BEGIN_EXAMPLE

		if (searchp->lists.shared)
			drain_array_locked(searchp, searchp->lists.shared, 0);
  #+END_EXAMPLE
  只有多核系统lists.shared才会不会为空,ulk:In multiprocessor systems, the function drains
  the slab shared cache (see the section "Local Caches of Free Slab Objects" in Chapter
  8);
- kmem_list3.free_touched设置了表示最近有新的slab加到这个cache里来,ulk:If a new slab has
  been recently added to the cachethat is, if the free_touched flag of the kmem_list3
  structure inside the cache descriptor is set.

  若free_touched设置了就不能又马上回收了
  #+BEGIN_EXAMPLE

		if (searchp->lists.free_touched) {
			searchp->lists.free_touched = 0;
			goto next_unlock;
		}
  #+END_EXAMPLE
  free_touched只有在这里清0.所以可以知道在一个cache新加入slab的时候,要第二次回收这个cache
  时才可以真正能够回收cache里的slab.
- 从当前的cache里释放大于kmem_cache_t.free_limit/(5*kmem_cache_t.num)的最小整数个
  slab,kmem_cache_t.num指的是一个slab里有多少个对象,kmem_cache_t.free_limit指的是cache里空
  闲的slab上限,为什么要这样计算释放的slab数呢?
- 是从kmem_list3.slabs_free里释放slab的,在之前从drain_array_locked()收回slab是可能通过
  drain_array_locked()里调用的free_block()来放回slabs_free链表.

  从slabs_free里释放一个slab的步骤就是(1)从slabs_free把该slab删除
  #+BEGIN_EXAMPLE
			list_del(&slabp->list);
  #+END_EXAMPLE
  (2)从kmem_list3.free_objects里把单个slab所包含的对象数减去.
  #+BEGIN_EXAMPLE
			searchp->lists.free_objects -= searchp->num;
  #+END_EXAMPLE
  (3)调用slab_destroy()
- 每处理完一个cache就要调度一次
  #+BEGIN_EXAMPLE
next:
		cond_resched();
  #+END_EXAMPLE 
- 把cache_chain链表里所有的cache都处理完之后就会把这个函数重新插入工作队列
  #+BEGIN_EXAMPLE
	schedule_delayed_work(&__get_cpu_var(reap_work), REAPTIMEOUT_CPUC + smp_processor_id());
  #+END_EXAMPLE 
** void out_of_memory(unsigned int __nocast gfp_mask)
*** mm/oom_kill.c:
- 这个函数在少内存时调用来选一进程来杀死
- 用select_bad_process()找出一个合适的进程.
- 调用oom_kill_process()把进程杀死.
** static struct task_struct * select_bad_process(void)
*** mm/oom_kill.c:
- 这个函数选一个合适的进程给oom_kill_process()杀死.
- 有什么条件呢?ulk有说,在select_bad_process()调用的badness()里有判断
  (1)拥有大量页框
  #+BEGIN_EXAMPLE
	/*
	 * Processes which fork a lot of child processes are likely
	 * a good choice. We add the vmsize of the childs if they
	 * have an own mm. This prevents forking servers to flood the
	 * machine with an endless amount of childs
	 */
	list_for_each(tsk, &p->children) {
		struct task_struct *chld;
		chld = list_entry(tsk, struct task_struct, sibling);
		if (chld->mm != p->mm && chld->mm)
			points += chld->mm->total_vm;
	}
  #+END_EXAMPLE
  (2)不能丢失大量的工作,可以用运行时间判断
  #+BEGIN_EXAMPLE
	/*
	 * CPU time is in tens of seconds and run time is in thousands
         * of seconds. There is no particular reason for this other than
         * that it turned out to work very well in practice.
	 */
	cpu_time = (cputime_to_jiffies(p->utime) + cputime_to_jiffies(p->stime))
		>> (SHIFT_HZ + 3);

	if (uptime >= p->stlart_time.tv_sec)
		run_time = (uptime - p->start_time.tv_sec) >> 10;
	else
		run_time = 0;

	s = int_sqrt(cpu_time);
	if (s)
		points /= s;
	s = int_sqrt(int_sqrt(run_time));
	if (s)
		points /= s;
  #+END_EXAMPLE
  (3)优先级比较低的
  #+BEGIN_EXAMPLE

	/*
	 * Niced processes are most likely less important, so double
	 * their badness points.
	 */
	if (task_nice(p) > 0)
		points *= 2;
  #+END_EXAMPLE
  (4)最好不是超级用户的进程
  #+BEGIN_EXAMPLE
	/*
	 * Superuser processes are usually more important, so we make it
	 * less likely that we kill those.
	 */
	if (cap_t(p->cap_effective) & CAP_TO_MASK(CAP_SYS_ADMIN) ||
				p->uid == 0 || p->euid == 0)
		points /= 4;
  #+END_EXAMPLE
  (5)进程最好不是访问硬件的
  #+BEGIN_EXAMPLE
	/*
	 * We don't want to kill a process with direct hardware access.
	 * Not only could that mess up the hardware, but usually users
	 * tend to only have this flag set on applications they think
	 * of as important.
	 */
	if (cap_t(p->cap_effective) & CAP_TO_MASK(CAP_SYS_RAWIO))
		points /= 4;
  #+END_EXAMPLE

  以上的条件是通过加权来确定这个进程是合适程度.加权值返回给select_bad_process()
- select_bad_process()通过遍历所有的进程找出badness()返回加权值最大的进程来返回给
  out_of_memory().
- 进程0(swapper)和进程1(init)是不能被杀死的,task_struct.oomkilladj为OOM_DISABLE时也不能杀
  死
  #+BEGIN_EXAMPLE
		if (p->pid > 1 && p->oomkilladj != OOM_DISABLE) {
  #+END_EXAMPLE 
** static struct mm_struct *oom_kill_process(struct task_struct *p)
*** mm/oom_kill.c:
- The oom_kill_process( )function also kills all clones that share the same memory
  descriptor with the selected victim.
- 这个函数不会先把进程p给杀死,而是找一个子进程来杀死,
- 因为杀一个子进程是与其父进程共享内存描述符的,那么就不选择这个子进程
  #+BEGIN_EXAMPLE
		if (c->mm == p->mm)
			continue;
  #+END_EXAMPLE 
** void grab_swap_token(void)
*** mm/thrash.c:
- 调用这个函数是用来获取swap token的,以使该进程的页不被换出去.
- 关于swap token,swap token只有一个,在同一时刻只有一个进程可以获取它.

  在初始化的时候是把swap token给了init进程
  #+BEGIN_EXAMPLE
struct mm_struct * swap_token_mm = &init_mm;
  #+END_EXAMPLE 
  可以通过判断进程的task_struct.mm是否等于swap_token_mm就可以知道该进程有没有获取swap
  token了.
- 这个函数只有filemap_nopage()和do_swap_page()调用
- 如果在已获得swap token的情况下还去获取swap token,那么就会设置task_struct.recent_pagein.
  task_struct.recent_pagein只在should_release_swap_token()里清
  零,should_release_swap_token()清零的原因,0表示就仅仅清零,1表示没有两次连续调用
  grab_swap_token()时都是该进程来获取swap token,2表示获取的时间过长了.
- 注意这个函数是给当前进程获取swap token的,所以要通过看正在拥有swap token的mm是否能够释放
  swap token,这要看should_release_swap_token()的返回值,若是返回0就不会释放swap token,若是
  返回1或2就会释放swap token并给当前的进程.

  所以只有在已获得swap token的进程连续两次获取swap token且没有超过swap token的持有时间就不
  会把swap token给当前进程.
- 关于swap_token_check这个全局变量,只有一个这个函数会读取这个值来使用,就是只有时间超过
  swap_token_check的时间才可以去对swap_token_mm操作.

  swap_token_check在这个函数修改和在__put_swap_token()修改.
- 关于swap_token_timeout,只有在修改了swap_token_mm时才会修改
  #+BEGIN_EXAMPLE
			swap_token_timeout = jiffies + swap_token_default_timeout;
			swap_token_mm = current->mm;
  #+END_EXAMPLE
- 关于task_struct.mm.swap_token_time,通过分析代码,swap_token_time的意思是进程交出(释
  放)swap_token_mm的时间
  #+BEGIN_EXAMPLE
		if ((reason = should_release_swap_token(mm))) {
			unsigned long eligible = jiffies;
			if (reason == SWAP_TOKEN_TIMED_OUT) {
				eligible += swap_token_default_timeout;
			}
			mm->swap_token_time = eligible;
  #+END_EXAMPLE
  为什么在超时的时候释放会加多一个swap_token_default_timeout呢?可能是为了惩罚吧.所以释改时
  间是有可能大于当前时间的.

  所以若一个进程想获得swap_token_mm,但是它不久前是在超时的情况下释放swap_token_mm的,且释放
  的时间还没有到当前获得swap_token_mm的进程的超时时间
  #+BEGIN_EXAMPLE
		/* ... or if we recently held the token. */
		if (time_before(jiffies, current->mm->swap_token_time))
			return;
  #+END_EXAMPLE
  #+BEGIN_EXAMPLE
			swap_token_timeout = jiffies + swap_token_default_timeout;
			swap_token_mm = current->mm;
  #+END_EXAMPLE 
- 在两个地方调用这个函数,

  一个是filemap_nopage()发现的页不是page cache里要从disk去读,

  另一个是do_swap_page()从swap area里读一个新的页.

  所以都是从disk里读数据时才会获得swap token.

** not function
*** 17.4
- 有三种页是要用到swap的,

  1.进程的匿名页(在匿名区里的页)(用户态的堆和栈)

  2.进程的 _私有_ 内存映射的 _脏_ 页

  3.IPC共享内存区的页

- swapping是使用页表项的Present位来确定页不在内存里的,若页不在内存里,那么页表项里的其它位
  就给swapping用来确定页是放在swapping里的哪个地方.
- 什么是swap area:The pages swapped out from memory are stored in a swap area, which may
  be implemented either as a disk partition of its own or as a file included in a larger
  partition.
- swap area的第一页是保存swap area信息的,这些信息保存在union swap_header里,这个联合体的大
  小是一个页的大小,回为联合体里的magic结构体的大小是一个页的大小.

  至于为什么要用联合体来表示呢?可能是因为想达到在一个页的最后10个字节保存10个字符来说明这
  个swap area是以一个文件的形式存在还是以一个分区的形式存在;

  那又为什么要在页的最后保存而不是在开头保存呢?可能是因为当一个swap area是一个分区的时候要
  用到前面的1024个字节作特殊的用途
  #+BEGIN_EXAMPLE
union swap_header {
	struct {
		char reserved[PAGE_SIZE - 10];
		char magic[10];			/* SWAP-SPACE or SWAPSPACE2 */
	} magic;
	struct {
		char	     bootbits[1024];	/* Space for disklabel etc. */
		unsigned int version;
		unsigned int last_page;
		unsigned int nr_badpages;
		unsigned int padding[125];
		unsigned int badpages[1];
	} info;
};
  #+END_EXAMPLE

  bootbits:Not used by the swapping algorithm; this field corresponds to the first 1,024
  bytes of the swap area, which may store partition data, disk labels, and so on.

  version是swapping算法的版本号.

  nr_badpages:因为disk是有可能有坏块的,所以要用这个成员来说明swap area里有多少个坏块.

  struct swap_extent指定了从哪个页到哪个页是连续的.是不是用来处理坏块的呢?好像也不是,ulk:An
  ordered list of the extents that compose a swap area is created when activating the swap
  area itself. A swap area stored in a disk partition is composed of just one extent;
  conversely, a swap area stored in a regular file can be composed of several extents,
  because the filesystem may not have allocated the whole file in contiguous blocks on
  disk.

  swapping算法最重要的是在换出时能尽量把页放到连续的块里.

  swapping算法是这样把页分布到不同的swap area的:先在具有相同的且是最高优先级的swap area里
  扫描来找空slot,找不到再到低优先级的swap area里找.在同级里循环是不让其中一个过载.

  应该是按swap area的快慢来定优先级的.

  swap_info是一个数组,包含了所有的用来描述 _激活_ 的swap area的swap_info_struct.

  nr_swapfiles:因为swap_info这个数组是固定分配的,所以要用nr_swapfiles来说明数组里有多少个
  有效的元素.

  swap_info_struct.next的作用是在换出页时要把next表示的swap_info数组下标里的元素作为下一个
  换出页的swap area.

  swap_list不知道用来作什么的.

  swap_info_struct.max指swap area总的页数,swap_info_struct.pags指swap area可用的slot,不同
  是因为第一个slot不用且有一些坏块.

  nr_swap_pages这个全局变量包含的是在所有swap area里free的且没有坏的slot的个数.

  total_swap_pages这个全局变量包含的是在所有的swap area里所有没有坏的slot的个数.

  所以要定位一个slot,只要两个东西,一个是在swap_info的索引,一个是在swap area的slot的索引.
** int swap_duplicate(swp_entry_t entry)
*** mm/swapfile.c:
- ulk:The swap_duplicate( ) function is usually invoked while trying to swap out an
  already swapped-out page.因为一个页框可能是属于多个地址空间的,所以有可能被多次换出.

  页被正真换出到disk是第一次被换出时,而接下来的换出只是增加该页在swap_info_struct.swap_map
  数组里对应的值.
- 这个函数就是找到entry对应的slot的swap_map的元素递增
** asmlinkage long sys_swapon(const char __user * specialfile, int swap_flags)
*** mm/swapfile.c:
- 这个函数是打开一个swap area,这个swap area是由specialfile指定的文件或分区.
- 至于打开的这个swap area是在swap_info的哪个元素呢?就是下标最小的,没有被使用的元素
  #+BEGIN_EXAMPLE
	for (type = 0 ; type < nr_swapfiles ; type++,p++)
		if (!(p->flags & SWP_USED))
			break;
  #+END_EXAMPLE
  注意是不超过nr_swapfiles的,若第nr_swapfiles被使用了,所面会使用第nr_swapfiles+1个.
  #+BEGIN_EXAMPLE
	if (type >= nr_swapfiles)
		nr_swapfiles = type+1;
  #+END_EXAMPLE 
  现在发现nr_swapfiles只有加没有减的.

- 不明白下面的检查有什么作用
  #+BEGIN_EXAMPLE
	/*
	 * Test if adding another swap device is possible. There are
	 * two limiting factors: 1) the number of bits for the swap
	 * type swp_entry_t definition and 2) the number of bits for
	 * the swap type in the swap ptes as defined by the different
	 * architectures. To honor both limitations a swap entry
	 * with swap offset 0 and swap type ~0UL is created, encoded
	 * to a swap pte, decoded to a swp_entry_t again and finally
	 * the swap type part is extracted. This will mask all bits
	 * from the initial ~0UL that can't be encoded in either the
	 * swp_entry_t or the architecture definition of a swap pte.
	 */
	if (type > swp_type(pte_to_swp_entry(swp_entry_to_pte(swp_entry(~0UL,0))))) {
		swap_list_unlock();
		goto out;
	}
  #+END_EXAMPLE 
- 参数swap_flags的作用是指定所打开的这个swap area是不是使用了优先级且指定了优先级是多
  少:swap_flags用一个位指定使用不使用优先级,其它的31个位指定优先级.
  #+BEGIN_EXAMPLE
	if (swap_flags & SWAP_FLAG_PREFER) {
		p->prio =
		  (swap_flags & SWAP_FLAG_PRIO_MASK)>>SWAP_FLAG_PRIO_SHIFT;
  #+END_EXAMPLE
  若没有设置SWAP_FLAG_PREFER,那么就使用这个函数的静态变量least_prioprity来确定它的优先级是
  多少.

  least_prioprity的使用方法是:least_prioprity从0开始,调用一次过程中若SWAP_FLAG_PREFER没有
  设置且swap area已成功打开就把least_prioprity _自减_.
  #+BEGIN_EXAMPLE
	} else {
		p->prio = --least_priority;
	}
  #+END_EXAMPLE 
- 接下来就是打开specialfile指定的文件了
  #+BEGIN_EXAMPLE
	swap_list_unlock();
	name = getname(specialfile);
	error = PTR_ERR(name);
	if (IS_ERR(name)) {
		name = NULL;
		goto bad_swap_2;
	}
	swap_file = filp_open(name, O_RDWR|O_LARGEFILE, 0);
	error = PTR_ERR(swap_file);
	if (IS_ERR(swap_file)) {
		swap_file = NULL;
		goto bad_swap_2;
	}

	p->swap_file = swap_file;
	mapping = swap_file->f_mapping;
	inode = mapping->host;
  #+END_EXAMPLE 
- 接下来就是检查这个被打开的作为swap area的文件是不是已经被当作swap area的文件了,
  #+BEGIN_EXAMPLE
	error = -EBUSY;
	for (i = 0; i < nr_swapfiles; i++) {
		struct swap_info_struct *q = &swap_info[i];

		if (i == type || !q->swap_file)
			continue;
		if (mapping == q->swap_file->f_mapping)
			goto bad_swap;
	}
  #+END_EXAMPLE 
- 若是swap area是一个块设备,(1)那么就调用bd_claim()来独占这个设备,(2)接着就是重新设置设备
  的块大小block_device.bd_block_size为页的大小,在设置之前要把原来的块大小保存起来,保存在
  swap_info_struct.old_block_size里,swap_info_struct.old_block_size就是这样使用的.
  #+BEGIN_EXAMPLE
	if (S_ISBLK(inode->i_mode)) {
		bdev = I_BDEV(inode);
		error = bd_claim(bdev, sys_swapon);
		if (error < 0) {
			bdev = NULL;
			goto bad_swap;
		}
		p->old_block_size = block_size(bdev);
		error = set_blocksize(bdev, PAGE_SIZE);
		if (error < 0)
			goto bad_swap;
		p->bdev = bdev;
	}
  #+END_EXAMPLE 
- 若swap area是一个文件,也要有bd_claim()相似的独占检查
  #+BEGIN_EXAMPLE
	} else if (S_ISREG(inode->i_mode)) {
		p->bdev = inode->i_sb->s_bdev;
		down(&inode->i_sem);
		did_down = 1;
		if (IS_SWAPFILE(inode)) {
			error = -EBUSY;
			goto bad_swap;
		}
  #+END_EXAMPLE 
- 接下来就是获取swap area的第一个页,这个页存放的就是swap_head的数据,这些数据应该是在创建
  swap area时就已经写入到这个页里了,把获取的第一个页的数据所在内存的页框的首地址赋给
  swap_header.
  #+BEGIN_EXAMPLE
	/*
	 * Read the swap header.
	 */
	if (!mapping->a_ops->readpage) {
		error = -EINVAL;
		goto bad_swap;
	}
	page = read_cache_page(mapping, 0,
			(filler_t *)mapping->a_ops->readpage, swap_file);
	if (IS_ERR(page)) {
		error = PTR_ERR(page);
		goto bad_swap;
	}
	wait_on_page_locked(page);
	if (!PageUptodate(page))
		goto bad_swap;
	kmap(page);
	swap_header = page_address(page);
  #+END_EXAMPLE 
- swap_header.magic.magic为SWAP-SPACE这个版本不再被支持
  #+BEGIN_EXAMPLE
	if (!memcmp("SWAP-SPACE",swap_header->magic.magic,10))
		swap_header_version = 1;
	else if (!memcmp("SWAPSPACE2",swap_header->magic.magic,10))
		swap_header_version = 2;
	else {
		printk("Unable to find swap-space signature\n");
		error = -EINVAL;
		goto bad_swap;
	}
	
	switch (swap_header_version) {
	case 1:
		printk(KERN_ERR "version 0 swap is no longer supported. "
			"Use mkswap -v1 %s\n", name);
		error = -EINVAL;
		goto bad_swap;
  #+END_EXAMPLE 
- 还要检查swap header的子版本号
  #+BEGIN_EXAMPLE
		/* Check the swap header's sub-version and the size of
                   the swap file and bad block lists */
		if (swap_header->info.version != 1) {
			printk(KERN_WARNING
			       "Unable to handle swap header version %d\n",
			       swap_header->info.version);
			error = -EINVAL;
			goto bad_swap;
		}
  #+END_EXAMPLE
- 最后就是找出这个swap area的允许多少个页,swap_info_struct.lowest_bit:First page slot to
  be scanned when searching for a free one.

  swap_info_struct.highest_bit:Last page slot to be scanned when searching for a free one.

  swap_head.info.last_page:Last page slot that is effectively usable.

  #+BEGIN_EXAMPLE

		p->lowest_bit  = 1;
		/*
		 * Find out how many pages are allowed for a single swap
		 * device. There are two limiting factors: 1) the number of
		 * bits for the swap offset in the swp_entry_t type and
		 * 2) the number of bits in the a swap pte as defined by
		 * the different architectures. In order to find the
		 * largest possible bit mask a swap entry with swap type 0
		 * and swap offset ~0UL is created, encoded to a swap pte,
		 * decoded to a swp_entry_t again and finally the swap
		 * offset is extracted. This will mask all the bits from
		 * the initial ~0UL mask that can't be encoded in either
		 * the swp_entry_t or the architecture definition of a
		 * swap pte.
		 */
		maxpages = swp_offset(pte_to_swp_entry(swp_entry_to_pte(swp_entry(0,~0UL)))) - 1;
		if (maxpages > swap_header->info.last_page)
			maxpages = swap_header->info.last_page;
		p->highest_bit = maxpages - 1;
  #+END_EXAMPLE 
- 如果有坏的页就要给坏的页标上SWAP_MAP_BAD
  #+BEGIN_EXAMPLE
		/* OK, set up the swap map and apply the bad block list */
		if (!(p->swap_map = vmalloc(maxpages * sizeof(short)))) {
			error = -ENOMEM;
			goto bad_swap;
		}

		error = 0;
		memset(p->swap_map, 0, maxpages * sizeof(short));
		for (i=0; i<swap_header->info.nr_badpages; i++) {
			int page = swap_header->info.badpages[i];
			if (page <= 0 || page >= swap_header->info.last_page)
				error = -EINVAL;
			else
				p->swap_map[page] = SWAP_MAP_BAD;
		}
  #+END_EXAMPLE

  通过swap_header.info.badpages数组来看哪个页是坏页.
- 接下来就是调用setup_swap_extents()给这个swap_info_struct来建立swap extent
- 接下来就是设置这个swap area的优先级
  #+BEGIN_EXAMPLE
	/* insert swap space into swap_list: */
	prev = -1;
	for (i = swap_list.head; i >= 0; i = swap_info[i].next) {
		if (p->prio >= swap_info[i].prio) {
			break;
		}
		prev = i;
	}
	p->next = i;
	if (prev < 0) {
		swap_list.head = swap_list.next = p - swap_info;
	} else {
		swap_info[prev].next = p - swap_info;
	}
  #+END_EXAMPLE
  所有的swap area的swap_info_struct都放在swap_info这个数组里,且放的顺序没有规律,要把它们按
  优先级排序起来,使用了swap_info_struct.next成员,这个是成员表示的是下一个同级或低一级的
  swap area所在swap_info数组里的位置的下标,注意是同级或低一级的,所以所有的swap_info_struct
  是一条线连起来的,而不是同级的一条线,低一级的另一条线.
** int bd_claim(struct block_device *bdev, void *holder)
*** fs/block_dev.c:
- 这个函数是处理block_device.bd_holder/bd_contains的
- block_device.bd_holder是主要由这个函数和对应的bd_release()来处理,处理就只给
  blkdev_close()使用一次.
- 关于bd_contains,bd_holder和这个函数的说明:ulk:If the block device descriptor refers to a
  disk partition, the bd_contains field points to the descriptor of the block device
  associated with the whole disk, while the bd_part field points to the hd_struct
  partition descriptor (see the section "Representing Disks and Disk Partitions" earlier
  in this chapter). Otherwise, if the block device descriptor refers to a whole disk, the
  bd_contains field points to the block device descriptor itself, and the bd_part_count
  field records how many times the partitions on the disk have been opened.
  
  The bd_holder field stores a linear address representing the holder of the block
  device. The holder is not the block device driver that services the I/O data transfers
  of the device; rather, it is a kernel component that makes use of the device and has
  exclusive, special privileges (for instance, it can freely use the bd_private field of
  the block device descriptor). Typically, the holder of a block device is the filesystem
  mounted over it. Another common case occurs when a block device file is opened for
  exclusive access: the holder is the corresponding file object.
  
  The bd_claim( ) function sets the bd_holder field with a specified address; conversely,
  the bd_release( ) function resets the field to NULL. Be aware, however, that the same
  kernel component can invoke bd_claim( ) many times; each invocation increases the
  bd_holders field. To release the block device, the kernel component must invoke
  bd_release( ) a corresponding number of times.

  关于bd_contains,也就是若block_device描述符是一个分区的描述符,那么bd_contains指向的是该分区所在的disk的
  block_device描述符.若block_device描述符是一个disk的block_device的描述符,那么
  block_device.bd_contains指向自已的block_device.

  关于bd_holder,它保存holder的线性地址,这个线性地址不是该block device的驱动,而是具在独占
  使用这个block_device的kernel component.如sys_swapon()函数就是这样的一个holder.bd_holders
  是bd_holder的计数器.

- 对于以下的代码有两种不同的结果
  #+BEGIN_EXAMPLE
	if (res==0) {
		/* note that for a whole device bd_holders
		 * will be incremented twice, and bd_holder will
		 * be set to bd_claim before being set to holder
		 */
		bdev->bd_contains->bd_holders ++;
		bdev->bd_contains->bd_holder = bd_claim;
		bdev->bd_holders++;
		bdev->bd_holder = holder;
	}
  #+END_EXAMPLE

  若bdev是一分区的block_device,那么分区的bd_holders和包含这个分区的disk的bd_holders各自增
  加1,否则,disk的bd_holders就会加2.

  若bdev是一分区的block_device,那么分区的bd_holder指向参数holder,包含这个分区的disk的
  bd_holder指向bd_claim这个函数,否则,disk的bd_holder指向参数holder.简而言之就是参数holder
  一定是被引用的,有分区时分区就引用否则就disk引用.
** static int setup_swap_extents(struct swap_info_struct *sis)
*** mm/swapfile.c:
- 这个函数是给一个swap area建立一个swap extents list的
- 无论swap area是一个文件还是一个块设备,都是以 PAGE_SIZE 来操作
- swap_info_struct.max:Size of swap area in pages.是该架构能寻址到的最大的swap entry号和
  swap_head.info.last_page这两个中的最小的那个值
- 如果swap area是一个块设备,那么就建一个swap extent就可以了,add_swap_extent()后面有介绍
  #+BEGIN_EXAMPLE
	inode = sis->swap_file->f_mapping->host;
	if (S_ISBLK(inode->i_mode)) {
		ret = add_swap_extent(sis, 0, sis->max, 0);
		goto done;
	}
  #+END_EXAMPLE
- bmap()通过在文件里的块号计算出对应的块设备的块号.
  #+BEGIN_EXAMPLE
		first_block = bmap(inode, probe_block);
		if (first_block == 0)
			goto bad_bmap;
  #+END_EXAMPLE
  得到的first_block必须为与页大小对齐,所以有以下的代码
  #+BEGIN_EXAMPLE

		/*
		 * It must be PAGE_SIZE aligned on-disk
		 */
		if (first_block & (blocks_per_page - 1)) {
			probe_block++;
			goto reprobe;
		}
  #+END_EXAMPLE
  当不与页大小对齐时就会增加probe_block回到循环的开始,那么又可以用bmap()来计算下一块设备的
  逻辑块号了.为什么要对齐呢?应该是为了方便计算吧.
- 让first_block这个块号与页大小对齐之外,还需要从这个块往后的一定的块数(这些块数的大小加起
  来是一个页的大小)是连续的,因为一个文件不一定是用连续的块来存放的.
  #+BEGIN_EXAMPLE
		for (block_in_page = 1; block_in_page < blocks_per_page;
					block_in_page++) {
			sector_t block;

			block = bmap(inode, probe_block + block_in_page);
			if (block == 0)
				goto bad_bmap;
			if (block != first_block + block_in_page) {
				/* Discontiguity */
				probe_block++;
				goto reprobe;
			}
		}
  #+END_EXAMPLE 
- 有了之前的检查,first_block是一个合适的块,这时就可以用add_swap_extent()把这些块作为一个
  swap slot来加到swap extent list里,因为add_swap_extent()里若发现被加的slot可以合并到某个
  swap extent的话就会合并
  #+BEGIN_EXAMPLE
		/*
		 * We found a PAGE_SIZE-length, PAGE_SIZE-aligned run of blocks
		 */
		ret = add_swap_extent(sis, page_no, 1,
				first_block >> (PAGE_SHIFT - blkbits));
  #+END_EXAMPLE 
- 从以下的代码可以看出,swap_info_struct.max就是swap area里有效的slot
  #+BEGIN_EXAMPLE
	if (page_no == 0)
		ret = -EINVAL;
	sis->max = page_no;
	sis->highest_bit = page_no - 1;
  #+END_EXAMPLE 
- 所以这个函数主要是给swap area建立swap extent list的,并设置
  swap_info_struct.max,
  swap_info_struct.highest_bit,
  swap_info_struct.curr_swap_extent.
- 注意swap_info_struct.max,swap_info_struct.highest_bit这两个变量是在swap area是文件时才会
  设置的,那么若是块设备又是怎么设置的呢?在sys_swapon()里无论是文件还是块设备都设置了这两个
  成员,所以是块设备的话在这个函数里也不用设置了.
- 在sys_swapon()调用这个函数之前已经设置了swap_info_struct.pages:Number of usable page
  slots.所以可以推断出在setup_swap_extents()函数里添加到swap extent里的slot数一定是
  swap_info_struct.pages.

** static int add_swap_extent(struct swap_info_struct *sis, unsigned long start_page, unsigned long nr_pages, sector_t start_block)
*** mm/swapfile.c:
- 调用这个函数把一段连续的块加入到swap extent list里.
- 首先要看这一段连续的块能不能与原来的swap extent合并,只要原来的list里有一个extent的最后一
  个block和page是与被加的连续的块的第一个block和page是连着的就可以合并
  #+BEGIN_EXAMPLE
	while (lh != &sis->extent_list) {
		se = list_entry(lh, struct swap_extent, list);
		if (se->start_block + se->nr_pages == start_block &&
		    se->start_page  + se->nr_pages == start_page) {
			/* Merge it */
			se->nr_pages += nr_pages;
			return 0;
		}
		lh = lh->next;
	}
  #+END_EXAMPLE 
- swap_extent.start_page和swap_extent.start_block有什么区别吗?start_page可以与
  swap_extent.nr_pages相加start_block也可以与nr_pages相加.

  在sys_swapon()里,若swap area是一个块设备,那么会把block_device.bd_block_szie设为页的大小.
- 如果不能合并就新增一个extent list.
  #+BEGIN_EXAMPLE

	/*
	 * No merge.  Insert a new extent, preserving ordering.
	 */
	new_se = kmalloc(sizeof(*se), GFP_KERNEL);
	if (new_se == NULL)
		return -ENOMEM;
	new_se->start_page = start_page;
	new_se->nr_pages = nr_pages;
	new_se->start_block = start_block;
  #+END_EXAMPLE 
- 要把新增的swap extent按块的顺序加到extent list里
  #+BEGIN_EXAMPLE
	lh = sis->extent_list.prev;	/* The lowest block */
	while (lh != &sis->extent_list) {
		se = list_entry(lh, struct swap_extent, list);
		if (se->start_block > start_block)
			break;
		lh = lh->prev;
	}
	list_add_tail(&new_se->list, lh);
  #+END_EXAMPLE 
** asmlinkage long sys_swapoff(const char __user * specialfile)
*** mm/swapfile.c:
- 先打开参数specialfile指定的swap area文件.
  #+BEGIN_EXAMPLE

	pathname = getname(specialfile);
	err = PTR_ERR(pathname);
	if (IS_ERR(pathname))
		goto out;

	victim = filp_open(pathname, O_RDWR|O_LARGEFILE, 0);
	putname(pathname);
	err = PTR_ERR(victim);
	if (IS_ERR(victim))
		goto out;
  #+END_EXAMPLE
- 成功打开后就要找出swap_info数组里与该文件对应的swap_info_struct.

  找的方法是用swap_info_struct.swap_file.f_mapping与file.f_mapping比较,不能用
  swap_info_struct.swap_file与file比较.
  #+BEGIN_EXAMPLE
	mapping = victim->f_mapping;
	prev = -1;
	swap_list_lock();
	for (type = swap_list.head; type >= 0; type = swap_info[type].next) {
		p = swap_info + type;
		if ((p->flags & SWP_ACTIVE) == SWP_ACTIVE) {
			if (p->swap_file->f_mapping == mapping)
				break;
		}
		prev = type;
	}
  #+END_EXAMPLE 
- 接着就看有没有足够的内存可以把swap area的页全部换入.
  #+BEGIN_EXAMPLE
	if (!security_vm_enough_memory(p->pages))
		vm_unacct_memory(p->pages);
	else {
		err = -ENOMEM;
		swap_list_unlock();
		goto out_dput;
	}
  #+END_EXAMPLE 
- 接着就可以把找到的swap_info_struct从swap_info数组里删除了.删除的方法就是从链表里脱掉而已.
  #+BEGIN_EXAMPLE
	if (prev < 0) {
		swap_list.head = p->next;
	} else {
		swap_info[prev].next = p->next;
	}
  #+END_EXAMPLE
- SWP_WRITEOK:1 if it is possible to write into the swap area; 0 if the swap area is
  read-only (it is being activated or inactivated).
  #+BEGIN_EXAMPLE
	p->flags &= ~SWP_WRITEOK;
  #+END_EXAMPLE 
- 设置task_struct.flag的PF_SWAPOFF表示这个进程正在换入一个swap area所有slot.
  #+BEGIN_EXAMPLE
	current->flags |= PF_SWAPOFF;
	err = try_to_unuse(type);
	current->flags &= ~PF_SWAPOFF;
  #+END_EXAMPLE 
- 如果不能把所有的页换入,那么就不能删除这个swap area,那么就要撤回之前的操作,如恢复
  swap_info数组等
- 若把所有的slot都换入了,那么就把swap extent list给删掉.
** static int try_to_unuse(unsigned int type)
*** mm/swapfile.c:
- 在swap area里找出一个正在被用的slot,用find_next_to_unuse(),找的方法是用
  swap_info_struct.swap_map[]的slot计数来判断.
- 若进程接收到信号就退出,为什么要这样子做呢?
  #+BEGIN_EXAMPLE
		if (signal_pending(current)) {
			retval = -EINTR;
			break;
		}
  #+END_EXAMPLE
- read_swap_cache_async()后面在介绍,在没有相应的swap entry的时候会继续下一个swap entry,在
  没有内存的时候会退出换入操作.
  #+BEGIN_EXAMPLE
		page = read_swap_cache_async(entry, NULL, 0);
		if (!page) {
			/*
			 * Either swap_duplicate() failed because entry
			 * has been freed independently, and will not be
			 * reused since sys_swapoff() already disabled
			 * allocation from here, or alloc_page() failed.
			 */
			if (!*swap_map)
				continue;
			retval = -ENOMEM;
			break;
		}
  #+END_EXAMPLE 
- 不知道下面代码什么意思
  #+BEGIN_EXAMPLE
		/*
		 * Don't hold on to start_mm if it looks like exiting.
		 */
		if (atomic_read(&start_mm->mm_users) == 1) {
			mmput(start_mm);
			start_mm = &init_mm;
			atomic_inc(&init_mm.mm_users);
		}
  #+END_EXAMPLE
  是不是说start_mm.mm_users为1的时候就只有这个函数在使用这个mm_struct?若是这样的话就没有必
  要再扫描这个mm_struct了,但是为什么要重新扫描呢,把start_mm改成init_mm呢?
- try_to_unuse()和do_swap_page()可能会竞争,所以要把页给锁起来
  #+BEGIN_EXAMPLE

		/*
		 * Wait for and lock page.  When do_swap_page races with
		 * try_to_unuse, do_swap_page can handle the fault much
		 * faster than try_to_unuse can locate the entry.  This
		 * apparently redundant "wait_on_page_locked" lets try_to_unuse
		 * defer to do_swap_page in such a case - in some tests,
		 * do_swap_page and try_to_unuse repeatedly compete.
		 */
		wait_on_page_locked(page);
		wait_on_page_writeback(page);
		lock_page(page);
		wait_on_page_writeback(page);
  #+END_EXAMPLE
  这段代码好像有点问题,就是不能保证是try_to_unuse()把页给锁起来的,还是页的PG_locked的理解
  有错?
- 是不是没有一个mm_struct的指针引用了一个mm_struct时就会增加mm_struct.mm_users呢?就好像下
  面的代码,start_mm就的mm_users就被增加了两次
  #+BEGIN_EXAMPLE
		if (*swap_map > 1) {
			int set_start_mm = (*swap_map >= swcount);
			struct list_head *p = &start_mm->mmlist;
			struct mm_struct *new_start_mm = start_mm;
			struct mm_struct *prev_mm = start_mm;
			struct mm_struct *mm;

			atomic_inc(&new_start_mm->mm_users);
			atomic_inc(&prev_mm->mm_users);
			spin_lock(&mmlist_lock);
			while (*swap_map > 1 && !retval &&
					(p = p->next) != &start_mm->mmlist) {
				mm = list_entry(p, struct mm_struct, mmlist);
				if (atomic_inc_return(&mm->mm_users) == 1) {
					atomic_dec(&mm->mm_users);
					continue;
				}
  #+END_EXAMPLE
- 接下来就是循环整个mm链表来找出所有引用这个被换入页的pte页表项,对于每一个mm_struct使用
  unuse_mm()来处理.下面有介绍.
- 不知道下面的代码的原因
  #+BEGIN_EXAMPLE
		/*
		 * If a reference remains (rare), we would like to leave
		 * the page in the swap cache; but try_to_unmap could
		 * then re-duplicate the entry once we drop page lock,
		 * so we might loop indefinitely; also, that page could
		 * not be swapped out to other storage meanwhile.  So:
		 * delete from cache even if there's another reference,
		 * after ensuring that the data has been saved to disk -
		 * since if the reference remains (rarer), it will be
		 * read from disk into another page.  Splitting into two
		 * pages would be incorrect if swap supported "shared
		 * private" pages, but they are handled by tmpfs files.
		 *
		 * Note shmem_unuse already deleted a swappage from
		 * the swap cache, unless the move to filepage failed:
		 * in which case it left swappage in cache, lowered its
		 * swap count to pass quickly through the loops above,
		 * and now we must reincrement count to try again later.
		 */
		if ((*swap_map > 1) && PageDirty(page) && PageSwapCache(page)) {
			struct writeback_control wbc = {
				.sync_mode = WB_SYNC_NONE,
			};

			swap_writepage(page, &wbc);
			lock_page(page);
			wait_on_page_writeback(page);
		}
  #+END_EXAMPLE 
** static int unuse_mm(struct mm_struct *mm,swp_entry_t entry, struct page *page)
*** mm/swapfile.c:
- 这个函数就是对在mm.mmap链表里的所有vm_area都调用unuse_vma()把该vm_area里有虚拟地址对应的
  页表项是entry的项改成引用page页.在调用unuse_vma()之前要先判断vm_area.anon_vma是不是空,不
  为空才可以调用,也就是说该区是匿名区才可以调用.
- 这个函数通过层层调
  用:unuse_mm()->unuse_pud_range()->unuse_pmd_range()->unuse_pte_range()->unuse_pte()->set_pte_at()
  把entry对应的页表项改成引用page.同时unuse_pte()还调用了swap_free()把
  swap_info_struct.swap_map里entry对应的计数减1.
** struct page *read_swap_cache_async(swp_entry_t entry, struct vm_area_struct *vma, unsigned long addr)
*** mm/swap_state.c:
- ulk:The read_swap_cache_async( ) function is invoked whenever the kernel must swap in a
  page.
- 不管有多少个swap area都只有一个radix树是包含所有的swap cache的,所以任何一个swap area的
  slot对应swap page的是使用swp_entry()来生成的,swp_entry()使用了swap area类型和对应的slot
  号.
- 先在swapper_space的radix树里找出entry对应的页.若找到就返回这个页.
  #+BEGIN_EXAMPLE
		found_page = find_get_page(&swapper_space, entry.val);
		if (found_page)
			break;
  #+END_EXAMPLE 
- 若没找到就分配一个
  #+BEGIN_EXAMPLE

		/*
		 * Get a new page to read into from swap.
		 */
		if (!new_page) {
			new_page = alloc_page_vma(GFP_HIGHUSER, vma, addr);
			if (!new_page)
				break;		/* Out of memory */
		}
  #+END_EXAMPLE 
- 接着就把这个页加到swap cache里,
  #+BEGIN_EXAMPLE
		/*
		 * Associate the page with swap entry in the swap cache.
		 * May fail (-ENOENT) if swap entry has been freed since
		 * our caller observed it.  May fail (-EEXIST) if there
		 * is already a page associated with this entry in the
		 * swap cache: added by a racing read_swap_cache_async,
		 * or by try_to_swap_out (or shmem_writepage) re-using
		 * the just freed swap entry for an existing page.
		 * May fail (-ENOMEM) if radix-tree node allocation failed.
		 */
		err = add_to_swap_cache(new_page, entry);
  #+END_EXAMPLE

  对于注释里说的返回值是ENOENT时,在add_to_swap_cache()里看出是因为swap_duplicate()返回0时
  才会返回ENOENT,在swap_duplicate()里返回0的情况在这里应该是对应的
  swap_info_struct.swap_map[]是0,就是这个slot当前没有被使用

  若add_to_swap_cache()返回的是ENOENT或ENOMEM就返回NULL,就是说若swap entry被释放了或若没有
  足够的内存了,就会返回NULL.
- 成功把页加到swap_space之后就可以调用swap_readpage()来把数据从swap area里读到页里.
** static inline int scan_swap_map(struct swap_info_struct *si)
*** mm/swapfile.c:
- ulk:The scan_swap_map( ) function is used to find a free page slot in a given swap
  area. It acts on a single parameter, which points to a swap area descriptor and returns
  the index of a free page slot.
- 终于知道swap_info_struct.lowest_bit和swap_info_struct.highest_bit是什么意思了,就是在
  swap_info_struct.swap_map[]里比lowest_bit还小的下标是元素都不为空,就是page slot都被占
  了.highest_bit就是比highest_bit大的下标也都被占用了.有了lowest_bit和highest_bit那么就可
  以提高搜索空slot速度.
- 可以执行到got_page标签处就说明offset这个局部变量表示的就是在swap_map[offset]为空
- swap_info_struct.cluster_nr表示从swap_info_struct.cluster_next这个swap_map的下标开始还有
  连续swap_info_struct.cluster_nr个空闲的slot可以使用.
- 在重新获得一个cluster时可以看出一个cluster里的所有page slot都是连续为空的.
  #+BEGIN_EXAMPLE
	/* try to find an empty (even not aligned) cluster. */
	offset = si->lowest_bit;
 check_next_cluster:
	if (offset+SWAPFILE_CLUSTER-1 <= si->highest_bit)
	{
		unsigned long nr;
		for (nr = offset; nr < offset+SWAPFILE_CLUSTER; nr++)
			if (si->swap_map[nr])
			{
				offset = nr+1;
				goto check_next_cluster;
			}
		/* We found a completly empty cluster, so start
		 * using it.
		 */
		goto got_page;
	}
  #+END_EXAMPLE
- 若从cluster_next到cluster_nr都为空闲的话,那么为什么下面的代码要判断swap_map相应的下标是
  否为空呢?
  #+BEGIN_EXAMPLE
	if (si->cluster_nr) {
		while (si->cluster_next <= si->highest_bit) {
			offset = si->cluster_next++;
			if (si->swap_map[offset])
				continue;
			si->cluster_nr--;
			goto got_page;
		}
	}
	si->cluster_nr = SWAPFILE_CLUSTER;
  #+END_EXAMPLE 
- 若在cluster_next到cluster_nr之间没有找到空闲的slot,那么就尝试获取一个cluster
  #+BEGIN_EXAMPLE
	/* try to find an empty (even not aligned) cluster. */
	offset = si->lowest_bit;
 check_next_cluster:
	if (offset+SWAPFILE_CLUSTER-1 <= si->highest_bit)
	{
		unsigned long nr;
		for (nr = offset; nr < offset+SWAPFILE_CLUSTER; nr++)
			if (si->swap_map[nr])
			{
				offset = nr+1;
				goto check_next_cluster;
			}
		/* We found a completly empty cluster, so start
		 * using it.
		 */
		goto got_page;
	}
  #+END_EXAMPLE

  如果分配一个新的cluster不成功,那么就只能在lowest_bit到highest_bit之间一个一个slot地找,直
  到找到一个空的slot
  #+BEGIN_EXAMPLE
	/* No luck, so now go finegrined as usual. -Andrea */
	for (offset = si->lowest_bit; offset <= si->highest_bit ; offset++) {
		if (si->swap_map[offset])
			continue;
  #+END_EXAMPLE 
- 若offset与lowest_bit或highest_bit相等,那么就要相应调整lowest_bit和highest_bit,因为超出
  lowest_bit到highest_bit这个范围的slot一定是被占用的.
  #+BEGIN_EXAMPLE
		if (offset == si->lowest_bit)
			si->lowest_bit++;
		if (offset == si->highest_bit)
			si->highest_bit--;
  #+END_EXAMPLE 
- ulk: If no null entry is found, it sets the lowest_bit field to the maximum index in the
  array, the highest_bit field to 0, and returns 0 (the swap area is full).
  #+BEGIN_EXAMPLE
	si->lowest_bit = si->max;
	si->highest_bit = 0;
  #+END_EXAMPLE 
** swp_entry_t get_swap_page(void)
*** mm/swapfile.c:
- swap_list.head应该指向的是第一个最高优先级的swap area.
- 特别是从下面的这段代码来看,可以看出Round Robin是这样子的:假设所有的swap area都没有可用的
  slot了,一开始是从swap_list.next开始找slot,若跟在swap_list.next这个swap area后面的是同优
  先级的,那么接着下一个swap area就是跟在swap_list.next后面的那个swap area,否则,就会从
  swap_list.head开始把所有的swap area扫描一遍,注意是一遍而已.
  #+BEGIN_EXAMPLE
		if (!wrapped) {
			if (type < 0 || p->prio != swap_info[type].prio) {
				type = swap_list.head;
				wrapped = 1;
			}
		} else
			if (type < 0)
				goto out;	/* out of swap space */
  #+END_EXAMPLE
  当执行到 type = swap_list.head;时就表明同级的已经扫描完毕,而且!wrapped的条件不再成立,再
  加上当type < 0的条件成立时,那么就说明已经扫描完一遍了,应该退出.
** void swap_free(swp_entry_t entry)
*** mm/swapfile.c:
- 这个函数主要是释放entry这个slot,主要是调用swap_entry_free()
** static int swap_entry_free(struct swap_info_struct *p, unsigned long offset)
*** mm/swapfile.c:
- 若slot的计数为SWAP_MAP_MAX的话就什么都不做,因为这个是永久使用的
- 若不是永久使用的,那么就减它的计数
- 减完计数后还要更新lowest_bit,highest_bit,nr_swap_pages,inuse_pages.

** not function
*** 17.4.6
- 使用PG_locked标志来处理同时换入一个共享页和同时换出换入操作.
- 同时换入和换出有点意思:Before being written to disk, each page to be swapped out is
  stored in the swap cache by shrink_list( ). Consider a page P that is shared among two
  processes, A and B. Initially, the Page Table entries of both processes contain a
  reference to the page frame, and the page has two owners; this case is illustrated in
  Figure 17-8(a). When the PFRA selects the page for reclaiming, shrink_list( ) inserts
  the page frame in the swap cache. As illustrated in Figure 17-8(b), now the page frame
  has three owners, while the page slot in the swap area is referenced only by the swap
  cache. Next, the PFRA invokes try_to_unmap( ) to remove the references to the page frame
  from the Page Table of the processes; once this function terminates, the page frame is
  referenced only by the swap cache, while the page slot is referenced by the two
  processes and the swap cache, as illustrated in Figure 17-8(c). Let's suppose that,
  while the page's contents are being written to disk, process B accesses the pagethat is,
  it tries to access a memory cell using a linear address inside the page. Then, the page
  fault handler finds the page frame in the swap cache and puts back its physical address
  in the Page Table entry of process B, as illustrated in Figure 17-8(d). Conversely, if
  the swap-out operation terminates without concurrent swap-in operations, the
  shrink_list( ) function removes the page frame from the swap cache and releases the page
  frame to the Buddy system, as illustrated in Figure 17-8(e).

  从第d个图可以看出,当一个被很多进程共享的页在swap cache里且所有共享该页的进程的相应的页表
  项都指向所在相应的page slot(就是所有共享进程都对该页换出了),如果其中一个进程要访问该页时,就
  会发生缺页异常,因为页还是swap cache,那么就不用从disk里换入了(其实disk也没有这个页的数据,
  只是增加了page slot的引用而已),而是把引起缺页的pte页表项给妀成这个页的物理地址,但这个页
  并没有从swap cache里去掉引用,所以不但pte引用了,还有swap cache的radix树也引用了,同时还有
  一点就是若接下来还有其它共享进程重新访问这个页的内容,那么也会引起缺页异常,所以一个被共享
  的页就算在内存里也可以引起缺页异常.
- when the page is put in the swap cache, both the count field of the page descriptor and
  the page slot usage counters are increased, because the swap cache uses both the page
  frame and the page slot.当一个页放到swap cache时,page._count和相应的
  swap_info_struct.swap_map[]会增加,就算一个页没有真正换出到disk,也可以增加swap_map.
** int add_to_swap(struct page * page)
*** mm/swap_state.c:
- 这个函数把一个页加到swap cache radix树里
- (1)首先这个函数调用get_swap_page()获得一个空闲的slot相应的entry,接着调

  (2)用__add_to_swap_cache()把页插到swap_space radix树里,为什么之前要获得entry呢?因为radix树是
  根据这个entry索引的,调__add_to_swap_cache()时用了GFP_ATOMIC和__GFP_NOMEMALLOC标志来分配
  radix结点.

  (3)最后就是调用设置PG_uptodate和PG_dirty, 设置这两个标志是为了让shrink_list()把这个页写
  到disk
- 所以这个函数只是把一个不在swap cache里的页放到swap cache里,就是让swap cache引用这个页,设
  置了两个标志,而没有把引用这个页的的页表项作处理,也没有把这个页回写到disk.但是已占用了一
  个slot.
- 一旦shrink_list()调用完这个函数,就相用try_to_unmap()处理引用这个页的所有pte页表项,把这些
  页表项都妀为引用这个entry.
** int swap_writepage(struct page *page, struct writeback_control *wbc)
*** mm/page_io.c:
- 这个函数是把在swap cache里的页写到disk里.
- shrink_list()调用完try_to_unmap()之后,就调用pageout(),转而调用address_space.writepage方
  法,这个方法就是用swap_writepage()实现的.
- 调用remove_exclusive_swap_page()看有没有必要真正把一个页的数据回写disk.
- 在调用submit_bio()之前会清掉PG_locked
- shrink_list()成功调用完这个函数之后就是把这个页从swap cache里删除,调用
  delete_from_swap_cache()来删除,因为这个页只有swap cache引用而已,所以这个页也就会相应就被
  回收到伙伴系统.

  那么换入页时page._count又是怎么恢复的呢?应该是通过相应的slot的计数来恢复,每换入一次slot
  的计数就减1,相应的page._count就加1.

  只要swap cache还有引用一个页,那么就说明一定还有进程在访问这个页时会出现缺页异常,所以反过
  来也说明若发生缺页异常,那么这个页要么在swap cache里,要么在swap area里,不能同时在两个地方.
** int remove_exclusive_swap_page(struct page *page)
*** mm/swapfile.c:
- 若这个页没有正在被回写

  且这个页的对应的slot的计数为1

  且在的_count为2(us+cache),

  那么这个页就可以删掉了,因为没有进程引用了.
- Checks whether at least one User Mode process is referencing the page. If not, it
  removes the page from the swap cache and returns 0. This check is necessary because a
  process might race with the PRFA and release a page after the check performed by
  shrink_list().
** static int do_swap_page(struct mm_struct * mm, struct vm_area_struct * vma, unsigned long address, pte_t *page_table, pmd_t *pmd, pte_t orig_pte, int write_access)
*** mm/swapfile.c:
- 返回值是VM_FAULT_MINOR表示次缺页,VM_FAULT_MAJOR表示主缺页.
- 从之前的分析可以得出,就算被请求的页已经在内存,也有可能发生缺页异常,所以要先调用
  lookup_swap_cache()来看看swap cache里是不是存放这个页.
- 若页不在swap cache里,那就一个在disk里的swap area里,这时就要读disk了,
  #+BEGIN_EXAMPLE
	page = lookup_swap_cache(entry);
	if (!page) {
 		swapin_readahead(entry, address, vma);
 		page = read_swap_cache_async(entry, vma, address);
		if (!page) {
			/*
			 * Back out if somebody else faulted in this pte while
			 * we released the page table lock.
			 */
			spin_lock(&mm->page_table_lock);
			page_table = pte_offset_map(pmd, address);
			if (likely(pte_same(*page_table, orig_pte)))
				ret = VM_FAULT_OOM;
			else
				ret = VM_FAULT_MINOR;
			pte_unmap(page_table);
			spin_unlock(&mm->page_table_lock);
			goto out;
		}

		/* Had to read the page from swap area: Major fault */
		ret = VM_FAULT_MAJOR;
		inc_page_state(pgmajfault);
		grab_swap_token();
	}
  #+END_EXAMPLE
  ulk:Invokes the swapin_readahead( ) function to read from the swap area a group of at
  most 2n pages, including the requested one. The value n is stored in the page_cluster
  variable, and is usually equal to 3.[*] Each page is read by invoking the
  read_swap_cache_async( ) function.
  
  swapin_readahead()是预读2^page_cluster个页.

  预读了页但是不能表明所要的页就在预读到的页里,所以还要调用read_swap_cache_async(),因为
  page_cluster有可能为0,或读一组空闲页,或读了一组坏页.

  ulk:Invokes read_swap_cache_async( ) once more to swap in precisely the page accessed by
  the process that caused the Page Fault. This step might appear redundant, but it isn't
  really. The swapin_readahead( ) function might fail in reading the requested pagefor
  instance, because page_cluster is set to 0 or the function tried to read a group of
  pages including a free page slot or a defective page slot (SWAP_MAP_BAD). On the other
  hand, if swapin_readahead( ) succeeded, this invocation of read_swap_cache_async( )
  terminates quickly because it finds the page in the swap cache.
  #+BEGIN_EXAMPLE
 		swapin_readahead(entry, address, vma);
 		page = read_swap_cache_async(entry, vma, address);
  #+END_EXAMPLE 
- 如果调用了read_swap_cache_async()还读不到想要的页,那么就说明这个页有可以被其它的进程换入
  到swap cache里了,为什么会出现在种情况呢?因为在预读页之前解了mm.page_table_lock锁,所以是
  有可能被其它进程换入的,这时应该返回VM_FAULT_MINOR.

  但还有一种可能会读不到所要的页,就是在没有足够的内存空间来存放换入的页时,这时应该返回
  VM_FAULT_OOM.

  #+BEGIN_EXAMPLE
 		page = read_swap_cache_async(entry, vma, address);
		if (!page) {
			/*
			 * Back out if somebody else faulted in this pte while
			 * we released the page table lock.
			 */
			spin_lock(&mm->page_table_lock);
			page_table = pte_offset_map(pmd, address);
			if (likely(pte_same(*page_table, orig_pte)))
				ret = VM_FAULT_OOM;
			else
				ret = VM_FAULT_MINOR;
  #+END_EXAMPLE
  如果真的是从disk里读进了页,那么下面还要做很多事情.
- 先调用mark_page_accessed()把这个页标记一下被访问了
  #+BEGIN_EXAMPLE
	mark_page_accessed(page);
  #+END_EXAMPLE 
- 还是之前解mm.page_table_lock的问题
  #+BEGIN_EXAMPLE
	spin_lock(&mm->page_table_lock);
	page_table = pte_offset_map(pmd, address);
	if (unlikely(!pte_same(*page_table, orig_pte))) {
		ret = VM_FAULT_MINOR;
		goto out_nomap;
	}
  #+END_EXAMPLE 
- 原来被过判断PG_uptodate来确定是不是发生了总线错误
  #+BEGIN_EXAMPLE
	if (unlikely(!PageUptodate(page))) {
		ret = VM_FAULT_SIGBUS;
		goto out_nomap;
	}
  #+END_EXAMPLE 
- 因为都已经是确定从disk里读到了所要的页,那么就可以释放这个slot了
  #+BEGIN_EXAMPLE
	swap_free(entry);
  #+END_EXAMPLE 
- 稍微控制一下swap cache的空闲页数量,少于50%就要调用remove_exclusive_swap_page()从swap
  cache删除一些没有进程引用的页.
  #+BEGIN_EXAMPLE
	swap_free(entry);
	if (vm_swap_full())
		remove_exclusive_swap_page(page);
  #+END_EXAMPLE 
- 被换入的页的原来的标志和权限是怎么保存的呢?其实是这样的,至于读写这些权限是与页所在内存区
  的权限有关,所以也不用特意去保存,
  #+BEGIN_EXAMPLE
	pte = mk_pte(page, vma->vm_page_prot);
  #+END_EXAMPLE 
- ulk:if the access that caused the fault was a write and the faulting process is the
  unique owner of the page, the function also sets the Dirty flag and the Read/Write flag
  to prevent a useless Copy On Write fault.

  #+BEGIN_EXAMPLE
	if (write_access && can_share_swap_page(page)) {
		pte = maybe_mkwrite(pte_mkdirty(pte), vma);
		write_access = 0;
	}
  #+END_EXAMPLE 

  can_share_swap_page()是不是说明只有一个进程是访问这个页的呢?应该是.因为只有一个进程在访
  问,所以就不用COW了.
- 设置完pte页表项之后就是把这个匿名页插入到反向链表里.调用page_add_anon_rmap()
- 到了这里若进程还是以写方式访问该页,那么就要用do_wp_page()来处理COW了


  
