#+STARTUP: showall
* question/answer:
- 因为中断的处理函数在内核态运行，又因为中断可以嵌套的，如果一连续有行验中断产生，那么会不
  会把内核栈给爆了？应该不会,因为产生某个中断之后系统会禁止相应的中断.
- 内核线程用谁的栈呢？应该是当前进程的,再次回到用户态的时候线程已经执行完了,所以内核栈不用
  因为再次回到用户态的时候把内核栈保存下来.但像ksoftirqd()这样的内核线程会调用schedule(),在
  内核空间用schdule()会切换进程,那原来的硬件上下文保存在那里了呢?什么时候会再用呢?
- 进程0最后执行cpu_idle()
- linux2.6有exit_group()和_exit()系统调用来终止一个用户程序。前者用do_group_exit()实现，C库
  的exit()调用，后者用do_exit()实现，如C库的pthread_exit()调用。
- TGID，PGID，SID这几个hash表的主要目的是让你容易通过一个领头进程找到它的线程组中的其它线程，
  进程组中的其它进程，对话中的其它进程。
- 一个中断号一个irq_desc_t结构体。
- irq_desc_t里的depth表示这个中断被禁止了多少次，但是Linux经常是禁止所有中断，为了效率不能
  在这种情况一个一个中断来禁止。
- arm的用arch/arm/kernel/irq.c的，里面有disable_irq()之类的，i386用kernel/irq/manage.c也有
  disable_irq()之类的，但是disable_irq()的实现是一样的。request_irq()也有类似的情况.
- 中断丢失是发生要多处理器中的。CPU A在应答一个中断线前被CPU B禁止了相同的中断，但是CPU A在
  应答之后执行do_irq()，但在do_irq()里会检查IRQ_DISABLE,所以do_irq()不会往下执行了。
- SA_INTERRUPT 这个标志指的是在禁止本地上所有的中断，而不是自已本身。
- IRQ_PER_CPU:该IRQ只能发生在一个cpu上
  IRQ_LEVEL:该中断由电平触发
  IRQ_MASKED:屏蔽了其他中断
- irq_desc->handler和irqaction->handler老是混淆.
- set_current_state这个函数不就只是改变进程的状态吗?并没有对运行队列做出相应的操作.这有什么
  用呢?好像在schedule()里会检测这个标志.
- ULK有句这样的话被我误解了:Right after switching from User Mode to Kernel Mode, the
  kernel stack of a process is always empty, and therefore the esp register points to the
  byte immediately following the stack.我以为从内核态切回用户态后就会把内核栈给清掉了,原来
  不是,只是说把esp给改变了,当这个进程再切回到内核态时就会用回原来在内核态的数据,如调用read
  函数时可能会休眠以致切换到其它进程.
- ret_from_intr()和ret_from_exception()时若有可能会进行调度。
- ULK:a preemptive kernel differs from a nonpreemptive kernel on the way a process running
  in Kernel Mode reacts to asynchronous events that could induce a process switchfor
  instance, an interrupt handler that awakes a higher priority process. We will call this
  kind of process switch a forced process switch。linux不能在中断里进行进程切换。那么不在定
  时器中断里进行切换那么它是怎么做到分时的呢？
- ULK：Both in preemptive and nonpreemptive kernels, a process running in Kernel Mode can
  voluntarily relinquish the CPU, for instance because it has to sleep waiting for some
  resource. We will call this kind of process switch a planned process switch.非抢占不是说
  不能在内核态进行进程切换。
- schedule()可能因为preempt_count设了PREEMPT_ACTIVE而会不调度，是不是说明调用schedule()有可
  能不会切换进程的可能呢？
- 对于多核处理机，使用PREEMPT_ACTIVE实现禁止抢占是没有什么作用的，不能防止其它CPU防问临界区，
  仅仅是不能进行进程切换而已，要用自旋锁之类的.不是这样的，还是有用的，不禁止抢占会
- ULK：kernel preemption may happen either when a kernel control path (usually, an
  interrupt handler) is terminated, or when an exception handler reenables kernel
  preemption by means of preempt_enable( )，kernel preemption may also happen when
  deferrable functions are enabled.所以有三个地方是内核固定会调度的。
- 感觉PREEMPT_ACTIVE实现的抢占在单核上就像一个LBK，在多核中又没什么作用。
- ULK:spin locks are usually convenient, because many kernel resources are locked for a
  fraction of a millisecond only;therefore, it would be far more time-consuming to release
  the CPU and reacquire it later.
- ULK:In the case of a uniprocessor system, the locks themselves are useless, and the spin
  lock primitives just disable or enable the kernel preemption.
- 可以确定设置PREEMPT_ACTIVE是不能/会抢占的意思。
- 通过一个虚拟地址访问物理地址时，是不是先判断这时的权限是什么，是内核权限呢还是用户权限？
  如果是内核权限那么就减去PAGE_OFFSET就是内核地址，如果是用户权限那么就用页表找到物理地址？
  是这样子的吗？
- 每个进程都有自已的页表，是吗？应该是的，若只有一个页表，那么不同的进程使用不同的线性地址
  时会访问到相同的物理地址。
- 内核访问内存时是不通过MMU的吗？MMU是怎么被使用的呢？使用MMU是要有一个寄存器指向全局页表的
  物理地址，当进程访问内存时（读写内存），MMU就会用虚拟地址和那个寄存器找到具体的物理地址。
  所以在打开MMU之前要把页表准备好，之后再把全局页表的地址给那个寄存器。
  www.cnblogs.com/leaven/archive/2011/04/18/2019696.html
- http://daimajishu.iteye.com/blog/1080667 这里有说到进程切换时会把那个存放进程全局页表地址
  的寄存器修改，若不使用MMU就不修改。当系统访问一个32位虚拟地址时，假设cache未命中且TLB（页
  地址快表）中未存有这个地址页的情况下，MMU将全局页目录存放PGD的地址读出来进行寻址。ARM存储
  体系支持的页的大小有几种——1M,64KB,4KB,1KB，支持的二级页表大小有两种：粗粒度和细粒度。
- 虽然内核的线性地址与物理地址是线性关系，差了PAGE_OFFSET,但是这不表明访问内核内存空间时不
  使用MMU。也因为是线性关系，所以内核在它自已的空间里如果线性地址是连续的，那么对应的物理地
  址也是连续的。
- 内核映像并不是可直接运行的目标代码，而是一个压缩过的zImage（小内核）。但是，也并非是
  zImage映像中的一切均被压缩了，映像中包含未被压缩的部分，这部分中包含解压缩程序，解压缩程
  序会解压缩映像中被压缩的部分。zImage使用gzip压缩的，它不仅仅是一个压缩文件，而且在这个文
  件的开头部分内嵌有gzip解压缩代码。http://blog.chinaunix.net/uid-12567959-id-160966.html
- 如何保证进程不能访问内核数据？http://daimajishu.iteye.com/blog/1080667 有回答。虚拟内存到
  物理内存的映射真的能保证系统安全稳定吗？也有回答
- x86在进程切换*前*确实会有类似 movl 新的页表地址，cr3 的处理，新的页表地址即是新的进程的页
  表基地址。这一点不会有问题，因为此时CPU仍然在系统空间运行，而所有进程的页面目录中与系统空
  间相对应的目录项都指向相同的页面表，所以，不管换上哪一个进程的页面目录都一样，受影响的只
  是用户空间，系统空间的映射则永远不变。http://bbs.csdn.net/topics/100049760
- 所有进程的pgd里的768-1023都是一样，都是拷贝swapper_pg_dir的，且大小刚好是
  1G.http://bbs.csdn.net/topics/100049760 这个帖子讨论了内核空间使用不使用MMU，和
  PAGE_OFFSET的关系。我觉得访问内核空间也要用MMU的,为什么要PAGE_OFFSET呢？我觉得这个可以使
  得在系统启动初始化页表时更方便且在一些时候也方便，如可以在内核DUMP时打印的信息可以判断出
  错的内存空间是否在与内核相关。
- 若是内存小于896的话，虽然这些内存都可以映射到内核空间，但肯定不可能全映射到内核空间去的，
  如果可以确定一个地址是内核空间的线性地址，那么可以用它减去PAGE_OFFSET来获得物理地址，但若
  是用户空间的地址，那么就算是小于896的内存也不能用它减去PAGE_OFFSET获得物理地址。
- 页描述符有一个成员叫_mapcount： Number of Page Table entries that refer to the page
  frame (-1 if none).可以看出一个页框可以被多个Page Table页表项映射。

** struct pid * fastcall find_pid(enum pid_type type, int nr)
- 没办法，还是要用遍历
** int fastcall attach_pid(task_t *task, enum pid_type type, int nr)
- 注意所有的成员都要设置好。
** static fastcall int __detach_pid(task_t *task, enum pid_type type)
- 返回值被detach_pid使用得有点巧妙。
** void fastcall detach_pid(task_t *task, enum pid_type type)
- 比__detach_pid ()多一个功能就是把从pid位图里把nr删掉。
** task_t *find_task_by_pid_type(int type, int nr)
** void switch_exec_pids(task_t *leader, task_t *thread)
- 一个非领头线程调用sys_execve()时就调用它。
** void __init pidhash_init(void)
** void __init pidmap_init(void)
- 主要是做0号进程的工作。只分配一页。
** int alloc_pidmap(void)
- 在这里也分配页给page
- alloc_pidmap里的求max_scan的方法为什么要减!offset呢?因为若不在一页的起始位置就要减去0而不是1是因为想多循环一次当前页，所以max_scan指的是
  将要经过多少次页头（页尾）.
** int default_wake_function(wait_queue_t *curr, unsigned mode, int sync, void *key)
- 仅仅是调用try_to_wake_up
- key参数没有用
- 所以自定义唤醒函数时,里面可以一开始调用default_wake_function,在最后做一些想做的事情,而不
  能在一开始做想做的事情因为进程还没有切换.不是这样的,无论在default_wake_function之前还是之
  后所做的事都是在被切换的进程的上下文中进行的.
** int autoremove_wake_function(wait_queue_t *wait, unsigned mode, int sync, void *key)
- 调用上一个函数后从链表中删除,删除用list_del_init,与__remove_wait_queue所用的list_del不一
  样,为什么是在调用上一个函数之前而不是之后呢?被try_to_wake_up()之后没有从等待队列里删除会
  不会又再次唤醒呢?
** #define DEFINE_WAIT(name)
- 用了上一个函数作为唤醒函数。
- 若用这个定义一个WAIT,因为用上一个函数作为唤醒调用函数,所以同时会把它从队列删除.唤醒之后不
  用再把它从队列删除.
** static inline void init_waitqueue_func_entry(wait_queue_t *q, wait_queue_func_t func) 
*** include/linux/wait.h:
- 可以自定义唤醒函数。仅此而已,没有赋值给task
** static inline void init_waitqueue_entry(wait_queue_t *q, struct task_struct *p)
*** include/linux/wait.h:
- 与上一个比多了初始化进程。但唤醒函数用default_wake_function, flags都是0
** #define DECLARE_WAITQUEUE(name, tsk)
*** include/linux/wait.h:
- 注意与DEFINE_WAIT的不同，用tsk,default_wake_function,NULL和NULL初始task_list,而不是
  current,autoremove_wake_function,LIST_HEAD_INIT
- 那么用DECLARE_WAITQUEUE定义的要不要在删除的时候把它从链表删除呢？要的,用
  remove_wait_queue，在ulk里也有说的:unless DEFINE_WAIT or finish_wait( ) are used, the
  kernel must remove the wait queue element from the list after the waiting process has
  been awakened.
** #define DECLARE_WAIT_QUEUE_HEAD(name)
*** include/linux/wait.h:
- 用自已来初始化链表.
** static inline void init_waitqueue_head(wait_queue_head_t *q)
*** include/linux/wait.h:
- 结果和DECLARE_WAIT_QUEUE_HEAD(name)一样.
** static inline int waitqueue_active(wait_queue_head_t *q)
*** include/linux/wait.h:
- 看队列是否为空
** static inline void __add_wait_queue(wait_queue_head_t *head, wait_queue_t *new)
*** include/linux/wait.h:
- 这个是加在队列前面的
** static inline void __add_wait_queue_tail(wait_queue_head_t *head, wait_queue_t *new)
*** include/linux/wait.h:
- 这个是加在队列尾的
** void fastcall __sched sleep_on(wait_queue_head_t *q)
*** kernel/sched.c:
- 就是改状态,加入队列,schedule,删除队列. 要注意加锁.
- sleep_on系列的函数是与等待队列相关的.
- 时间窗口出现在改状态和schedule之间可能会被唤醒.
- the sleep_on( )-like functions cannot be used in the common situation where one has to
  test a condition and atomically put the process to sleep when the condition is not
  verified; therefore, because they are a well-known source of race conditions, their use
  is discouraged.
** long fastcall __sched sleep_on_timeout(wait_queue_head_t *q, long timeout)
** long fastcall __sched interruptible_sleep_on_timeout(wait_queue_head_t *q, long timeout)
** void fastcall __sched interruptible_sleep_on(wait_queue_head_t *q)
** void fastcall prepare_to_wait(wait_queue_head_t *q, wait_queue_t *wait, int state)
*** include/linux/wait.h:
- 这个用于把current加入等待队列的。
- 注释有说为什么把设置进程状态放在加入队列的后面
- 要先判断wait->task_list为空的时候才把wait加入队列。为什么在sleep_on里不用呢?因为
  prepare_to_wait的应用场合不同，prepare_to_wait会放在一个循环里重复调用，但是finish_wait不会被放到循环里，看看__wait_event就知道了。
- 虽然在is_sync_wait里会检查wait是否为空，但进入prepare_to_wait是肯定不会为空的，所以is_sync_wait做了多余的事情。
** #define is_sync_wait(wait)	(!(wait) || ((wait)->task))
*** include/linux/wait.h:
- 有一段注释：Used to distinguish between sync and async io wait context: sync i/o typically specifies a NULL wait queue entry or a wait
  queue entry bound to a task (current task) to wake up. aio specifies a wait queue entry with an async notification
  callback routine, not associated with any task.为什么同步io可以指定一个NULL 的wait呢？
** void fastcall prepare_to_wait_exclusive(wait_queue_head_t *q, wait_queue_t *wait, int state)
*** include/linux/wait.h:
- 不同的是设置了exclusive标志。
** void fastcall finish_wait(wait_queue_head_t *q, wait_queue_t *wait)
*** kernel/wait.c:
- 用了list_empty_careful，为什么呢？只能用于调用list_del_init的情况，因为list_del_init里调用了INIT_LIST_HEAD
- sleep_on是状态->插入队列->schedule->删除队列;插入队列(prepare)（检测是否已插入）->状态
  (prepare)（检查同步）->schedule->状态(finish)->删除队列(finish)(先list_empty_careful)
- 有一个例子：
#+BEGIN_EXAMPLE
    DEFINE_WAIT(wait);
    prepare_to_wait_exclusive(&wq, &wait, TASK_INTERRUPTIBLE);
                                /* wq is the head of the wait queue */
    ...
    if (!condition)
        schedule();
    finish_wait(&wq, &wait);
#+END_EXAMPLE
** #define wake_up(x)			__wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, 1, NULL)
*** kernel/sched.c:
- 要知道linux是不能指定下一个切换到某个进程。
- wake_up也不能指定唤醒某个进程（把某个进程状态改成运行），注意只有一个参数x，但是找到一个
  被唤醒的进程后就会马上调用它的func，因为大部分的func是default_wake_function，会调用
  try_to_wake_up
- 等待队列是从第一个开始唤醒的，一个wait可以加入到队列头add_wait_queue也可以加到队列尾
  add_wait_queue_tail，同时还有互斥和非互斥的wait，所以可以用这些东西组合成一个有优先级的队
  列。

** #define wake_up_nr(x, nr)		__wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, nr, NULL)
** #define wake_up_all(x)			__wake_up(x, TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE, 0, NULL)
** #define wake_up_interruptible(x)	__wake_up(x, TASK_INTERRUPTIBLE, 1, NULL)
** #define wake_up_interruptible_nr(x, nr)	__wake_up(x, TASK_INTERRUPTIBLE, nr, NULL)
** #define wake_up_interruptible_all(x)	__wake_up(x, TASK_INTERRUPTIBLE, 0, NULL)
** #define wake_up_locked(x)		__wake_up_locked((x), TASK_UNINTERRUPTIBLE | TASK_INTERRUPTIBLE)
- 已经把队列给lock住了
** #define wake_up_interruptible_sync(x)   __wake_up_sync((x),TASK_INTERRUPTIBLE, 1)
** void fastcall __wake_up_sync(wait_queue_head_t *q, unsigned int mode, int nr_exclusive)
*** kernel/sched.c:
- 这个函数目前为止只是用于上一个宏，所以nr_exclusive一直是1，但是在实现的时候nr_exclusive为0的时候就不同步了，为什么呢？
** static void __wake_up_common(wait_queue_head_t *q, unsigned int mode, int nr_exclusive, int sync, void *key)
- sync这个参数是只是传给func而已。
- 如果想唤醒所有的进程而不管它是否互斥，那么nr_exclusive就是0，实现的方法是!--nr_exclusive
** clone
- 这个是C的库函数，它有多个参数但是它调用的sys_clone只有一个参数，转而调用的do_fork有多个参
  数。但是ARM又是不一样的，它的包含了很多参数。
- 关于fn和arg参数在ULK有：the wrapper function saves the pointer fn into the child's stack
  position corresponding to the return address of the wrapper function itself; the pointer
  arg is saved on the child's stack right below fn.
** fork
- 也是一个C库函数。
- ULK:The traditional fork( ) system call is implemented by Linux as a clone( ) system
  call whose flags parameter specifies both a SIGCHLD signal and all the clone flags
  cleared, and whose child_stack parameter is the current parent stack pointer. Therefore,
  the parent and child temporarily share the same User Mode stack.总之比clone就多了一个
  SIGCHLD和与父进程共用一个堆栈.
** vfork
- 也是一个C库函数。
- ULK:The vfork( ) system call, introduced in the previous section, is implemented by
  Linux as a clone( ) system call whose flags parameter specifies both a SIGCHLD signal
  and the flags CLONE_VM and CLONE_VFORK, and whose child_stack parameter is equal to the
  current parent stack pointer.总之比fork就多了CLONE_VM和CLONE_VFORK
** long do_fork(unsigned long clone_flags, unsigned long stack_start, struct pt_regs *regs, unsigned long stack_size, int __user *parent_tidptr, int __user *child_tidptr)
*** kernel/fork.c:
- 注意参数的意思
- 如果clone_flags和current->ptrace的符合某些条件时，就算clone_flags不设置CLONE_PTRACE也给它加上。主要是看current_ptrace的设置。
- 关于调用ptrace_notify在ULK有这样说：If the parent process is being traced, it stores the
  PID of the child in the ptrace_message field of current and invokes ptrace_notify( ),
  which essentially stops the current process and sends a SIGCHLD signal to its
  parent. The "grandparent" of the child is the debugger that is tracing the parent; the
  SIGCHLD signal notifies the debugger that current has forked a child, whose PID can be
  retrieved by looking into the current->ptrace_message field.
- 在调用ptrace_notify时的参数在里面被赋给了task_struct->exit_code，为什么要这样呢？也赋给了
  si_code,这还可以理解
- 调的ptrace_notify的原因：因为父进程被跟踪而且要求被创建的子进程也要被跟踪，所以就调用了。
  调用完这个函数的过程中因调用schedule(do_notify_parent_cldstop())所以会停下。
- 这个函数是在哪里真正创建一个进程并开始以两个执行路径运行的呢？好像整个进程都是以current来
  运行的。那创建的子进程在什么时候运行呢？可能是在copy_process函数里把它插入到了某个运行队列里了。
- 若CLONE_VFORK设置了要在ptrace_notify之后才可以等待，子进程运行完。
** static inline int fork_traceflag (unsigned clone_flags)
*** kernel/fork.c:
- 在clone_flags里的最低8位是指定退出时所要发送的信号。
- 若系统调用是由vfork发起的且想跟踪vfork发起的创建的子进程就返回PTRACE_EVENT_VFORK;若子进程
  退出时所发的信号不是SIGCHLD(为什么要这个条件呢？)且想跟踪clone创建的子进程就返回PTRACE_EVENT_CLONE；若想跟踪由
  fork创建的子进程就返回PTRACE_EVENT_FORK.
- 为什么是CLONE_VFORK是要使用completion原语呢？因为vfork的man手册有一段这样的话：vfork()
       is a special case of clone(2).  It is used to create new processes without copying
       the page tables of the parent process.  It may be useful in performance-sensitive
       applica‐ tions where a child is created which then immediately issues an
       execve(2)vfork() differs from fork(2) in that the parent is suspended until the
       child terminates (either normally, by calling _exit(2), or abnormally, after
       delivery of a fatal signal), or it makes a call to execve(2).  Until that point,
       the child shares all memory with its parent, including the stack.  The child must
       not return from the current function or call exit(3), but may call _exit(2).
       Signal handlers are inherited, but not shared.  Signals to the parent arrive after
       the child releases the parent's memory (i.e., after the child terminates or calls
       execve(2)).
- CLONE_STOPPED:Forces the child to start in the TASK_STOPPED state.
- 若设置了CLONE_STOPPED,为什么还要设置PT_PTRACE才可以添加SIGSTOP的信号呢?
** void fastcall wake_up_new_task(task_t * p, unsigned long clone_flags)
*** kernel/sched.c:
- 再次说一下task_t->array是指向CPU运行队列里的某一个active或expire成员.
- 如何通过一个task_t来获得一个运行队列:从task_t里的thread_inof里的CPU来找到task是在哪一个
  CPU上,知道哪个CPU就可以找出相应的运行队列了.task_t里的run_list就是task_t->array链表里的一
  个结点.
- 会根据是否共用相同的VM和是否在同一个CPU来插入进程和父进程的相对位置。
- __activate_task会使用enqueue_task来把进程插入到相应的运行队列尾，而不是头。
- 在这个函数里current->array会有空的时候,是什么时候呢？
- 为什么不共享VM就要子进程运行先呢？注释有说明是因为可能运行exec，那么是不是子进程运行
  exec后会把所有的原来的VM删掉呢？
- 好像在cpu==this且CLONE_VM清除且current->array不为空时的情况下没有设置array->bitmap,这个是
  一个bug吗？在这里为什么要把子进程的prio设置成父进程的prio呢？难道仅是为了想在父进程之前运
  行而把它放在程父进程相同优先级的运行队列中？
- 它的this_rq为什么不是通过task_rq_lock来获取的呢？而是根据cpu==this_cpu来判断的呢？
- 为什么在不是同一个CPU时要重新计算timestamp呢？计算的方法是减去父进程所在运行队列的
  timestamp_last_tick再加上子进程所在运行队列的timestamp_last_tick
- __activate_task这个函数里会把进程加入到运行队列里的，虽然名子看起来不是这样子的，但结合参
  数一起还是可以看来的。
- 把一个进程加入到另外一个CPU之后还要看那个CPU需不需要重新调度。实现很简单，用被加入的进程
  的优先级（动态优先级）与CPU上的运行队列里的curr->prio比较即可。
- 为什么不实现CONFIG_SMP版和非CONFIG_SMP版的呢？像resched_task那样。
- set_need_resched()和resched_task()不一样的，前者只是设置了current的标志，而后者会让其它
  CPU的进程重新调度。
- 为什么要把current的运行队列锁住呢？
- 这个函数只有do_fork调用而已
** void ptrace_notify(int exit_code)
*** kernel/signal.c:
- ULK有解释：ptrace_notify( ), which essentially stops the current process and sends a
  SIGCHLD signal to its parent.
- si_signo的是SIGTRAP，且调的的ptrace_stop函数里的do_notify_parent_cldstop是用相应的
  CLD_TRAPPED
- 在这个函数里建立的siginfo_t是在ptrace_stop函数被放到last_siginfo里，在
  do_notify_parent_cldstop里建立的siginfo_t是发给跟踪进程的.
- 调的这个函数因ptrace_stop的schedule，所以可能会被调度.是在do_notify_parent_cldstop里唤醒
  父进程,在ptrace_stop调度。
- current重新可以运行后是马上看有没有挂起的进程。为什么这之前要把last_sigpending清掉呢？
** static void ptrace_stop(int exit_code, int nostop_code, siginfo_t *info)
*** kernel/signal.c:
- 这个函数的作用应该是在current被跟踪时用来停止current的,并通知跟踪进程
- 不知道为什么要自减group_stop_count
- task_t->last_siginfo是给跟踪用的,保存的东西有什么用呢?
- 为什么要设置current->exit_code呢?
- 把current的状态改成TASK_TRACED之后没有把它从运行队列中删除吗？
- 函数里是先解siglock的锁再加上siglock的锁
- 为什么那个PT_ATTACHED要非呢?好像不对的吧
- current->parent->signal不等于current->signal是不是说明current与current->parent不在同一个
  线程组呢?
- 为什么要各种条件不成立的时候再次设置进程为TASK_RUNNING状态呢？有注释说是跟踪进程已不在了。
** static void do_notify_parent_cldstop(struct task_struct *tsk, struct task_struct *parent, int why)
*** kernel/signal.c:
- 重新认识一下struct siginfo_t,结构体里的联合体用得有技巧，因为_kill->_pid和_sigchld->_pid
  的地址相同，_kill->_uid和_sigchld->_uid地址相同，所以只需提供访问_kill或_sigchld里其
  中_pid和_uid即可，所以只提供了访问了_kill->_uid和_kill->_pid的宏而没有_sigchld.si_errno是
  The error code of the instruction that caused the signal to be raised, or 0 if there was
  no error。si_code是A code identifying who raised the signal 。si_status是exit code.不知道
  si_utime和si_stime有什么用，保存什么的，但是它们分别被赋tsk->utime和tsk->stime,原来ULK里
  列出的si_code只是一部分，还有一些SIGILL类、SIGFPE类、SIGSEGV类、SIGBUF类的、SIGTRAP类的、
  SIGCHLD类的等。每次发信号时都要填这个结构体，在这个函数里因为要给发一个信号所以要填这个结
  构体,又因为发的是SIGCHLD信号，所以要填结构体中的联合体的SIGCHLD结构体。
- CLD_TRAPPED这个表示被跟踪进程被捕获，可能运行到了breakpoint,所以运行要停下来，这时候应该
  给跟踪进程发一个信号，所以在这个函数里要检查CLD_TRAPPED是否被置。但是为什么也要检查
  CLD_CONTINUED.
- 为什么是CLD_STOPPED时要这样设置si_status呢？是CLD_TRAPPED时要这样设置si_status呢?
- 既然要在两个判断之后才使用info为什么花那么多时间先设置info呢？
- 因为一个子进程只有一个父进程，所以__wake_up_parent函里使用的wait_chldexit等待队列最多只有
  一个进程，这样解释对吗？
- wait_chldexit是给wait4()系统调用用的，但是子进程只给父进程发一个SIGCHLD信号而已，父进程是
  怎么是子进程退出了呢？
- 这个函数的主要功能是什么呢？是给父进程发一个SIGCHLD信号，而且还必须与CLD_CONTINUED、
  CLD_STOPPED、CLD_TRAPPED相关的。
** static task_t *copy_process(unsigned long clone_flags, unsigned long stack_start, struct pt_regs *regs, long stack_size, int __user *parent_tidptr,int __user *child_tidptr, int pid)
*** kernel/fork.c:
- p->user这个成员是一个指针，就是说还是和父进程共用的。这是对的，因为创建子进程时的用户是与
  父进程的用户相同的
- nr_threads在这个函数里增加，在__unhash_process减，__unhash_process被release_task和
  unhash_process调用
- max_threads表示什么意思呢？它在fork_init里被初始化
- 我觉得nr_threads>=max_threads不应该放在nr_threads++之前，如果root调用fork足够多次且每次都
  在nr_threads>=max_threads这个比较之后且在nr_threads++之前那么nr_threads就会大于
  max_threads
- 在这里把tgid设置了pid,就是说创建一个进程时把子进程作一个新线程组的领头进程,是这样子吗?为
  什么呢?但是设置了CLONE_THREAD又不同了.
- 为什么要设置set_child_tid和clear_child_tid呢？clear_child_tid是什么来的，有一句主释：
  Clear TID on mm_release()?
- 创建的子进程是不能跟踪它运行后的系统调用的.
- 把父进程的执行域设置成了自已的执行域,为什么呢?
- 为什么要这样设置exit_signal呢?若没设置CLONE_THREAD,那么用clone_flags里的
- 要设置group_leader为子进程,好像不对的吧.是不对,在后面会因为CLONE_THREAD而做修改的
- 把cpus_allowed设置成与current的一样,设置子进程的cpu与current的一样.
- 若current有一个SIGKILL信号,那么它就不能创建子进程.
- 若设置了CLONE_PARENT或CLONE_THREAD的时候要把子进程的real_parent设置了current的
  reald_parent,为什么有CLONE_THREAD的要这样设置,正真创建子进程的current竟然不是current,
- 刚创建完的子进程的parent与real_parent是一样的.
- 若CLONE_THREAD设置了同是SIGNAL_GROUP_EXIT也设置了,那么是不能创建子进程的.
- 若CLONE_THREAD设置了那么把子进程的group_leader设置成current的group_leader
- SIGNAL_GROUP_EXIT和group_stop_count没有关联的吗?可以不设置SIGNAL_GROUP_EXIT但
  group_stop_count可以大于0?
- 若group_stop_count大于0说明一个全组的停止正在进行,那么就要把正创建的进程加入到停止组中.
- 有一个跟踪进程的被跟踪进程链表(task_struct->ptrace_list, task_struct->ptrace_childen)
- 子进程一定会被插入到PID hash表和TGID hash（tgid在上面被设置成正确的值了）表，但是为什么要
  在子进程为线程组领头进程时才会把子进程插入PGID hash表和SID hash表呢？
- 在这里居然还会检查p->pid是否为0，是否多此一举？
- 每个CPU都有一个进程个数计数器process_counts
- ULK有一句这样的话：If the child is a thread group leader (flag CLONE_THREAD cleared)。就
  是说CLONE_THREAD若不设置那么子进程就是线程组领头进程，若进程是一个线程而不是一个线程组领
  头进程那么它就不是一个进程组的成员，若进程是一个线程组领头进程那么它就是一个进程组的成员，
  那么一个进程怎样才可以成为进程组领头进程呢？
- 关于进程归属的问题总结ULK:若子进程是一个thread group leader(清CLONE_THREAD),就设tgid为
  pid(就是自已),设group_leader为自已(这个有点想不明白,源码的确的这样的.)那么除了把子进程插
  入到TGID,PGID之外还要插入到SID中.若子进程不是一个thread group leader(置CLONE_THREAD),那么
  tgid设为current->tgid(注意不是current,所以线程组不能嵌套),设group_leader为
  current->group_leader并把它插入到TGID中去,但是为什么子进程不是thread group leader了还要插
  入到TGID中呢？哦看错了，源码是这样的attach_pid(p, PIDTYPE_TGID, p->tgid);不是插p而是
  p->tgid那么每创建一个线程时领头线程不是都要被插一次，这点在ULK上表述有错。不是，我错了，
  的确的把子进程插入TGID中。要重新认识一下那4个链表。
** static struct task_struct *dup_task_struct(struct task_struct *orig)
*** kernel/fork.c:
- prepare_to_copy()在i386里用来关闭fpu，在arm里什么也不做。
- 就是分配了task_struct和thread_info并拷贝和设置之间的指针；再设置tsk->usage.
** static inline void copy_flags(unsigned long clone_flags, struct task_struct *p)
*** kernel/fork.c:
- 这个函数是被copy_process调用的，为什么要把PF_SUPERPRIV清掉呢？不能继承父进程的
  PF_SUPERPRIV吗？为什么把PF_FORKNOEXEC给置了？
- 由copy_process传入的clone_flags在do_fork可能对CLONE_PTRACE动了手脚.task_struct->ptrace是
  在这里设置的,在do_fork里用到,为什么不在do_fork做修改呢?隔太远了.
** static int copy_files(unsigned long clone_flags, struct task_struct * tsk)
*** kernel/fork.c:
- 一个后台程序可能不包含任何文件.
- 若设置了CLONE_FILES那么就用current的files,不用改.
- 
** static int count_open_files(struct files_struct *files, int size)
*** kernel/fork.c:
- 这种的计算方法是不是有的粗略了?
** static inline int copy_fs(unsigned long clone_flags, struct task_struct * tsk)
*** kernel/fork.c:
- CLONE_FS:ulk:Shares the table that identifies the root directory and the current working
  directory, as well as the value of the bitmask used to mask the initial file permissions
  of a new file (the so-called file umask ).
** static inline struct fs_struct *__copy_fs_struct(struct fs_struct *old)
*** kernel/fork.c:
- 主要的步骤的分配一个fs_struct再把old里的值拷贝过去
** static inline int copy_sighand(unsigned long clone_flags, struct task_struct * tsk)
*** kernel/fork.c:
- 若是CLONE_sighand或CLONE_THREAD其中一个设置就与父进程共享，即增加计数器就可以了。
- 若不共享，但还要把action的内容拷贝过来，为什么呢？所以无论共享与否都会把共享action
** static inline int copy_signal(unsigned long clone_flags, struct task_struct * tsk)
*** kernel/fork.c:
- 若CLONE_THREAD设置那么就共享进程的，增加计数器即可。
** static int copy_mm(unsigned long clone_flags, struct task_struct * tsk)
*** kernel/fork.c:
- 若current->mm为空的时候就不用拷贝也不分配了，为什么不分配了呢？
- 若设置了CLONE_VM就共享.若不共享,还会把父进程的mm给拷贝过来.再后来又修改了部分成员
** int copy_thread(int nr, unsigned long clone_flags, unsigned long esp, unsigned long unused, struct task_struct * p, struct pt_regs * regs)
*** arch/i386/kernel/process.c:
- 这个函数初始化了task_struct->thread_info结构体,其中有reg成员、栈、ip寄存器，使子进程被调
  度运行时在正确的栈和ip指针下运行。
- ULK:The value returned by the system call is contained in eax: the value is 0 for the
  child and equal to the PID for the child's parent. To understand how this is done, look
  back at what copy_thread() does on the eax register of the child's process.
** void fastcall sched_fork(task_t *p)
*** kernel/sched.c:
- 函数一开始把进程状态设置为运行,但到目前为止还没有把进程插入到运行队列,所以还不会被调度.但
  可以不可以通过把它插入到一个等待队列来获得运行呢?
- 给子进程分时间片时为什么current的时间片要先加1呢?current->time_slice不可能是0;若是1的话,
  如果不加1,那么子进程的时间就是0,又因为current的时间片又会用移位的方式重新调整,所以若是1,
  最后父和子进程的时间片都是0.用这种计算方式无论如何都不会增加和减少原来current的时间片.
- 调整之后的current时间片若为0那么会马上开始定时器的调度.
** NORET_TYPE void do_group_exit(int exit_code)
*** kernel/exit.c:
- SIGNAL_GROUP_EXIT是该线程组中所有的线程都要设置的吗？还是只是调用这个函数的线程才会设置。
  不是这样的，task_struct->signal是线程共享的。
- 这代码实现了退出代码的传递性，就是第一个调用该函数的线程所使用的exit_code参数才会被一值采
  用。
** void zap_other_threads(struct task_struct *p)
*** kernel/signal.c:
- SIGNAL_GROUP_EXIT不是在do_group_exit里设置了吗？为什么又要在这里设置SIGNAL_GROUP_EXIT呢？
  多此一举吗？
- 要把group_stop_count清零。所以group_stop_count是从0开始计数的。
- thread_group_exit不是在do_goup_exit里查过了吗？这里是不是多此一举呢？
- 在copy_process里可以看出线程的group_leader与领头线程的一样。但这里为什么有不相等的情况呢？
  一个线程可以不与领头线程在相同的进程组吗？可以从注释上看出若线程执行execve或类似的东西的
  时候不在同一进程组
- 不在同一进程组时要设exit_signal为-1呢？就算不在同一线程组也要发SIGKILL信号给它。
- 会把被杀线程的SIGSTOP,SIGSTP,SIGTTIN,SIGTTOU的信号删掉。
** void signal_wake_up(struct task_struct *t, int resume)
*** kernel/signal.c:
- ULK:to notify the process about the new pending signal
- 若想恢复执行，那么就不管进程的状态是TASK_INTERRUPTIBLE，TASK_STOPPED还是TASK_TRACED.
- 会调的wake_up_state,转而调用try_to_wake_up来唤配进程。若在其它CPU会发一个CPU间中断让它调
  度。
** fastcall NORET_TYPE void do_exit(long code)
*** kernel/exit.c:
- 在能在中断上下文调的，不能对进程0进程1调用
- 按照字面意思，PT_TRACE_EXIT应该是指跟踪进程退出，就是在退出时通知跟踪进程。
** static void exit_notify(struct task_struct *tsk)
*** kernel/exit.c:
- 有一个调用exit同时该进程被选中来执行一个线程组信号。这时它要找其它线程来处理。
- 有注释：Check to see if any process groups have become orphaned as a result of our
  exiting, and if they have any stopped jobs, send them a SIGHUP and then a SIGCONT.
  (POSIX 3.2.2.2)
- 如果被杀进程的exit_signal不等于SIGCHLD且exit_signal不为空，安全域或执行域被修改且没有
  KILL权限那么改exit_signla为SIGCHLD。为了是让父进程知道子进程已死。
- 进入这个函数之后，退出进程有可能在EXIT_ZOMBIE状态，也有可能在EXIT_DEAD状态，如果进入
  EXIT_DEAD了，那么之后会调用release_task.若没在出退信号且不被跟踪或在进行组退出就是
  EXIT_DEAD,否则是EXIT_ZOMBIE.如果在这里被设置成EXIT_ZOMBIE,那么会在那被设成EXIT_DEAD呢？好
  像是如果在EXIT_ZOMBIE时，就表明等待父进程调用wait类函数。
- 在这里会把forget_original_parent收集到的子进程给release_task掉。
- 在这个函数一定会把tsk->flags设为PF_DEAD，说明无论是EXIT_DEAD还是EXIT_ZOMBIE,那是PF_DEAD.
- 想到一个问题：release_task之后应该是所有的内存空都被收回了，但是为什么被退出的进程还可以
  运行呢？因为内存空间被收回后并没有把内存空间的内容清掉且这些内存没有被分配，因为已把抢占
  给禁止了，所以进程的代码代和数据都没被破坏。好像上面那个解释是错的，因为ULK：The
  release_task( ) function detaches the last data structures from the descriptor of a
  zombie process; it is applied on a zombie process in two possible ways: by the
  do_exit()function if the parent is not interested in receiving signals from the child,
  or by the wait4( ) or waitpid( ) system calls after a signal has been sent to the
  parent. In the latter case, the function also will reclaim the memory used by the
  process descriptor,while in the former case the memory reclaiming will be done by the
  scheduler (see Chapter7).
** static inline void forget_original_parent(struct task_struct * father, struct list_head *to_release)
*** kernel/exit.c:
- ULK：All child processes created by the terminating process become children of another
  process in the same thread group, if any is running, or otherwise of the init process.
- 里面有一个child_reaper是被初始化为init_task的。
- 要处理这个进程的两个进程链表：子进程链表，跟踪进程链表。在task_struct->children里这两种的
  进程那包含了。
- 有注释：If something other than our normal parent is ptracing us, then send it a SIGCHLD
  instead of honoring exit_signal.  exit_signal only has special meaning to our real
  parent.被杀进程要是被跟踪，就发SIGCHLD，否则且exit_signal不为空且所在的线程组没有其它线程
  (thread_group_empty(tsk)是指tsk作为一个线程所在的线程组没有其它线程，不是指以tsk作为线
  程组领头线程的线程组没有其它线程)（为什么要这个条件呢？）
** static inline void reparent_thread(task_t *p, task_t *father, int traced)
*** kernel/exit.c:
- exit_signal等于-1是什么意思,表示没有任何信号，就是初始值，不能用0，这个好像用来测试的。
- 若子进程有退出信号，那么把它改成SIGCHLD,为什么要这样子做呢？
- pdeath_singal是在ULK：The signal sent when the parent dies时候发的。但是发给谁呢？从代码
  看好像是发给自已,从forget_original_parent来看好像发给父进程的.
- 在task_struct里关于跟踪的链表也有两个：ptrace_list和ptrace_children.
- 在这里father是正在退出的进程,在进入这个函数之前real_parent被改了，无论是有没跟踪的。
- 参数traced说明的是父进程有没有被跟踪。
- 如果父进程是被跟踪的且p的parent和real_parent不相同，那么就把p插入到real_parent的
  ptrace_children中,但什么时候父进程是被跟踪的且parent与修改之后的real_parent是相同的呢？好
  像这样是不可能的吧，因为在以参数trace=1调用之前p是在father->ptrace_children链表里的，所以
  p->parent一定是father，又因为在调用reparent_thread () 之前调用了choose_new_parent把
  p->real_parent改成了不可能为father的reaper,所以在reparent_thread里是不可能有
  p->parent==p->real_parent的,如果有可能的话，那么可能是p虽在father->ptrace_children里但是
  p->parent不等于father.
- ptrace不为0是什么意思，为0又是什么意思。
- 如果进程A在进程B的children中，那么进程A的real_parent一定是进程B吗？
- 如果进程A在进程B的ptrace_children中，那么进程A的parent一定是进程B吗？
- 不知道为什么在traced假的时候要设p->ptrace为0而且还把p->parent改为p->real_parent，如果是这
  样的话，那么有可能有这样一种情况：p->parent和p->real_parent不相同(p被p->parent跟踪)且它的
  real_parent正被杀成为father参数且又先以traced=0调用reparent_thread ()，这时会在reparent里
  把p->ptrace设为0，把p->parent设为p->real_parent,但一直没有从p->parent的ptrace_children把
  p删掉，这合理吗？
- 在这个函数里如果发现有僵死进程且有退出信号那么就通知父进程，为什么还要加多一个判断线程组是否为空呢？
- 关于p->state==TASK_TRACED 有注释:If it was at a trace stop, turn it into a normal stop
  since it's no longer being traced.
- 又有不明白了，本来p和father是父子关系，按理说应该在同一个进程组，但是为什么还要判断是否在
  同一个进程组呢？可以这样的。孤儿进程组： 一个进程组中的所有进程的父进程要么是该进程组的一
  个进程，要么不是该进程组所在的会话中的进程。 一个进程组不是孤儿进程组的条件是，该组中有一
  个进程其父进程在属于同一个会话的另一个组中。函数里有一个判断孤立进程组的代码？又有一个问
  题：在同一个进程组里的任意一个进程都与同组中其它至少一个进程有父子或兄弟关系吗？
- 在forget_original_parent里以traced为0调用reparent_thread时的father是p的real_parent,在这种
  情况下，在reparent_thread里会把对p的跟踪去掉，换句话说就是如果一个进程A的父进程
  real_parent被杀掉，那么进程A就不能再被跟踪和去跟踪了，因为p->ptrace被清和p->parent设为
  p->real_parent。
- 如果进程A的父进程被杀掉，且进程A被跟踪且是EXIT_ZOMBIE状态，且没有退出信号，这种情况为什么
  要收集这些进程呢？
- 在forget_original_parent里以traced为1调用reparent_thread时的father是p的parent（可能与
  real_parent一样），且p->parent不等于p->real_parent,在这种情况下，reparent_thread里会把
  p->ptrace_list插入到p->real_parent->ptrace_children中，但p->real_parent可能不是一个跟踪函
  数，为什么要这样做呢？有这样的注释：Preserve ptrace links if someone else is tracing
  this child.
** void ptrace_untrace(task_t *child)
*** kernel/ptrace.c:
- 也不是一定是切回TASK_STOPPED状态，还要看看是不是有停止信号。
** void release_task(struct task_struct * p)
*** kernel/exit.c:
- 在这里p->ptrace还有可能不为0，这是会调用__ptrace_unlink把p->trace改为0
- 为什么如果这个函数被跟踪就要脱离跟踪呢?而且是在release_task里做
- ULK关于task_struct->parent的说明：this is the process that must be signaled whn the
  child process terminates。
- 看了ULK的一段话:If the process is not a thread group leader, the leader is a zombie, and
  the process is the last member of the thread group, the function sends a signal to the
  parent of the leader to notify it of the death of the process.和看了源码,有一个结
  论:task_struct->group_leader是线程组的领头进程的task_struct.为什么要这样的需求呢?
- 通知已在EXIT_ZOMBIE状态的进程还有用吗?它会做出响应.
- 为什么thread_group_empty(leader)为true时表示被杀进程是最后一个线程,因为被杀进程
  在_unhash_process里被从PIDTYPE_TGID中删除了.
- 在这里调用了put_task_struct回收task_struct了
- 有注释:If we were the last child thread and the leader has exited already, and the
  leader's parent ignores SIGCHLD, then we are the one who should release the leader.所以在
  最后p又会回到函数的开始来把leader删除掉.
** void __ptrace_unlink(task_t *child)
*** kernel/ptrace.c:
- 有一个问题：silbing是一定与real_parent有关系的吗？如果parent与real_parent不相等就和
  parent没有任何关系吗？如果是这样，那为什么还要在这个函数里调同REMOVE_LINKS(child)呢？因为
  REMOVE_LINKS是与real_parent有关的.
- 
** void __exit_signal(struct task_struct *tsk)
*** kernel/signal.c:
- atomic_dec_and_test(v):Subtract 1 from *v and return 1 if the result is zero; 0
  otherwise
- 为什么没有其它进程用signal之后还可以找到next_thread呢？
- 关于group_exit_task有注释： notify group_exit_task when ->count is equal to notify_count；
  everyone except group_exit_task is stopped during signal delivery of fatal signals,
  group_exit_task processes the signal.
- notify_count是通知group_exit_task的阀值，有这种需求吗？
- flush_sigqueue(tsk->pending)是一定的，但是共享的要在signal_struct使用计数为0的时候才flush.
- 如果没有人用signal_struct会在最后回收signal_struct结构体。
** void __exit_sighand(struct task_struct *tsk)
*** kernel/signal.c:
- 这个函数比较简单，没人使用就直接回收。
** static void __unhash_process(struct task_struct *p)
*** kernel/exit.c:
- 减nr_threads, 从PIDTYPE_PID, PIDTYPE_TGID,中删除， 若是线程组领头进程就从PIDTYPE_PGID和
  PIDTYPE_SID中删除，从进程链表中删除。
- 要为线程组领头进程才可以减process_counts,为什么会这样呢?可能创建一个非领头线程时不会增加
  process_coun
- 在这里有可能p->pid为空吗?
** void fastcall sched_exit(task_t * p)
*** kernel/sched.c:
- 用来修改父进程的时间片和平均睡眠时间.注意父进程是parent而不是real_parent
- 有注释:Potentially available exiting-child timeslices are retrieved here - this way the
  parent does not get penalized for creating too many threads.
- 若进程还没有执行完一次，那么把退出的进程的时间片加回到父进程，以防父进程因创建太多的子进
  程后马上退出而使得父进程的时间减少。
- 若进程的平均睡眠时间比父进程的平均睡眠小那么就更新父进程平均睡眠时间
** void disable_irq(unsigned int irq)
*** kernel/irq/manage.c:
- 这个函数要同步的，看是否正在执行这个中断函数。所以进入中断函数之后不能禁止本中断，这可能是
  为什么要在进入该中断函数之前系统会自已禁止中断。
** void disable_irq_nosync(unsigned int irq)
*** kernel/irq/manage.c:
- depth是先判断再增加的
- 这个函数可以多次被调用，仅是在depth上有变化。
** void synchronize_irq(unsigned int irq)
*** kernel/irq/manage.c:
- 这里的这个是在多处理器的情况下实现的，在非多处理器的情况下的实现在
  include/linux/hardirq.h下，就是一个barrier而己。
- 在进行处理中就relax cpu.
** void enable_irq(unsigned int irq)
*** kernel/irq/manage.c:
- 在case 1时不用break,我还以为depth没有减少
- 虽然在case 1时会挽回丢失的中断，但是已经晚了，因为中断不是在被应答之后马上处理的，这种情
  况有点意思，CPU A在接收到中断后接着CPU B才禁止中断，但是因为中断丢失所以要在CPU B禁止之后
  的不确定时间后才执行中断。
- 有IRQ_PENDING和IRQ_DISABLE就表明有中断丢失了
** fastcall unsigned int __do_IRQ(unsigned int irq, struct pt_regs *regs)
*** kernel/irq/handle.c:
- 在这里会增加kstat->irqs[irq],kstat是per_cpu变量。
- 为什么在这个函数里还要检查IRQ_DISABLE和IRQ_INPROGRESS呢？
- 在这个函数里会把IRQ_PENDING清掉，若IRQ_DISABLE或IRQ_INPROGRESS设置了也有可能会把
  IRQ_PENDING设置了。
- 相同的中断处理函数有可能正在别的CPU上执行，为什么不推到接收的那CPU上运行该中断处理
  函数呢？ULK：This leads to a simpler kernel architecture because device drivers'
  interrupt service routines need not to be reentrant (their execution is
  serialized). Moreover, the freed CPU can quickly return to what it was doing, without
  dirtying its hardware cache; this is beneficial to system performance.
- 那个死循环里处理特点是可以在执行这个函数的过程中处理别的CPU刚刚接收到的相同的中断。但是假
  如有多个相同的中断发生，那么只有一个未决的中断,这个未决的中断在handle_IRQ_event里发生（设
  置IRQ_PENDING）有注释：* This applies to any hw interrupts that allow a second instance
  of the same irq to arrive while we are in do_IRQ or in the handler. But the code here
  only handles the _second_ instance of the irq, not the third or fourth. So it is mostly
  useful for irq hardware that does not mask cleanly in an SMP environment.
- 如果IRQ_PER_CPU被设置了,那么就不用加锁了,也不用处理IRQ_PENDING, IRQ_INPROGRESS所有这些标志.
** int request_irq(unsigned int irq, irqreturn_t (*handler)(int, void *, struct pt_regs *), unsigned long irqflags, const char * devname, void *dev_id)
*** kernel/irq/manage.c:
- 是SA_SHIRQ时那么dev_id就不能为空.
- 基本是用参数初始化一个irqaction之后调用setup_irq()
** int setup_irq(unsigned int irq, struct irqaction * new)
*** kernel/irq/manage.c:
- 不能
** void open_softirq(int nr, void (*action)(struct softirq_action*), void *data)
*** kernel/softirq.c:
- 就是在softirq_vec的第nr个元素下设置data和action
** void fastcall raise_softirq(unsigned int nr)
*** kernel/softirq.c:
- 仅是调用了raise_softirq_irqoff
- 执行软中断是有检测点的(local_bh_enable,do_IRQ等),所以raise_softirq之后不会马上进入软中断.
** inline fastcall void raise_softirq_irqoff(unsigned int nr)
*** kernel/softirq.c:
- 调用了__raise_softirq_irqoff.
- 若不在中断上下文且没禁止软中断就马上调用wakeup_softirqd用线程来执行软中断
- ULK:If we're in an interrupt or softirq, we're done (this also catches softirq-disabled
  code). We will actually run the softirq once we return from the irq or softirq.
** #define __raise_softirq_irqoff(nr)
*** include/linux/interrupt.h:
- 主要是用local_softirq_pending获取__softirq_pending来将相应的位置1.
- 所以在pending了某个软中断之后多次raise_softirq()它是没有效果的.
** #define in_interrupt()		(irq_count())
*** include/linux/hardirq.h:
- 这个为1表明是在中断也可能是禁止了中断.
** asmlinkage void do_softirq(void)
*** kernel/softirq.c:
- 若抢占禁止那么一开始就要退出去,禁止枪占时不处理软中断.
** asmlinkage void do_softirq(void)
*** kernel/softirq.c:
- 在raise的时候会把__softirq_pending的位给置了,那在哪里清呢?是在这里.
- 在执行软中断函数的时候是打开软中断的,没有禁止.
- 在这个函数里会把所有pending的软中断都处理掉.
- 轮询完一次后发现又有新的软中断就又重新开始,但重新开始的次数是有限的,当次数到限后还有
  pending的软中断那么就用线程来处理了.
- 因为是轮询整个softirq_vec,所以优先级的区别就没那么大了.
** static int ksoftirqd(void * __bind_cpu)
*** kernel/softirq.c:
- 这个函数只有在被通知退出的时候才会退出,退出的结果是线程也终止.
- 在执行do_softirq()前要禁止抢占,将进程状态设为运行,执行do_softirq()后要使能抢占,将进程状态
  设为可中断,
** int kthread_should_stop(void)
*** kernel/kthread.c:
- 这个函数什么意思呢?
- 这个函数会在内核线程里用循环使用这个函数来判断是否应该退出线程
- 这个函数的实现比较简单，就是把线程停止信息中(struct kthread_stop_info)的停止进程与
  current比较
- 但是只有一个struct kthread_stop_info的全局变量，所有的进程都用这个变量，但多个进程都用这
  个变量的话若是多个进程都要禁止线程执行不是会出问题吗？
- 会不会在多个进程中都会执行相同的线程呢？如在进程A进入内核态时执行了线程C，在线程C还没有被
  退出时切换到了进程B，在进程B进入内核态时又执行了线程C，这时就会有两个进程执行线程C了。关
  键要看kthread_stop()什么时候调用了。
** int kthread_stop(struct task_struct *k)
*** kernel/kthread.c:
- 因为thread_stop_info引用了stask_struct结构体，所以就要调用get_task_struct()
- 在这个函数里用了init_completion()来初始化了kthread_stop_info.done,且调用了
  wait_for_completion(),但是在ksoftirqd这个线程函数里没有相应地调用complete(),也没有设置相
  应的kthread_stop_info.err
** struct task_struct *kthread_create(int (*threadfn)(void *data), void *data, const char namefmt[], ...)
*** kernel/kthread.c:
- 这个函数用来创建一个线程，是用工作队列来运行线程函数的，用completion来同步创建完成而不是
  同步创建开始，其实给工作队列执行的函数还不是线程函数，还是一个中间过程而已。执行的函数是
  keventd_create_kthread,被执行的函数在它的参数create里。
- 为什么要先判断helper_wq呢?因为工作队列的创建也是用到这个函数的,执行工作队列里的那些工作是
  在通过创建一个线程来执行的.但是又因为创建内核线程也是用工作队列来完成的
  (keventd_create_kthread()),所以这个先有鸡还是先有蛋的问题就出来了.
** static void keventd_create_kthread(void *_create)
*** kernel/kthread.c:
- 在这里没有调用线程的执行函数，而是以线程的执行函数来创建一个内核线程，是调用
  kernel_thread()创建内核线程的。
- 创建完成之后要等待创建开始的completion.
- 最后要complete创建远成以通知kthread_create().
** int kernel_thread(int (*fn)(void *), void * arg, unsigned long flags)
*** arch/i386/kernel/process.c:
- 这个函数是与架构相关的，与sys_clone的类似，里面调用了do_fork来创建一个线程，被创建的这个
  线程所执行的函数还不是我们要执行的函数，还是一个系统函数kthread函数。
- 就i386的架构来看，执行的函数是先是kernel_thread_helper,再在里面调用kthread(),返回后而调用
  do_exit(),所以线程函数执行完返回到kthread()，kthread()再返回到
  kernel_thread_helper,kernel_thread_helper再调用do_exit();
** static int kthread(void *_create)
*** kernel/kthread.c:
- 在这个函数里会调用我们所要执行的函数
- 这个内核线程会屏蔽所有的信号。
- 为什么在执行线程函数之前要以TASK_INTERRUPTIBLE把current给切换出去呢？所以也可以看出创建完
  线程还有一点延时才执行线程函数。
- 为什么要在执行schedule()之前complete创建开始完成。
- 因为先调用kthread_should_stop再调用线程函数，所以可能一次都没执行线程函数就终止了线程。
- 所以创建内核线程的函数执行顺序是kthread_create创建一个工作队列的工作执行
  keventd_create_kthread(),keventd_create_kthread()调用与架构相关的
  kernel_thread(),kernel_thread()调用do_fork()来执行kthread(),kthread()里会调用要执行的线程
  函数，kthread()函数会等待线程的终止。
- 执行软中断函数的方式也是以一个独立的软中断线程为载体执行的，以kksoftirqd函数为参数调用
  kthread_create,ksoftirqd()这个函数是被kthread()这个函数调用的，有意思的一点是kthread()使
  用了kthread_should_stop()判断是否退出循环，ksoftirqd()也使用了，但从代码的设计看这是必要的。
** __init int spawn_ksoftirqd(void)
*** kernel/softirq.c:
- 创建当前CPU的软中断线程,要创建多个CPU的ksoftirqd线程时就要调用多次了。
- 先用CPU_UP_PREPARE调用cpu_callback()，再用CPU_ONLINE调用cpu_callback()
- 这些线程是在哪个进程的内核栈中执行的呢?这个进程会不会退出呢?这个线程应该是在init进程的内
  核栈里执行的.好像这些线程又不是用内核栈的?是的,内核线程也一个进程,只不过它在内核运行,一个
  进程的执行怎么会用到其它进程的栈呢?使用内核栈的都是系统调用和中断执行之类的.
- 那么线程是怎样被调度的呢?与用户进程一样吗?
** void kthread_bind(struct task_struct *k, unsigned int cpu)
*** kernel/kthread.c:
- 原来把一个进程绑定到特定的CPU执行是那么容易就实现了的。
- 这个函数的作用就是把线程k绑定到第cpu个cpu上执行.
- 绑定之前要先说线程先停下来.
- 绑定的方法就是设置线程k的cpu再设置cpumask.
** static int __devinit cpu_callback(struct notifier_block *nfb, unsigned long action, void *hcpu)
*** kernel/softirq.c:
- 这个函数只是用来给软中断用的.
- 根据kthread_should_pending()来决定是否退出。
- 为CPU_UP_PREPARE时就创建线程,再绑定到一个CPU上,为CPU_ONLINE时就唤醒一个线程.
- 这个函数创建的线程是调用ksoftirqd这个函数的.软中断只需一个线程就可以了,ksoftirqd里调用
  do_softrirq(),所以它会轮询所有等级的软中断.执行do_softirq()会禁止抢占。
- 每个CPU都有一个执行本地CPU的所有等级软中断的内核线程(ksoftirqd()),而执行HI_SOFTIRQ和
  TASKLET_SOFTIRQ这些等级的软中断时会执行完tasklet函数链表中函数.
** void __init softirq_init(void)
*** kernel/softirq.c:
- 就是打开了TASKLET_SOFTIRQ和HI_SOFTIRQ而已,与其它的软中断无关了,也就这两个软中断是提供给内
  核开发者用的.
- 被添加到这两个软中断函数是用链表的形式关联的.
- struct tasklet_head是struct tasklet_struct里next的头结点.
** void fastcall __tasklet_schedule(struct tasklet_struct *t)
*** kernel/softirq.c:
- 新加入的tasklet是放在链表头的,但执行的时候也是从链表头开始执行的,就是先进后出(堆栈)
- 插入后会raise,就像向一个工作队列里添加一个工作之后马上唤醒处理该工作队列的线程。
** static inline int tasklet_trylock(struct tasklet_struct *t)
*** include/linux/interrupt.h:
- 看tasklet是否在TASKLET_STATE_RUN的状态,主要用来防止一个tasklet在其它CPU被执行,本地CPU不会
  嵌套tasklet吗?执行tasklet函数之前没有禁止抢占.
- 禁止抢占就是禁止进程切换禁止调度，不能禁止中断，在中断里是不能进行调度的。
** static inline void tasklet_unlock(struct tasklet_struct *t)
- 清掉TASKLET_STATE_RUN状态,可以看出tasklet用来防止本地CPU嵌套执行相同的tasklet函数(不是同
  一等级的tasklet)是用tasklet_struct->state是否在TASKLET_STATE_RUN决定的,所以在运行tasklet
  函数时是可以被中断的.
** static inline void tasklet_schedule(struct tasklet_struct *t)
*** include/linux/interrupt.h:
- 要先看这个tasklet是否正在被调度,若是就不能再把这个tasklet插到tasklet的运行链表里了(就是调
  用__tasklet_schedule()),所以正在schedule的函数不能在插入到运行链表中.
** static void tasklet_action(struct softirq_action *a)
*** kernel/softirq.c:
- 这个函数就是TASKLET_SOFTIRQ这个等级所要的执行的函数,这个函数会轮询tasklet_vec这个链表中所
  有的tasklet函数.
- 每执行完这个函数就会把所有的tasklet函数执行完,再等到下一次执行TASKLET_SOFTIRQ这个等级的软
  中断时就又会执行这个函数.
- tasklet_trylock(),tasklet_unlock(),tasklet_unlock_wait()这些函数是与
  TASKLET_STATE_SCHED/RUN有关的,tasklet_disable_nosync(),tasklet_disable(),tasklet_enable()这
  些是与tasklet_struct->count有关的,为什么要两个标志呢?前者是用来防止同一个tasklet在多个
  CPU执行的,而count可以防止同一个tasklet在同一个CPU嵌套执行,因为执行一个tasklet时是没有禁止
  中断的,所以不用count的话可能会同一个tasklet在同一个CPU嵌套执行(这个说法错误,同一个
  tasklet是不可能被嵌套执行的),使用count是可以使得即使某个tasklet被插入了也可以使它不被执
  行.两者的区别可能是另一种情况：TASKLET_STATE_SCHED/RUN表明的是tasklet有没有运行，所以
  tasklet_trylock(),tasklet_unlock(),tasklet_unlock_wait()这些只能用于执行tasklet的前后，而
  tasklet_struct->count是用来显示禁止tasklet执行的,不管在TASKLET_STATE_SCHED还是在
  TASKLET_STATE_RUN，同时tasklet_disable()会调用tasklet_disable_nosync()和
  tasklet_unlock_wait()这说明这个函数会被阻塞，要在为TASKLET_STATE_SCHED时才可以。
- 从代码可以看出如果一个tasklet在其它的CPU上正在被执行或被禁止时会把这个tasklet重新插到链表
  头,等着下一次执行tasklet_action时再执行,因为tasklet_action()下一次被调用的时候可能是在
  do_softirq()函数里的下一次循环中所以会很快被调用,也有可以是下一次进入从ksoftirqd()进入
  do_softirq()的时候.
- ULK:using two kinds of non-urgent interruptible kernel functions: the so-called
  deferrable functions, and those executed by means of some work queues.这可以看出软中断函
  数是可以被中断的.但其实不然,因为在调用__do_softirq()之前已经把中断给禁止了(这种说法是错误
  的,禁止的是抢占而不是中断)
- ULK:interrupt context : it specifies that the kernel is currently executing either an
  interrupt handler or a deferrable function.禁止抢占了就是在中断上下文吗?,禁止中断了也是在
  中断上下文吗?可能叫临界区,中断上下文与临界区是不一样的.临界区应该包含中断上下文的意思,都不能调度不能休眠.
- Softirqs are statically allocated, while tasklets can also be allocated and initialized
  at runtime.
- 因为在执行__do_softirq()时已经禁止抢占但没有禁止中断,所以执行可延迟函数时是不能做调度和休
  眠的,但是可以被中断,所以软中断是执行在中断上下文的.
** static void tasklet_hi_action(struct softirq_action *a)
*** kernel/softirq.c:
- 与上一个类似
** #define in_interrupt()		(irq_count())
*** include/linux/hardirq.h:
- in_interrupte()判断的是Softirq counter域和Hardirq counter是否为正,不检查Preemption
  counter域,而preempt_disable()增加的是Preemption counter域,所以in_interrupte()和
  preempt_disable()是不相干的.
** struct workqueue_struct *__create_workqueue(const char *name, int singlethread)
*** kernel/workqueue.c:
- 与工作队列相关的结构体和变量:struct workqueue_struct,struct cpu_workqueue_struct(这个结构
  体包含在struct workqueue_struct中,wq成员指回包含它的struct workqueue_struct),workqueues工
  作队列链表
- 为什么当创建非单个CPU的工作队列时会把要创建的工作队列插入到workqueues里而创建单个CPU的工
  作队列时就不用呢?又在什么地方删除呢?
- 调用了create_workqueue_thread()创建工作队列.创建之后马上唤醒它。所以主要就两个任务：创建后唤醒。
** static struct task_struct *create_workqueue_thread(struct workqueue_struct *wq,
*** kernel/workqueue.c:
- 初始化完cpu_workqueue_thread结构体后调用kthread_create()创建一个线程调用worker_thread(),
  传以初始化完的cpu_workqueue_thread.kthread_create()创建线程的方式用了工作队列.
** static int worker_thread(void *__cwq)
*** kernel/workqueue.c:
- 进入这个函数时已经是在新创建的内核线程里执行了
- 这里要把线程状态加多一个PF_NOFREEZE状态,这个状态有什么用呢?用来指明这个线程在休眠时不能冻
  结,/linux/Documentation/power/freezing-of-tasks.txt有说明.
- 在这个函数里就是一直循环执行已插入工作队列的工作,直到这个线程被要求停止
  (kthread_should_stop())
- 把一个没有执行函数的wait_queue_t插入到struct cpu_workqueue_struct->more_work是什么意思呢?应
  该是这样的:这个wait_queue_t的task被设为current了,就是这个执行工作队列的内核线程,所以要想
  执行这个工作队列的工作可以wake_up这个more_work里的进程,是这个意思吗?ULK: more_work:Wait
  queue where the worker thread waiting for more work to be done sleeps.调用schedule()把当
  前线程加入到more_work()当schdule()返回后就从more_work删掉当前进程.虽然more_work是一个等待
  队列,但是并没有用到等待队列比较关键的东西(用wake_up类函数唤醒进程),仅仅是把一个有current
  的waitqueue插入到more_work而没有任何作用.不是这样的,在__queue_work()函数里有调用
  wake_up()把more_work的进程全唤醒.
- 因为这个函数是在一直循环执行的, 但是它是在一开始就用current定义了wait这个waitqueue,而
  current就是新创建的内核进程，所以current进程运行就会执行工作，在__queue_work里会在把一个
  工作插入工作队列之后马上唤醒current.但是为什么要在while循环里把wait加到more_work里又把它
  删除呢？就是在调度之前，在current状态为TASK_INTERRUPTIBLE的时候会把wait加到more_work里，
  而调度完之后会把wait从more_work里删除而把状态改为TASK_RUNNING。
- 好像只有current在more_work里.不会有其它进程了。
- 好像有一个BUG：在把wait从more_work里删除之后more_work可能为空了，但是queue_work这个函数不
  管有没有空都会从more_work里唤醒进程。这个BUG是不存在的，因为queue_work就是会把工作插入到
  当前的CPU的，所以执行这个函数的current（B）一定不是处理工作队列那个current（A），所以A已
  经是在调用schudule中，所以A一定是被插入到了more_work中，所以queue_work里调用wake_up是没有
  问题的,在remove之后再到add之前内核线程是不会切换出去的,即使产生了中断,在内核态时如果不调
  用schudle自动放弃，否则是不会被切到其它进程的。所以在run_workqueue()里执行的工作是在内核
  线程的上下文执行的.
** int fastcall queue_work(struct workqueue_struct *wq, struct work_struct *work)
*** kernel/workqueue.c:
- 一个工作被插入工作队列之后执行之前是不能再被插入的,这点和tasklet是一样的.
- 被插入的工作只会被插入到当前的CPU,不是所有的CPU.
** static void __queue_work(struct cpu_workqueue_struct *cwq,
*** kernel/workqueue.c:
- 在queue_work里找到CPU后在这个函数里设置wq_data,这个要找到CPU后才可以设置.
- insert_sequence在这里自加
- 会把more_work里的进程给唤醒以执行工作函数.
** static inline int is_single_threaded(struct workqueue_struct *wq)
*** kernel/workqueue.c:
- 若一个工作队列不是单CPU的，那么会把workqueue_struct.list插入到workqueues这个全局变量,难道
  workqueues仅仅是把所有非单CPU工作队列链在一起的作用吗？从代码看好像这样链也没什么作用。现
  在发现是有用的,因为struct workqueue_struct这个结构体里有一个cpu_wq成员是一个数组,就是说分
  配一个workqueue_struct结构体的时候就会分配整个数组,但是为了分辨一个workqueue_struct变量是
  一个CPU用的还是多个CPU用的,所以把所有的非单CPU的工作队列全放到一个链表里,以便对一个工作队
  列操作时可以根据该workqueue_struct是否被链到workqueues链表而作出不同的操作.
** static inline void run_workqueue(struct cpu_workqueue_struct *cwq)
*** kernel/workqueue.c:
- 注释说run_depth是防止run_workqueue()函数嵌套的,但是从代码看没有嵌套的可能啊.就算在工作函
  数中进行了休眠也不可能被嵌套的啊,因为CPU与cpu_workqueue_struct与内核线程是绑定的,而且
  run_workqueue是一个静态的函数所以不能被外部调用,在可能用的范围内也出现不了嵌套啊.出现了就是一个BUG.
- 执行这个函数一次会把worklist里的所有工作都处理掉。
** static void flush_cpu_workqueue(struct cpu_workqueue_struct *cwq)
*** kernel/workqueue.c:
- 如果是参数cpu_workqueue_struct是当前CPU的,那么就调用run_workqueue()这个是不会出现
  run_workqueue()嵌套的.
- 这里把当前的insert_sequence保存下来与remove_sequeuece比较来判断是否某个cpu_workqueue已经
  没有工作可以执行,但是为什么要用这种方式呢?直接用一个原子变量不行吗?或用一个加锁的计数器不
  行吗?但因为访问insert_sequence和remove_sequeuece都获得了自旋锁，所以也不会出问题。
- 如果调这个函数所在的CPU不与cpu_workqueue_struct所属的CPU不是同一个就不会调用
  run_workqueue()仅仅是调用了schdule()因为所以工作一定要在指定的CPU执行，这样也可以防止
  run_workqueue()嵌套执行。
- 在这里会使用work_done这个等待队列来实现同步，在run_workqueue()会wake_up这个工作队列。
  work_done就只是这个作用而已。
** int fastcall queue_delayed_work(struct workqueue_struct *wq, struct work_struct *work, unsigned long delay)
*** kernel/workqueue.c:
- 实现延时执行的方法就是定时器插入工作。在时间到来的时候调用delayed_work_timer_fn()来把工作
  插入工作队列。虽然把工作插入之后会马上调用wake_up把more_work唤醒，但是这个是有比较大的延
  迟的。
- 在没有把工作插入工作队列之前已经把work.pending给设置了，这个合适吗？合适的，因为不这样可
  能有多个定时器同时在等着把工作插入工作队列。
** #define get_cpu()		({ preempt_disable(); smp_processor_id(); })
** #define put_cpu()		preempt_enable()
*** include/linux/smp.h:
- 先调用preempt_enable_no_resched()递减抢占计数器然后再调用preempt_check_resched()判断是否
  要切换该进程来调用preempt_schedule().
** #define put_cpu_no_resched()	preempt_enable_no_resched()
** asmlinkage void __sched preempt_schedule(void)
*** kernel/sched.c:
- 
** #define DEFINE_PER_CPU(type, name)
*** include/asm-generic/percpu.h:
- 为什么在多核处理器下多单核处理器下多加了一个指定段的编译属性就可以实现给每个CPU分配变量了？
- 就像声明一个全局变量一样，因为这些全局变量都有一个per_cpu作为开头，所以以后声明变量时要注意
  了。
** #define per_cpu(var, cpu) (*RELOC_HIDE(&per_cpu__##var, __per_cpu_offset[cpu]))
*** include/asm-generic/percpu.h:
- 获取第cpu个的var变量。
** #define __get_cpu_var(var) per_cpu(var, smp_processor_id())
*** include/asm-generic/percpu.h:
- 用per_cpu实现的
** #define get_cpu_var(var) (*({ preempt_disable(); &__get_cpu_var(var); }))
-　比上一个多了一个禁止抢占
** #define put_cpu_var(var) preempt_enable()
*** include/linux/percpu.h:
- 打开抢占而已。name不使用
** #define alloc_percpu(type)
*** include/linux/percpu.h:
- 这个是动态分配的，直接调用__alloc_percpu
** static inline void *__alloc_percpu(size_t size, size_t align)
*** include/linux/percpu.h:
*** mm/slab.c:
- 是单核的话就直接用kmalloc分配，是多核也用kmalloc分配，但分配的是struct percpu_data结构，
  再用kmalloc_node来根据每个CPU各自所用的内存结点来分配。
** #define spin_lock_init(lock)	do { (void)(lock); } while(0)
*** include/linux/spinlock.h:
- 这是单核的定义.
** #define spin_lock_init(x)	do { *(x) = SPIN_LOCK_UNLOCKED; } while(0)
*** include/asm-i386/spinlock.h:
- 这是多核的的定义.
** #define spin_lock(lock)		_spin_lock(lock)
*** include/linux/spinlock.h:
- 这个的定义没有分单多核.这个函数与ULK说的是不一样的,竟然调用preempt_enable()的地方不一样,
  应该在spin_unlock里调用的把.看错ULK了,在spin_lock里调用preempt_enable()是在可抢占内核里实
  现的.如果在不可抢占的内核中会一直自旋,不放弃CPU.
- pause指令相当于rep;nop.
** #define _spin_lock(lock)
*** include/linux/spinlock.h:
- 这是单核的定义,三步:禁止抢占,调_raw_spin_lock,调__acquire.
- 为什么要禁止抢占呢?这样可以防止自已自动放弃CPU.
** void __lockfunc _spin_lock(spinlock_t *lock) __acquires(spinlock_t);
- 也上单核的操作方式
** #define _raw_spin_lock(lock)	do { (void)(lock); } while(0)
*** include/linux/spinlock.h:
- 这是单核的方式
** static inline void _raw_spin_lock(spinlock_t *lock)
*** include/asm-i386/spinlock.h:
- 多核的与架构有关,用汇编实现的.
** #define spin_unlock(lock)	_spin_unlock(lock)
*** include/linux/spinlock.h:
- 这个不分单多核.
** #define _spin_unlock(lock)
*** include/linux/spinlock.h:
- 这是单核的实现,就三步:调用_raw_spin_unlock,打开抢占,调用__release
** void __lockfunc _spin_unlock(spinlock_t *lock)
*** kernel/spinlock.c:
- 与单核的一样.
** #define spin_unlock_wait(lock)	(void)(lock)
*** include/linux/spinlock.h:
- 这是单核的实现
** #define spin_unlock_wait(x)	do { barrier(); } while(spin_is_locked(x))
*** include/linux/spinlock.h:
- 这是多核的实现
- 死循环调用spin_is_locked
** #define spin_is_locked(lock)	((void)(lock), 0)
*** include/linux/spinlock.h:
- 这是单核的实现
** #define spin_is_locked(x)	(*(volatile signed char *)(&(x)->slock) <= 0)
*** include/asm-i386/spinlock.h:
- 这个是多核的实现,就只是一个判断而已.
** #define rwlock_init(lock)	do { (void)(lock); } while(0)
*** include/linux/spinlock.h:
- 这是单核的实现。
** #define rwlock_init(x)	do { *(x) = RW_LOCK_UNLOCKED; } while(0)
*** include/asm-i386/spinlock.h:
- 这是多核的实现。
- 0x01000000为可读可写，当想读时就将它减1，为0x00ffffff，为0x00000000时是已加上了写锁。
- 像这些自旋锁的实现，要根据是否是单多核，是否配置为可抢占。
** void __lockfunc _spin_lock(spinlock_t *lock)
*** kernel/spinlock.c:
- 在这个头文件里实现了_read_lock(), _read_lock(), _spin_lock_irqsave(), _spin_lock_irq(),
  _spin_lock_irq(), _spin_lock_bh(), _read_lock_irqsave(), _read_lock_irq(),
  _read_lock_bh(), _write_lock_irqsave(), _write_lock_irq(), _write_lock_bh(),
  _spin_lock(), _write_lock()这些函数的可抢占版和非抢占版。而这些函数里面调用的前面有_raw字
  样的就是被实现了单核版和多核版，单核版在include/linux/spinlock.h里实现的是单核版，多核版
  在特定的架构的spinlock.h文件里实现。
- 抢占版的都会调用cpu_relax()和使用break_lock成员。抢占版的会打开抢占，允许其它进程抢占，自
  旋锁不再自旋。
- 这类函数的调用关系层是以spin_lock为例：spin_lock() -> _spin_lock()(加一个下划线前缀) ->
  _raw_spin_lock()
- 关于_spin_lock()类函数的可抢占版和非抢占版的所在的文件的定义：要使用自旇锁就要包含文件
  include/linux/spinlock.h(A文件),在文件A定义了单核版的_spin_lock()宏和多核版的_spin_lock()函
  数声明，单核版的_spin_lock()没有可和非可抢占版之分,多核版的可抢占版和非抢占版
  的_spin_lock()的实现都在kernel/spinlock.c里(注意:实现的函数的用宏的方式,所以cscope找不到
  的).单核版的_spin_lock()里调用的_raw_spin_lock()在文件A里定义,单核版的_spin_lock()调用
  的_raw_spin_lock()是有调试版和非调试版的;多核版的_spin_lock()的可抢占版调
  用_raw_spin_lock(),这个_raw_spin_lock()不是单核版那个,而是在include/asm-i386/spinlock.h里
  定义的那个,这个_raw_spin_lock()会用到pause指令(rep;nop),而多核版的_spin_lock()的非可抢占
  版(!CONFIG_PREEMPT)没调用_raw_spin_lock(),而是调用了cpu_relax()而且在调用cpu_relax()之前还使
  能抢占,cpu_relax()也是用rep;nop指令的.
- 单核版的_spin_lock() -> 禁止抢占后调用有调试版的_raw_spin_lock(); 多核版的可抢占版
  的_spin_lock() -> _raw_spin_lock()(架构相关), 多核版的非可抢占版的_spin_lock() -> 使能抢
  占再调用cpu_relax().
** #define seqlock_init(x)	do { *(x) = (seqlock_t) SEQLOCK_UNLOCKED; } while (0)
*** include/linux/seqlock.h:
- #define SEQLOCK_UNLOCKED { 0, SPIN_LOCK_UNLOCKED }
** static inline void write_seqlock(seqlock_t *sl)
*** include/linux/seqlock.h:
- seqlock_t这个锁只是防止多个写操作,与读无关.
- 把sequence自加。
** static inline void write_sequnlock(seqlock_t *sl) 
*** include/linux/seqlock.h:
- 又把sequence自加。
** static inline int write_tryseqlock(seqlock_t *sl)
*** include/linux/seqlock.h:
- 用了spin_trylock(),写是用锁的
** static inline unsigned read_seqbegin(const seqlock_t *sl)
*** include/linux/seqlock.h:
- 返回sequence，读是不用锁的
** static inline int read_seqretry(const seqlock_t *sl, unsigned iv)
*** include/linux/seqlock.h:
- iv为奇数表示正在写，写的锁没有解，sl不等于iv表示已被修改过。这两种情况都要重新执行。
** #define rcu_read_lock()		preempt_disable()
*** include/linux/rcupdate.h:
- 就只是禁止抢占而已
** #define rcu_read_unlock()	preempt_enable()
*** include/linux/rcupdate.h:
- 就只是使能抢占而已
- RCU锁和seq锁有一个区别是:RCU锁可以使用旧的数据作处理,不管数据是否被改变,而seq锁不能,如果
  在处理完之后发现被保护的数据被改变就要重新用新的数据再做一次处理.
** void fastcall call_rcu(struct rcu_head *head, void (*func)(struct rcu_head *rcu))
*** kernel/rcupdate.c:
- 把head插到nxttail这个链表的尾部.
- 这个函数把一个新的释放rcu旧数据的回调函数加到rcu_data这个per-cpu变量的nxttail这个成员的链
  表里去.
- 不是一个CPU经过了静止状态就调用所有的回调函数,而是所有的CPU都经过一次静止状态之后就执行一
  个tasklet来把所有在nxttail里的callback函数执行一次来释放旧的rcu数据,那么回调函数是怎么知
  道自己要释放的数据是什么呢?从回调函数的参数是struct rcu_head可以启发到被释放的数据度该是
  一个结构体,这个结构体里至少有一个struct rcu_head类型的成员,而这个成员就是做为rcu_head参数
  的那个,也是被链到nxttail的,所以可以从nxttail里把存放callback函数的struct rcu_head传给回调
  函数,回调函数再用container_of找到要释放的结构体就可以了.
** void synchronize_rcu(void)
*** kernel/rcupdate.c:
- 这个函数用completion来同步,等待所有CPU经过静止状态.
- 这个函数的实现就是把一个唤醒自己的函数插入到rcu_tasklet里.
- 有一个疑问:是不是被complete之后可能还有callback函数在tasklet里呢?可能有,这个函数的作用只
  是说经过了一个静止状态,而不是说返回之后就没有callback函数在tasklet里了.所以这个函数可以这
  样用:write一个rcu之后调用这个函数,这个函数返回之后可以读到最新的rcu数据没有延时太长的时间.
** void rcu_check_callbacks(int cpu, int user)
*** kernel/rcupdate.c:
- 这个函数有什么用呢?
** static void rcu_process_callbacks(unsigned long unused)
*** kernel/rcupdate.c:
- 就是调用__rcu_process_callbacks(),有软中断和非软中断之分.
** static void __rcu_process_callbacks(struct rcu_ctrlblk *rcp, struct rcu_state *rsp, struct rcu_data *rdp)
*** kernel/rcupdate.c:
- 在curlist不为空和完成的rcu(rcu_ctrlblk.completed)回收不小于rcu_data.batch的要求时就将
  rcu_data.curlist的移到rcu_data.donetail中去并把rcu_data.curlist清空,这个rcu_data.donetail有什么用
  呢?rcu_data.completed是在哪里加的呢?
- 如果rcu_data.nxtlist不空且rcu_data.curlist为空(已经移到了donetail里去了),就把nxtlist移到
  curlist里去并清空nxtlist.这三个链表nxtlist,curlist,donelist就是这种关系了.设置本cpu的
  batch号为当前cur的下一个
- RCU是怎么确定所有的CPU都不同时在RCU读锁里呢?经过的静止状态只是一个过去的状态而已.好像是这
  样的:一个CPU经过静止状态之后就等待所有的CPU经过所有的静止状态,等到所有的CPU都经过静止状态
  之后就可以把新的数据替换旧的数据.
- rdp->quiescbatch = rcp->cur 标识 quiesc period 的开始， rdp->qs_pending = 1， 标识一个
  quiesc period 尚未结束; rdp->passed_quiesc = 0 则是 cpu queisc 结束指示符， 当内核代码执
  行诸如抢占动作前， 会将这个变量置 1， 在后面的时钟中断处理中， 如果发现这个值为 1， 则将
  rdp->qs_pending 置 0 标识quiesc period 的结束， 并将 cpu 在 rcp->cpumask 中的位清除， 以
  表示本 cpu 同意 grace period 结束， 至于能否结束， 则要看其他的 cpu 是否同意。
  rdp->passed_quiesc = 0 还有一个作用是将 quiesc period 开始之前就 (多次) 置位的
  passed_quiesc 的动作取消。
- grace period 的开始代表的是 rcu 观察到至少一个 cpu 已经完成了至少一次数据更新操作，但可能
  善后工作还没有做， 例如上面说的释放旧数据； 而 grace period 的结束， 则代表者 rcu 认为所
  有的其他 cpu 都不再引用写者持有的旧数据了， 因此， 可以安全释放这份数据。 grace period 是
  上一轮 grace period 结束后， 第一次观察到一个 cpu 完成数据更新操作为开始的， 这时所有
  cpu 上一轮 quiesc period 都已经结束， 都处于停止状态。 grace period 一旦启动， 则后继的肯
  定是各个 cpu 各自的 quiesc period 的启动， 停止； grace period 结束于最后一个 quiesc
  period 的结束之后。 在 grace period 执行期间， 如果其他 cpu 上也出现了写者数据更新操作，
  则视这个 cpu 上的 quiesc period 启动与否而有不同行为， 如果没有启动， 则可以认为这个更新
  操作属于这个已启动的 grace period, 否则， 则将其归类于下一个 grace period。 但这个分类并
  不是 100% 正确， 会将本该分在这个 grace period 的结果分在了下一个 grace period, 这是 smp
  系统的缓存一致性决定的， 存在偶然因素， 但产生的结果不是不可接受的 ---- 即不会引起旧数据
  的提前释放， 只会引起其释放延迟， 这个在 rcu 目标应用场景中， 是可以接受的。
- 在退出所有读操作的那一瞬间， 那就不再引用了， 那么， 在那一瞬间之后呢， 会不会再次引用这
  些旧数据？ 不会， 因为它就算再读相同的数据， 读到的也不过是写者已经更新的新数据。 最终的
  结果就是， 只要发生了一次抢占， 则本 cpu 就会同意 grace period 的结束， 也就是说， 所有写
  者不用再担心运行在这个 cpu 上的代码引用它的旧数据。 那么当最后所有 cpu 都退出 quiesc
  period 的时候， 只会剩下写者自己持有旧数据， 它可以任意操作旧数据了。
- 写者修改数据前首先拷贝一个被修改元素的副本，然后在副本上进行修改，修改完毕后它向垃圾回收
  器注册一个回调函数以便在适当的时机执行真正的修改操作。等待适当时机的这一时期称为grace
  period，而CPU发生了上下文切换称为经历一个quiescent state，grace period就是所有CPU都经历一
  次quiescent state所需要的等待的时间。垃圾收集器就是在grace period之后调用写者注册的回调函
  数来完成真正的数据修改或数据释放操作的。https://www.ibm.com/developerworks/cn/linux/l-rcu/
- 因为 call_rcu_bh将把 softirq 的执行完毕也认为是一个 quiescent state，因此如果修改是通过
  call_rcu_bh 进行的，在进程上下文的读端临界区必须使用这一变种。
- 因为donelist一开始是赋初值的,所以就算在执行的过程中对链表进行增删改查也不用赋为NULL
- rcu_data.batch大于rcu_ctrlblk.completed时(就是rcu_data.batch == rcu_ctrlblk.completed+1)
- 如果nextlist里有数据且curlist里没有数据,那么当前CPU就会等待从当前时间开始的下一个完整的
  grace period,如果所等行待的grace period结束了(rcu_data.batch>=rcu_ctrlblk.completed)且
  rcu_data.curlist不为空,那么就会把curlist赋给donelist并在这个函数的最后调用rcu_do_batch()
  来处理所有的回调函数.
** static inline void rcu_qsctr_inc(int cpu)
*** include/linux/rcupdate.h:
- 当在CPU上发生进程切换时，函数rcu_qsctr_inc将被调用以标记该CPU已经经历了一个quiescent
  state。该函数也会被时钟中断触发调用。rcu_qsctr_inc函数的确在schedule()里被调用.
- 就是把per cpu变量rcu_data.passed_quiesc改为1.
- quiesc period 与 grace period 的关系， 就是 quiesc 从来都是在 grace period 开始之后开始，
  在 grace period 结束之前结束。
** void rcu_check_callbacks(int cpu, int user)
*** kernel/rcupdate.c:
- 时钟中断触发垃圾收集器运行，它会检查：否在该CPU上有需要处理的回调函数并且已经经过一个
  grace period；否没有需要处理的回调函数但有注册的回调函数；否该CPU已经完成回调函数的处理；
  否该CPU正在等待一个quiescent state的到来；如果以上四个条件只要有一个满足，它就调用函数
  rcu_check_callbacks。
- idle_cpu(cpu)说明第cpu正在执行idle进程.
- 检查CPU是否经历了一个quiescent state:1.当前进程运行在用户态;2.当前进程为idle且当前不处在
  运行softirq状态，也不处在运行IRQ处理函数的状态；
- 通过调用函数rcu_qsctr_inc标记该CPU的数据结构rcu_data和rcu_bh_data的标记字段passed_quiesc，
  以记录该CPU已经经历一个quiescent state。
- 函数rcu_check_callbacks将调用tasklet_schedule，它将调度为_该_CPU设置的tasklet rcu_tasklet，
  每一个CPU都有一个对应的rcu_tasklet.
- rcu_process_callbacks可能做以下事情：1． 开始一个新的grace period；这通过调用函数
  rcu_start_batch实现。2． 运行需要处理的回调函数；这通过调用函数rcu_do_batch实现。3． 检查
  该CPU是否经历一个quiescent state；这通过函数rcu_check_quiescent_state实现
** static void rcu_do_batch(struct rcu_data *rdp)
*** kernel/rcupdate.c:
- rcu_data.donelist才是被tasklet执行的回收rcu函数,不是rcu_data.curlist.
- 最终会在这个函数里调用回收函数.
- 只有在__rcu_process_callbacks()里检测到donelist不为空时才调用rcu_do_batch()
- 每调用里面的一个函数就是把一个旧数据释放掉.
- 里面的maxbatch是每次执行tasklet时调用的回调函数的个数,为什么要这个东西而不是一次调用所有
  的回调函数呢?
- 如果donelist为空就把&donelist赋给donetail,否则就用tasklet_schedule再调度这tasklet.
- 在这里,注意每次调用的回调函数有最大值限制.这样做主要是防止一次调用过多的回调函数而产生不
  必要系统负载.http://blog.chinaunix.net/uid-12260983-id-2952617.html
** static void rcu_check_quiescent_state(struct rcu_ctrlblk *rcp, struct rcu_state *rsp, struct rcu_data *rdp)
*** kernel/rcupdate.c:
- 调用函数rcu_check_quiescent_state检查该CPU是否经历了一个quiescent state,如果是并且是最后
  一个经历quiescent state的CPU，那么就结束grace period，并开始新的grace period。如果有完成
  的grace period，那么就调用rcu_do_batch运行所有需要处理的回调函数。
- rcu_data.quiescbatch(Batch # for grace period)不等于rcu_ctrlblk.cur(Current batch
  number)就说明当前CPU所在的quiesc period不是在全局的grace period里的,注释里说start new
  grace period,这个应该不正确应该是quiesc period。因为不等了，也说明了开始了一个全局的
  grace period之后，当前CPU还没经过一个quiesc period,所以就要开始一个quiesc period,而开始一
  个quiesc period就是要把rcu_data里的三个变量设置一下：qs_pending设为1（标识一个quiesc
  period 尚未结束,qs_pending只在这个函数被修改），passed_quiesc设为0（在经过quiesc period时设为1，schedule()调用
  rcu_qsctr_inc()转而设passed_quiesc为1，但和qs_pending有什么区别呢？）,quiescbatch设为rcu_ctrlblk.cur指定所开始的这个
  quiesc period是在第cur个grace period里的。
- batch这个词在这里的指第几批grace period.
- 若qs_pending为0，表明对于这个CPU来说已经经过了第quiescbatch个grace period了。
- qs_pending和passed_quiesc还是有一点不同的:qs_pending - 0, passed_quiesc - 0这种情况是不可
  能的,qs_pending - 0, passed_quiesc - 1:schedule()把passed_quiesc设为1了,之后也调用了这个
  函数,但在开始一个新的grace period之前就会出现这种情况.qs_pending - 1, passed_quiesc - 0:
  因发现又开始了一个新的grace period,所以当前CPU又要开始重新检查一个quiesc state,这种情况就
  是在这个函数一开始时设置的.qs_pending - 1, passed_quiesc - 1:开始一个quiesc后也执行了
  schedule()但还没有进行这个函数rcu_check_quiescent_state()
- 搞明白rcu_data和rcu_ctrlblk里面的成员就差不多了.
- 函数的功能就是若发现开始了一个新的grace period就开始一个新的quiesc period等待经过一个静止
  状态,否则看是否经过了静止状态,若经过了静止状态就清sq_pending.
- 对这个函数和里面调用的所有的函数层层解开之后功能就是：有可能开始一个新的quiesc period;有
  可能标识当前的CPU已经经过了静止状态；有可能因为当前CPU经过了静止状态后同时发现自已是最后
  一个经过静止状态的CPU而可能因为还有下一batch而开始一个新的grace period.
** static void cpu_quiet(int cpu, struct rcu_ctrlblk *rcp, struct rcu_state *rsp)
*** kernel/rcupdate.c:
- cpus_empty()参数全为空的时候就为真。所以rcu_state.cpumask为空的话，就说明所有的CPU都经过
  了当前第batch的grace period.当所有的CPU都经过静止状态时就把rcu_ctrlblk.completed给设为
  rcu_ctrlblk.cur，所以可以通过这两个是否相等判断是否正在等待下一个grace period.
- 这个函数主要是一个CPU经过静止状态的时候调用的（当经过一次静止状态时有被
  rcu_check_quiescent_state调用）。
** static void rcu_start_batch(struct rcu_ctrlblk *rcp, struct rcu_state *rsp,
*** kernel/rcupdate.c:
- 这个函数会根据传入的参数next_pending来修改rcu_ctrlblk.next_pending.在cpu_quiet()调用的
  rcu_start_batch()是以next_pending为0调用的，因为在cpu_quiet()调用rcu_start_batch()时一定
  是最后一个cpu经过了静止状态,就因为这个所以要以next_pending为0调用rcu_start_batch()吗?
- 如果rcu_ctrlblk.next_pending为1但是rcu_ctrlblk.completed不等于rcu_ctrlblk.cur时是不会改变
  rcu_ctrlblk.cur的,因为rcu_ctrlblk.completed不等于rcu_ctrlblk.cur说明当前批的grace period
  还没有结束,所以即使有下一批的回收挂起,也不能把改变cur而开始下一个grace period.如果
  rcu_ctrlblk.next_pending为0但是rcu_ctrlblk.completed等于rcu_ctrlblk.cur时也不能改变
  rcu_ctrlblk.cur,因为rcu_ctrlblk.completed等于rcu_ctrlblk.cur说明当前已经不在grace period
  里了,但因为next_pending为0即没有下一批需要回收的rcu,所以也不能设置rcu_ctrlblk.cur以开始一
  个新的grace period.开始下一个grace period(递加rcu_ctrlblk.cur)时前会清掉
  rcu_ctrlblk.next_pending,
- 到目前为止rcu_ctrlblk结构体里的cur,completed,next_pending成员都已经明白什么意思
  了,rcu_data结构体里的quiescbatch,passed_quiesc,qs_pending,donelist,donetail也明白什么意思
  了.
- 因为rcu_qsctr_inc()会在schedule()和rcu_check_callbacks()里调用,而且rcu_check_callbacks()
  又在定时器中断处理程序里调用，所以比较明了在哪里会去确定是否经过静止状态。
- 在next_pending不为假时才会开始一个grace period.
- 读者是可以嵌套的.也就是说rcu_read_lock()可以嵌套调用.
- The Linux kernel offers a number of RCU implementations, the first such implementation
  being called "Classic RCU". More material introducing RCU may be found in the
  Documentation/RCU directory in any recent Linux source tree, or at Paul McKenney's RCU
  page. Linux RCU的第一个实现
- 将rcp->next_pending置为1.设置这个变量主要是防止多个写者竞争的情况
- 如果CPU 1上有进程调用rcu_read_lock进入临界区,之后退出来,发生了进程切换,新进程又通过
  rcu_read­_lock进入临界区.由于RCU软中断中只判断一次上下文切换,因此,在调用回调函数的时候,仍
  然有进程处于RCU的读临界区,这样会不会有问题呢?只要使用被RCU保护数据的方法正确就不会有问题,引
  用时要用rcu_dereference()函数,rcu_dereference()函数里定义了一个新的局部变量来对保护指针的
  引用,所以一下进入临界区的时候就是最新的数据了如:
  #+BEGIN_EXAMPLE
  void foo_update_a(int new_a)//这个是更新数据
  {
  struct foo *new_fp;
  struct foo *old_fp;
  
  new_fp = kmalloc(sizeof(*new_fp), GFP_KERNEL);
  spin_lock(&foo_mutex);
  old_fp = gbl_foo;
  *new_fp = *old_fp;
  new_fp->a = new_a;
  rcu_assign_pointer(gbl_foo, new_fp);
  spin_unlock(&foo_mutex);
  synchronize_rcu();
  kfree(old_fp);
  }

  int foo_get_a(void)//这个是引用数据,不能使用gbl_foo直接引用.
  {
  int retval;
  
  rcu_read_lock();
  retval = rcu_dereference(gbl_foo)->a;
  rcu_read_unlock();
  return retval;
  } 
  #+END_EXAMPLE
** static inline void init_MUTEX (struct semaphore *sem)
*** include/asm-i386/semaphore.h:
- 就是调用sema_init这个函数.
** static inline void init_MUTEX_LOCKED (struct semaphore *sem)
*** include/asm-i386/semaphore.h:
- 就是调用sema_init这个函数.
** static inline void sema_init (struct semaphore *sem, int val)
*** include/asm-i386/semaphore.h:
- 初始化struct semaphore里的三个成员,count为参为val,sleepers(ULK:Stores a flag that
  indicates whether some processes are sleeping on the semaphore.)为0,wait(ULK:Stores the
  address of a wait queue list that includes all sleeping processes that are currently
  waiting for the resource.)用init_waitqueue_head初始化.
** #define DECLARE_MUTEX(name) __DECLARE_SEMAPHORE_GENERIC(name,1)
*** include/asm-i386/semaphore.h:
- 编译时声明
** static inline void up(struct semaphore * sem)
*** include/asm-i386/semaphore.h:
- 用汇编实现，两步：1.把计数器加1，2.计数器大于0就调用__up_wakeup().
** ".globl __up_wakeup\n"
*** arch/i386/kernel/semaphore.c:
- 这个函数全用汇编实现，就连函数名也是。
- 就是把__up()这个函数的参数压入栈后调用__up()，返回后再出栈。
** static fastcall void __attribute_used__  __up(struct semaphore *sem)
*** arch/i386/kernel/semaphore.c:
- 直接调用调用wake_up()，所以可以看出信号量是用等待队列实现的，可以得出它的性能如何。
** static fastcall void __attribute_used__ __sched __down(struct semaphore * sem)
*** arch/i386/kernel/semaphore.c:
- 这个函数与down()函数结合可以看出：如果进程A获取信号X后因为count为0而进入__down(),这时A成
  为了第一个等待信号A的进程，后来进程B也因为获取信号X进入休眠，但有可能进程B先获得信号量。
- 理解这个函数sem->count, sem->sleepers之间的关系的例子：当sleepers为1,count为-1时进入
  down()之后会把count自减1为-2，接着进入__down(),接着在__down()里会把sleepers自加为2，经过
  atomic_add_negative之后count就为-1，所以为-1并不表明只有一个进程在等待，接着又把sleepers
  改为1，所以这也表明sleepers为1也不说明只有一个睡眠进程,如果这时有一个进程up了，那么count
  会为0,这时因为sem->sleepers为1，所以atomic_add_negative()为假，接着把sem->sleepers改为0，
  这时只有一个进程在等待了，且sem->count为0,当这个等待的进程被唤醒之所再进入
  atomic_add_negative()时，因为sem->sleepers为0,sem->count为0所以atomic_add_negative()为真，
  所以会继续等待，且sem->count为-1了,接着sem->sleepers改为了1,再次执行
  atomic_add_negative()时还是为真。
- 其实信号量有使用等待队列里的锁来保护整个信号量结构体的访问。所以不会对struct semaphore有
  竞争访问，若有竞争访访问，会在一个地方有问题：若进程A是第一个等待某个信号量的进程，进程A
  在某一个时刻执行完atomic_add_negative之后在sem->sleepers=1之前可能被切换去出执行进程B，这
  时B又要调用down()获取这个信号量,down()会在一开始把sem->count自减1，接着进入__down()，在执
  行完sem->sleepers++之后被切换出去执行A，而这时sem->sleepers为2，但是恢复执行A之后会执行
  sem->sleepers=1这一句，所以当B再次恢复执行时就是执行int sleepers = sem->sleepers,而
  sem->sleepers已被改成了1而不是恢复执行前的2，所以在某个进程执行up()之前sem->count是-2，当
  某个进程真的执行up()时，按理说应该会唤醒一个进程，但是因为sem->count原来为-2所以最终也没
  有唤醒进程。所以必须有一个锁给以保护才可以,这个锁就是sem->wait->lock自旋锁了。
** static inline int down_trylock(struct semaphore * sem)
*** include/asm-i386/semaphore.h:
- 这个函数在一开始把sem->count减-1,如果为负数就进入__down_failed_trylock宏，转而直接调
  用__down_trylock()
** static fastcall int __attribute_used__ __down_trylock(struct semaphore * sem)
*** arch/i386/kernel/semaphore.c:
- 在一开始把sleepers赋予了sem->sleepers+1，接着再用atomic_add_negative()把sleepers加到
  sem->count里去。因为sleepers是从sem->sleepers加多了1，所以这个1把sem->count在
  down_trylock()里减的1给抵消了。
** static fastcall int __attribute_used__ __sched __down_interruptible(struct semaphore * sem)
*** arch/i386/kernel/semaphore.c:
- 这个函数与__down()类似，多了一个检查信号量的步骤（因为是interruptible的等待，可以被信号唤
  醒），还有一个地方不一样就是设置进程状态为TASK_INTERRUPTIBLE.
** static inline void init_rwsem(struct rw_semaphore *sem)
*** lib/rwsem-spinlock.c:
*** include/asm-i386/rwsem.h:
- linux通用的struct rw_semaphore和i386里的struct rw_semaphore是不同的。
- struct rw_semaphore里的wait_list是一个struct rwsem_waiter的链表，rwsem_waiter->flags表明
  了rwsem_waiter->task是读的还是写的。
- rw_semaphore->count的定义有点复杂：ULK:(Stores two 16-bit counters. The counter in the
  most significant word encodes in two's complement form the sum of the number of
  nonwaiting writers (either 0 or 1) and the number of waiting kernel control paths. The
  counter in the less significant word encodes the total number of nonwaiting readers and
  writers.)
** void fastcall __sched wait_for_completion(struct completion *x)
*** kernel/sched.c:
- ULK在complete一节一开始所举的例子是有会出现吗？up()是用汇编实现的，实现的时候是一开始就自
  加了sem->count,转而进入__up_wakeup(),这个函数在最后调用__up(),转而调用__wake_up()用
  sem->wait作为参数,在__wake_up()里先把sem->wait->lock给锁上之后就以非同步的方式调
  用__wake_up_common()，一定要用非同步的方式，因为现在持有自旋锁，__wake_up_common()退出之
  后就解sem->wait->lock锁，而ULK说的被抢占的情况只能发现在这个解锁之后，但解锁之后就是退
  到__up()再直接退到__up_wakeup()再直接退到up(),所以就算在解锁之后被等待信号量的进程抢占了
  之后把信号量结构体释放也不会有问题，因为在解锁之后不会再被调用。
- 与信号量的区别就是：信号量的sem->wait->lock不会完全保护sem->count的访问，因为在up()和
  down()的一开始在没有加锁的情况下就修改了sem->count,而wait_for_completion()和complete()是
  在加了锁之后才会访问completion->done.
- completion的等待都是用WQ_FLAG_EXCLUSIVE插入的，被插入的等待是放在队列尾的，是把进程的状态
  改为TASK_UNINTERRUPTIBLE的
- 如果在进入函数时发现done大于0时就不会进入休眠，这点与信号量是一样的。
- 信号量和completion都是用等待队列来实现同步的。
** unsigned long fastcall __sched wait_for_completion_timeout(struct completion *x, unsigned long timeout)
*** kernel/sched.c:
- 不同的是调用了schedule_timeout()而不是schedule(),在发现超时就会马上退出而不再获取。
- schedule_timeout()是与schedule()一样来处理切换的吗？而不会管时间来调度，就是说不会因为时
  间到了而调度某个进程，是在这某个进程被切回来执行之后而判断与所要求的切回时间有多大的差距。
** int fastcall __sched wait_for_completion_interruptible(struct completion *x)
*** kernel/sched.c:
- 这个的区别就是把进程的状态改成TASK_INTERRUPTIBLE，在改成这个状态之前和
  down_interruptible()一样也会检查是否还有未处理的信号。
** #define local_irq_disable() 	__asm__ __volatile__("cli": : :"memory")
*** include/asm-i386/system.h:
- i386就是用一个指令把本地的中断给禁止了，
** #define irqs_disabled()
*** include/asm-i386/system.h:
- 查看本地中断传递是否被禁止.ULK:yields the value one if the IF flag of the eflags
  register is clear, the value one if the flag is set.
** #define local_bh_disable()
*** include/linux/interrupt.h:
- 在preempt_count的软中断位置了加1，所以若要使能软中断就要使local_bh_disable()和
  local_bh_enable()的调用次数匹配。这点与local_irq_save()和local_irq_restore()不一样。
** #define time_after(a,b)
*** include/linux/jiffies.h:
- a的时间在b之后就为真。
- http://decimal.blog.51cto.com/1484476/410673 若b是固定的，那么无论b从哪里开始，只要开始时
  a等于b，且a是在递增的且递增的次数小于有符号整型的最大值，那么这个宏就是安全的，对于
  HZ=100，数据长度是32位的，2147483647/100秒 = 0.69年 = 248.5天才会出现不正确的判断。
- jiffies_64要溢出需要几百万年，这样就可以保存从开机到现在所经过的时间了。
** static inline u64 get_jiffies_64(void)
*** kernel/time.c:
*** include/linux/jiffies.h:
- 可以通过BITS_PER_LONG来判断。
- 若BITS_PER_LONG不是64位的，那么就要用顺序锁来读取jiffies_64了.
** struct timespec
*** include/linux/time.h:
- tv_sec成员是秒，从1970-7-1开始计算起。
- tv_nsec是纳秒
** void __init time_init(void)
*** arch/i386/kernel/time.c:
- 从cmos获取的值赋给了xtime.tv_sec
- xtime.tv_nsec设了一个特殊的值以在5min之后就会溢出。
- 设置单调时间，但是看不懂这个设置什么含义.
- 选择时间对象.
- 设置时间中断处理函数。
** irqreturn_t timer_interrupt(int irq, void *dev_id, struct pt_regs *regs)
*** arch/i386/kernel/time.c:
- 这个是时钟中断函数，注意它那执行了哪些函数。
- mark_offset()这个函数主要就是更新jiffies_64并记录相对应的
- ULK:a few timer interrupts can be lost, for instance when interrupts remain disabled for
  a long period of time; in other words, the kernel does not necessarily update the xtime
  variable at every tick. However, no tick is definitively lost, and in the long run,
  xtime stores the correct system time. The check for lost timer interrupts is done in the
  mark_offset method of cur_timer; 禁止中断会把产生的中断丢失，原来mark_offset真的是来防止
  时间中断丢失。这里有关如何使用tsc来防止中断丢失：http://book.51cto.com/art/200810/93782.htm
- 好像只有在调用这个函数的时候才会增加jiffies_64啊且在增加jiffies_64之后很快就会在下面所调
  用的update_times()函数里把wall_jiffies给补上，若是这样就只有在timer_interrupt()函数里才会
  出现jiffies_64比wall_jiffies大的情况，但是为什么在do_gettimeofday()会用jiffies减
  wall_jiffies呢？
- 接着调用do_timer_interrupt().
- 时钟中断：由系统定时硬件以周期性的间隔产生,hz：上述间隔由hz的值设定，hz是一个与体系结构相关的常数
- 全局变量xtime所维持的当前时间通常是供用户来检索和设置的，而其他内核模块通常很少使用它（其
  他内核模块用得最多的是jiffies），因此对xtime的更新并不是一项紧迫的任务，所以这一工作通常
  被延迟到时钟中断的底半部（bottom half）中来进行。由于bottom half的执行时间带有不确定性，
  因此为了记住内核上一次更新xtime是什么时候，Linux内核定义了一个类似于jiffies的全局变量
  wall_jiffies，来保存内核上一次更新xtime时的jiffies值。好像更新xtime不是在底半部做的，因为
  更新xtime.tv_nsec是在update_wall_time_one_tick()做的,而是在update_wall_time()被调用，
  update_wall_time_one_tick()更新xtime.tv_nsec，update_wall_time()更新xtime.tv_sec。
** static inline void do_timer_interrupt(int irq, void *dev_id, struct pt_regs *regs)
*** arch/i386/kernel/time.c:
- 只是调用了do_timer_interrupt_hook()
** static inline void do_timer_interrupt_hook(struct pt_regs *regs)
*** include/asm-i386/mach-default/do_timer.h:
- 连续调用了do_timer(),profile_tick(),update_process_times()
** void do_timer(struct pt_regs *regs)
*** kernel/timer.c:
- 为什么这里又要递增jiffies_64呢？不是已经在mark_offset里加了吗？好像mark_offset里的增加。
  这两个地方增加是不一样的，在mark_offset()里加的丢失的时钟中断的时间，而在这里加的是定时到
  来的时间。是jiffies_64是在中断丢失的情况下增加的。
- 调用了update_times
** static inline void update_times(void)
*** kernel/timer.c:
- 调用了update_wall_time()
- jiffies和wall_jiffies是什么关系呢？只有在这个函数wall_jiffies被修改，把它与jiffies的差值
  加到wall_jiffies,
- 调用calc_load()来更新均衡负载,所以更新均衡负载每个时钟周期都计算的.不是这样的要进入
  calc_load函数才知道,里面有一个循环定时器,经过了LOAD_FREQ个tick之后才会计算更新均衡负载.
** static void update_wall_time(unsigned long ticks)
*** kernel/timer.c:
- 调用ticks次update_wall_time_one_tick().和second_overflow()
- 为什么second_overflow()要在xtime.tv_sec增加之后才调用呢?
** static void update_wall_time_one_tick(void)
*** kernel/timer.c:
- 这个函数主要是用来更新xtime中的tv_nsec成员的。
- 这个函数的实现考虑到了NTP和adjtimex()系统调用。
- time_adjust是指要调整的时间，单位是微秒的（scale us）,它会在do_settimeofday()和
  do_adjtimex()被修改成0，在这个函数里会被修改成time_next_adjust,而time_next_adjust会在
  do_adjtimex()里被修改,所以do_timeadjx()调用之后不会马上改变时间，只是在时钟中断处理函数执
  行时才会改变,因为time_adjust_step是微秒的,所以要乘1000,又因为是在中断定时器时调用了这个函
  数所以要加上tick_nsec.
- 从代码来看就是把xtime.tv_nsec加上（减去）time_phase上的高几位。time_phase和time_adj是与
  second_overflow()有关的
** static void second_overflow(void)
*** kernel/timer.c:
- 用来处理微秒数成员溢出的情况。
** void update_process_times(int user_tick)
*** kernel/timer.c:
- 从do_timer_interrupt_hook()可以看出在多处理器的情况下才会调用这个函数.
- 在do_timer_interrupt_hook()调用这个函数时的参数使用了user_mode(reg)来判断reg是在用户还是
  在内核态以得出当前这个tick是在用户还是在内核态下发生的.但是这样的计算方法有点粗糙,因为在
  一个tick内一个用户进程可以先在用户态然后进程调用系统调用以进入内核态,接着完成系统调用并返
  回用户态执行.
- ULK:cutime and cstime are provided in the process descriptor to count the number of CPU
  ticks spent by the process children in User Mode and Kernel Mode, respectively.For
  reasons of efficiency, these fields are not updated by update_process_times( ) , but
  rather when the parent process queries the state of one of its children (see the section
  "Destroying Processes" in Chapter 3).
- cputime_add()这个宏只是将两个参数相加.
- 在这里调用rcu_check_callbacks()来更新rcu.
** void run_local_timers(void)
*** kernel/timer.c:
- 在update_process_times()里被调用来唤醒定时器中断处理的底半部软中断。
** void account_user_time(struct task_struct *p, cputime_t cputime)
*** kernel/sched.c:
- 认识一下struct cpu_usage_stat的成员：user:从系统启动开始累计到当前时刻，用户态的CPU时间，
 不包含 nice值为负进程。nice: 从系统启动开始累计到当前时刻，nice值为负的进程所占用的CPU时间。
 system: 从系统启动开始累计到当前时刻，核心时间.idle :从系统启动开始累计到当前时刻，除IO等
 待时间以外其它等待时间.  iowait 从系统启动开始累计到当前时刻，IO等待时间.irq:从系统启动开
 始累计到当前时刻，硬中断时间.softirq: 从系统启动开始累计到当前时刻，软中断时间./proc/stat
 文件里的第一行会显示这些信息。
- 在这里cputime一定加到进程描述符的utime里，进程的nice大于0时会把cputime加到cpustat->nice里
  否则加到cpustat->user里，可见utime的时间等于cpustat->nice加cpustat->user.为什么要这样做呢？
** void account_system_time(struct task_struct *p, int hardirq_offset, cputime_t cputime)
*** kernel/sched.c:
- 除了在account_user_time()里更新的cpustat成员，其它的成员在这个函数里更新。
- stime与cpustat里的成员是无关的。hardirq_count() - hardirq_offset的结果会不会忽略掉一个硬
  中断呢？
- stime等于cpustat->irq,cpustat->softirq,cpustat->system,cpustat->iowait,cpustat->idle之和。
  hardirq是优先级最高的，其次是softirq,在进程是idle的时候要分情况，不是idle进程就一定加计到
  cpustat->idle上，即使现在执行的是idle进程，但若有进程在等待io(runqueue->nr_iowait)，那就
  加计到cpustat->iowait而不是cpustat->idle。
** #define jiffies_to_cputime(__hz)
*** include/asm-generic/cputime.h:
- 用于将一个时钟中断转换为处理器时间
** static inline void init_timer(struct timer_list * timer)
*** include/linux/timer.h:
- 清base,设magic,初始化lock
- ULK:Indeed, a sleeping process may be woken up before the time-out is over; in this
  case, the process may choose to destroy the timer.
- ULK:Invoking del_timer( ) on a timer already removed from a list does no harm, so
  removing the timer within the timer function is considered a good practice.在定时器函数里
  用del_timer()来删除自身。
** int del_timer(struct timer_list *timer)
*** kernel/timer.c:
- ULK:del_timer()和del_timer_sync()之间的区别主要是后者用于多处理器中，以防止某个CPU在使用某个
  定时器结构时另一个CPU在调用del_timer()来删除它，而del_timer_sync()会等待。
- ULK:因为del_timer_sync()要考虑定时器自注册的可能所以慢又复杂，如果可以确定被删除的定时器
  没有自注册，那么就可以用del_singleshort_timer_sync().
- 因为每个CPU都有一个tvec_base_t的结构体，所以timer在哪个CPU插入就在哪个CPU执行。那如果被
  mod_timer()了呢？
- TVR应该是timer vector root的缩写,对应的是TVN。
- tv1 - 2的8次方-1的ticks, tv2 - 2的8+6次方-1ticks, tv3 - 2的8+6+6次方-1ticks, tv4 - 2的
  8+6+6+6次方-1ticks.
- 这些定时器是用软中断执行的,若软中断被禁止的时间长了也可能导致定时器的执行时间不精确。
- tv1,tv2等这些成员的类型是list_head的数组，所以这个数组的每一个成员就是一个特定的tick的定
  时器。但为什么还要分tv1,tv2,tv3这几组呢？
** static void run_timer_softirq(struct softirq_action *h)
*** kernel/timer.c:
- run_timer_softirq()这个函数是与TIMER_SOFTIRQ的执行函数.
- 这个函数里判断base->timer_jiffies小于jiffies就马上调用__run_timers().
** static inline void __run_timers(tvec_base_t *base)
*** kernel/timer.c:
- timer_jiffies开始是初始化为jiffies,接着就会在__run_timers()一直自加,因为timer_jiffies只是
  用来确定与jiffies的差值,所以不用管是否溢出,time_after_eq()这些函数会解决.
- 关于定时器这种级联设计是如何工作的:因为timer_jiffies是递增的,假设一开始是0,在大于255之前
  所有在tv1里的定时器被会被查一次即使没有定时器放在相应的数组中,当timer_jiffies的低8位因为
  溢出而全又变为0的时候,而第9位变为了1,这一位就是tv2的最低位,在这种情况下会调用cascade()把
  tv2里的所有定时器按照各自相应的定时时间放到tv1的255个元素里,放到第9位的定时器都有可能是
  255个里的任意一个,这时__run_timers()又可以在tv1找定时器来运行了,再等到tv1里的定时器又运行
  完之后,且因为timer_jiffies的第9位已为1,那么再进1就使得第10位为1而所有的位都为0了,所以又到
  了调用cascade()的时候了,就这样以此类推.用这种算法也不用担心在任何时候在任何地方插入定时器
  都不会有问题,因为这不是先进先出又不是先进后出.
- 如果要把一个新的定时器插入,是不是被插的地方要这样计算呢:用当前CPU的timer_jiffies提出从新
  定时器设定的expires为1的最高一位开始的所有低位值加上新的定时器设定的expires值就是被插入的
- 应该不是每个HZ的时间就执行一次__run_timer(),就是时钟中断是可能丢失的,而__run_timer()是在
  时钟中断产生后才会有一次执行的机会.jiffies可能会一次时钟中断加多个数而timer_jiffies是自加
  的.这样的话会使得所有的定时器的执行都延时.而且很多时候都是禁止中断的.
- 进入这个函数一般就只会把tv1里的某一个下标的所有定时器执行完,但如果执行的过程中又产生了时
  钟中断那么就再执行一次.
- 从这个函数可以看出是如何解决jiffies一次加上大于1的值的,因为这个函数里有一个while循环比较
  jiffies与timer_jiffies而且在while里会一直自加timer_jiffies,所以到最后timer_jiffies还是会
  追上jiffies的且也会把因jiffies跳增而遗下的定时器处理完.
** static void internal_add_timer(tvec_base_t *base, struct timer_list *timer)
*** kernel/timer.c:
- 这个函数就是根据定时器的expires和当前的timer_jiffies来确定把指定的定时器插入到相应的tv的
  下标里.
- 创建新的定时器时是用jiffies和需要延时的时间来计算expires的,但是在这个函数里的idx的计算却
  是expires减去timer_jiffies,是不是有点不对呢?因为timer_jiffies会丢失一些是钟.
- 得出的idx就是其实只是一个时间差,而不是下标,通过这个值判断是哪个区间以确定是在
  tv1/tv2/tv3/tv4/tv5,确定这个之后再用expries(不是idx)来确定放到哪个下标去.
- 定时器插入是FIFO形式的.
- 从这个函数可以看出tv1的组织方式是和tv2/tv3/tv4/tv5很不一样的,tv1里的任何一个元素都对应着
  一个特定的时间到期的定时器,而其它的tv的一个元素指定的是一个范围时间到期的定时器,如tv2的第
  2个元素所存放的定时器应该是在2的10次方减去2的9次方时间段内到期的定时器.
** static int cascade(tvec_base_t *base, tvec_t *tv, int index)
*** kernel/timer.c:
- 这个函数的主要功能就是把tv里的下标为index的所有链表结点传给internal_add_timer()来放到相应的位置.
** static void __devinit init_timers_cpu(int cpu)
*** kernel/timer.c:
- timer_jiffies在这里被初始化为jiffies.
- 在极端的条件下，同时会有多个 TV 需要进行 cascade 处理，会产生很大的时延。这也是为什么说
  timeout 类型的定时器是 timer wheel 的主要应用环境，或者说 timer wheel 是为 timeout 类型的
  定时器优化的。因为 timeout 类型的定时器的应用场景多是错误条件的检测，这类错误发生的机率很
  小，通常不到超时就被删除了，因此不会产生 cascade 的开销。另一方面，由于 timer wheel 是建
  立在 HZ 的基础上的，因此其计时精度无法进一步提高。毕竟一味的通过提高 HZ 值来提高计时精度
  并无意义，结果只能是产生大量的定时中断，增加额外的系统开销。因此，有必要将高精度的 timer
  与低精度的 timer 分开，这样既可以确保低精度的 timeout 类型的定时器应用，也便于高精度的
  timer 类型定时器的应用。还有一个重要的因素是 timer wheel 的实现与 jiffies 的耦合性太强，
  非常不便于扩展。因此，自从 2.6.16 开始，一个新的 timer 子系统 hrtimer 被加入到内核中。
- 内核使用全局变量 xtime 来记录这一信息，这就是通常所说的“Wall Time”或者“Real Time”。与
  此对应的是“System Time”。System Time 是一个单调递增的时间，每次系统启动时从 0 开始计时。
- 每个CPU都必须定义两个时钟源：REAL和MONOTONIC。REAL代表实时时钟，MONOTONIC代表单调递增时钟。
  两者的区别在于，当用户更改计算机时间时，REAL时钟会收到影响，但MONOTONIC不受影响。
- wall_to_monotonic 没有被EXPORT_SYMBOL,xtime被EXPORT_SYMBOL
** asmlinkage long sys_nanosleep(struct timespec __user *rqtp, struct timespec __user *rmtp)
*** kernel/timer.c:
- ULK:To be on the safe side, sys_nanosleep( ) adds one tick to the value computed by
  timespec_to_jiffies( ).加上一个tick来作补尝.
- 用TASK_INTERRUPTIBLE休眠,用schedule_timeout()
- 如果没有到时就被退出的话,那么就用当前进程的thread_info->restart_block来重新启动,这个的启
  去原理是什么呢?
- ULK:sys_nanosleep( ) system call. If the value returned by schedule_timeout( ) specifies
  that the process time-out is expired (value zero),the system call terminates. Otherwise,
  the system call is automatically restarted. 所以要想系统调用自动重启就把它放到
  restart_block里。
** fastcall signed long __sched schedule_timeout(signed long timeout)
*** kernel/timer.c:
- 原来是用定时器来实现进程切换的,定时器函数是调用wake_up_process()实现把进程切回来的,这样的
  切回时间会不会不准确呢?
- 可以指定唤醒某个进程但是不能指定切换到某个进程。
** #define udelay(n)
- the built-in function __builtin_constant_p to determine if a value is known to be
  constant at compile-time and hence that GCC can perform constant-folding on expressions
  involving that value.
- i386和arm的实现很不一样，因为i386是有时钟对象一说的，而arm没有。
** void do_gettimeofday(struct timeval *tv)
*** arch/i386/kernel/time.c:
- 这个也有在kernel/time.c里实现，这时是使用time_interpolotor的，但一般的架构都有自已的实现。
- 因为用到了cur_timer->get_offset()，所以精度比jiffies高。
- 这个函数主要做的处理是usec的，对于usec除了从xtime.tv_nsec获取之外还要考虑比jiffies更高精
  度的cur_timer,要考虑NTP的调整；sec就只是从xtime里获取。
- 因为jiffies/wall_jiffies -> lost -> xtime.tv_nsec,所以wall_jiffies的时间与xtime同步的而不
  是jiffies.在update_times()可以看出xtime与wall_jiffies.
** int do_setitimer(int which, struct itimerval *value, struct itimerval *ovalue)
*** kernel/itimer.c:
- ULK:interval timers The timerscause Unix signals (see Chapter 11) to be sent
  periodically to the process. It is also possible to activate an interval timer so that
  it sends just one signal after a specified delay.
- setitimer()的时间精度比alarm()和signal()的高。
- 不知道ovalue什么用。保存旧的timer值？
- setitimer()不是C的标准库，只是Linux的API。
- ITIMER_REAL的which收到SIGALRM,如果task_struct->signal->real_timer定时器已在等待，那么就要
  把它删除重新插入一个新的。ovalue不为0时，会把上一次的task_struct->signal->it_real_incr和
  task_struct->signal->it_real_value保存在ovalue里.
- ITIMER_VIRTUAL是用户态下的时间，收到
- task_struct->signal->real_timer这个timer_list结构体变量原来是给setitimer()和getitimer()使
  用的
** static unsigned long it_real_value(struct signal_struct *sig)
*** kernel/itimer.c:
- timer_pending()这个方法是通过timer_list->base是否为空来判断。
- 因为timer_list->expires是一个绝对时间，所以要减去jiffies。如果差值不为正就要为1，这是为什
  么呢？只有定时器pending的时候才会返回0。
** static inline void it_real_arm(struct task_struct *p, unsigned long interval)
*** include/linux/time.h:
- 这个函数的主要功能就是把task_struct->signal->real_time定时器装上。
- 把interval赋给task_struct->signal->it_real_value有什么用呢？但是interval经过调整之后才把
  它赋给signal->real_timer.expries,这个expires还要加1为了防止interval过小
** void it_real_fn(unsigned long __data)
*** kernel/itimer.c:
- 这个函数就只是在copy_singal()里赋给signal->real_timer.function的，因为这个函数调用
  it_real_arm(),且ti_real_arm()在interval不为0的时候又会把real_timer定时器装上，所以如果
  interval不为0就会一直按照interval间隔的时间产生信号.
** asmlinkage unsigned long sys_alarm(unsigned int seconds)
*** kernel/timer.c:
- 因为sys_alarm()是以ITIMER_REAL的方式调用do_setitimer()的,所以alarm()和setitimer()不能一起
  用.
** void scheduler_tick(void)
*** kernel/sched.c:
- 这个函数是在中断处理程序中调用的,所以这个函数内部是不会调用schedule()的.
- 一定要在调用update_cpu_clock()之后才更新rq->timestamp_last_tick,因为update_cpu_clock()里
  用到了rq->timestamp_last_tick.
- 若rq没有进程可执行了（正在执行idle），那么就要rebalance_tick()来移进程。rebalance之后就不
  执行后面的内容了。
- 在产生时钟中断的时候，有可能进程已不在rq->active里了,但是不是说这时TIF_NEED_RESCHED一定是
  没有设置的吗？什么情况下会出现进程正在运行而进程又不在rq->active里且TIF_NEED_RESCHED也没
  有设置。首先出现进程正在运行而进程又不在rq->active里就不知道怎么解释了,有一种情况就是进程
  A把自已从active里移出，在调用set_tsk_need_resched()的时候产生了时钟中断，这时就会出现这种
  情况，出不出现这种情况就要看会在哪里有把进程从active里移出,移出进程主要是调用
  dequeue_task(),dequeue_task()被
  deactivate_task(),pull_task(),scheduler_tick()schedule(),set_user_nice(),sys_sched_yield()
  调用，deactivate_task()又被
  schedule(),sched_setscheduler(),__migrate_task(),migration_call(),normalize_rt_tasks()调
  用,pull_task()被move_tasks()调用。
- 若current是一个实时的SCHED_RR进程就会算算时间片再判断是否调度，而SCHED_FIFO就不管时间片继续执行。
- 好像只有在这个函数里才会把first_time_slice设为0
- 在进程时间片用完的时候且expired_timestamp(Insertion time of the eldest process in the
  expired lists)为0就把expired_timestamp设为jiffies,expired_timestamp在nr_active为0时会设为
  0，也会在nr_running为0时设为0.但是为什么要在expired_timestamp为0时(就是还没有进程插入
  rq->expired)且在产生时钟中断时把expired_timestamp设为jiffies呢?这里还有一个前提条件就是进
  程的时间片已消耗完,但是消耗完了并一定会把它移到rq->expired里去的,但是在expired_timestamp
  为0时也要设置,不为0就不用,为什么要这样子呢?
- !TASK_INTERACTIVE(p) || EXPIRED_STARVING(rq) 的意思是若进程不是一个交互进程就插入
  rq->expired,若进程是一个交互进程但过时进程已经很久没有执行了也把进程插入rq->expired,否则
  就插入rq->active.
- 调用这个函数就一定会调用rebalance_tick(),有NOT_IDLE和SCHED_IDLE区别。
- 若进程的时间片还没有完，那么就要看看这个进程的时间片是否超过了允许的时间，若是就要把它分
  小片，分小片的方法就是把进程又重新插入到rq->active的尾部并设置需要调度，而不是减小它的时
  间片，这样的结果就是让rq->active的其它进程有运行的机会而不会使rq->expired的进程有积极或消
  极的影响。
** unsigned long long sched_clock(void)
*** arch/i386/kernel/timers/timer_tsc.c:
- 这个函数返回的时钟精度至少不比jiffies差.
** static inline void update_cpu_clock(task_t *p, runqueue_t *rq, unsigned long long now)
*** kernel/sched.c:
- 用现在的时间减去p->timestamp和rq->timestamp_last_tick的最大值累加到p->sched_time.
- p->timestamp和rq->timestamp_last_tick那是绝对的时间.
- p->timestamp的注释是:Time of last insertion of the process in the runqueue, or time of
  last process switch involving the process.p->timestamp是有可能大于
  rq->timestamp_last_tick的,因为进程切换不是在时钟中断里进程切换的,且
  rq->timestamp_last_tick只有在时钟中断里被调用的scheduler_tick()里更新为sched_clock()。
- 如果某个进程访问自已timestamp时就一定是切到这个进程执行时的时间,不会是Time of last
  insertion of the process in the runqueue。
- 这个函数在只在scheduler_tick()和schedule()里调用，schedule()里调用时是作用于被切换的进程,所
  以不管某个进程执行的过程中是否有产生过时钟中断都可以正确地把进程的执行时间计到
  p->sched_time里。p->sched_time这个时间不是与当前时间同步的。p->sched_time表示的是进程一共
  执行了多长的时间。而signal->sched_time表示的是已死去的线程组中线程的执行时间。
** static inline int wake_priority_sleeper(runqueue_t *rq)
*** kernel/sched.c:
- 这个函数只被scheduler_tick()在current是idle的时候调用
- 支持超线程的时候若发现rq->nr_running还有进程调进程调度，为什么要在支持超线程的时候才判断
  rq->nr_running呢？若不支持超线程这个函数就是没有任何作用的。
- 若正在执行idle就说明了rq没有可运行的进程了，但当前CPU可能还有正在休眠的进程，放到rq里的进
  程不包括在休眠的进程。
** static int try_to_wake_up(task_t * p, unsigned int state, int sync)
*** kernel/sched.c:
- p->state的状态要包含在state里才会往下执行。
- 当p->array不为空时就说明进程已是在运行状态，不管是在active还是在expires链表中。
- 这个函数主要是在让被唤醒的进程在哪个CPU运行的选择上做了很多工作，就是调度域的问题。
- 一般情况下,被唤醒的进程是在进程描述符指定的那个CPU上执行的.进程只会在进程描述符指定的CPU
  和当前CPU上迁移.
- 如果当前的CPU的负载很高而进程描述符指定的CPU的负载很低,那么就不要移了.
- 文档sched-domain.txt:A sched domain's span means "balance process load among these
  CPUs".base scheduling domain就是一个物理CPU的scheduling domain,CPU i一定被包含在CPU i的
  base scheduling domain里.每个scheduling domain会span几个CPU.Each scheduling domain must
  have one or more CPU groups (struct sched_group).每个scheduling domain有一个或多个组.The
  union of cpumasks of these groups MUST be the same as the domain's span.某个scheduling
  domain内的所有组的cpumasks的和一定是这个scheduling domain所包含的CPU.The intersection of
  cpumasks from any two of these groups MUST be the empty set.Groups may be shared among
  CPUs as they contain read only data after they have been set up.平衡是在组与组之间的进行
  的,那么在不同scheduling domain之间移也是通过组来进行的吗?Balancing within a sched domain
  occurs between groups.组的负载是组中所有CPU负载的总和,组负载超时就移,那么如果组中的一个
  CPU很大负载而其它负载很小呢?我觉得有这种情况的组应该还有子组.The load of a group is
  defined as the sum of the load of each of its member CPUs, and only when the load of a
  group becomes out of balance are tasks moved between groups.
- At the hyperthreaded processor level: balancing attempts can happen often (every 1-2ms),
  even when the imbalance between processors is small. There is no cache affinity at all:
  since hyperthreaded processors share cache, there is no cost to moving a process from
  one to another. Domains at this level are also marked as sharing CPU power; we'll see
  how that information is used shortly.超线程一级里的CPU是会每1-2ms就均衡一次就算差别很小,
  之间没有亲和之说因为CPU之间是共享cache的.
- At the physical processor level: balancing attempts do not have to happen quite so
  often, and they are curtailed fairly sharply if the system as a whole is busy. Processor
  loads must be somewhat farther out of balance before processes will be moved within the
  domain. Processes lose their cache affinity after a few milliseconds.
- balancing attempts are made relatively rarely, and cache affinity lasts longer. The cost
  of moving a process between NUMA nodes is relatively high, and the policy reflects that.
- SD_WAKE_IDLE:when a sleeping process is about to be awakened, the normal behavior would
  be to keep it on the same processor it was using before, on the theory that there might
  still be some useful cache information there. If that processor's scheduling domain has
  the SD_WAKE_IDLE flag set, however, the scheduler will look for an idle processor within
  the domain and move the process immediately if one is found. This flag is used at the
  hyperthreading level; since the cost of moving processes is insignificant, there is no
  point in leaving a processor idle when a process wants to run.调度域有SD_WAKE_IDLE标志说
  明会把该调度域的进程移到IDLE CPU上去.用于hyperthreading level.
- When a process calls exec() to run a new program, its current cache affinity is lost. At
  that point, it may make sense to move it elsewhere. So the scheduler works its way up
  the domain hierarchy looking for the highest domain which has the SD_BALANCE_EXEC flag
  set. The process will then be shifted over to the CPU within that domain with the lowest
  load. Similar decisions are made when a process forks.调度域设置SD_BALANCE_EXEC时,若执行
  了exec()那么就移到其它CPU,因为cache肯定丢失.
- If a processor becomes idle, and its domain has the SD_BALANCE_NEWIDLE flag set, the
  scheduler will go looking for processes to move over from a busy processor within the
  domain. A NUMA system might set this flag within NUMA nodes, but not at the top level.这
  个与SD_WAKE_IDLE是不一样的,SD_WAKE_IDLE是进程被唤醒后发现有IDLE的CPU说移过去,而
  SD_BALANCE_NEWIDLE是某个CPU发现自已IDLE了就从其它的CPU上移进程过来.
- Every scheduling domain has an interval which describes how often balancing efforts
  should be made; if the system tends to stay in balance, that interval will be allowed to
  grow. The scheduler "rebalance tick" function runs out of the clock interrupt handler;
  it works its way up the domain hierarchy and checks each one to see if the time has come
  to balance things out. If so, it looks at the load within each CPU group in the domain;
  if the loads differ by too much, the scheduler will try to move processes from the
  busiest group in the domain to the most idle group. In doing so, it will take into
  account factors like the cache affinity time for the domain.
- 调度域有四种:根据不同的硬件结构自上而下有NUMA、物理、核和超线程.一个NUMA可以有多个物理
  CPU组成,一个物理CUP有多个核组成,一个核可以有多个逻辑CPU.
- 系统中的每一个逻辑CPU都会关联一组调度域，相同类型的逻辑CPU处于同一个调度域中。
  http://blog.chinaunix.net/uid-7295895-id-2941412.html
- 系统中存在的负载有：中断、异常、软中断、系统调用、任务；系统调用是在任务的上下文中执行的，
  异常不可控制且一般也是由于任务触发可以忽略，只剩下中断、软中断和任务；有些中断可以通过
  CPU的中断亲和掩码在多个CPU上做均衡(X86架构中已有此实现，CONFIG_IRQBALANCE)，软中断过高的
  系统可以考虑软中断线程化后算成任务或RPS补丁(CONFIG_NETDEVICES_RPS)将软中断负载均衡至多个
  CPU间；标准内核的负载均衡算法是针对任务的。
- 历史权重目的是用来平滑静态权重，以缓解负载均衡时系统性能发生抖动。
- 系统运行过程中，内核会不停地进入负载均衡流程对当前cpu遍历其所关联的所有调度域（自下向上）；
  对每一个调度域遍历所属的所有调度组，找出一个负载最大的调度组（不能是当前cpu所属的调度组）；
  遍历此调度组中所属的cpu，找出静态权重符合条件的目标cpu，最后将此cpu上的可运行任务迁移至当
  前cpu（称为拉任务），以期达到负载均衡状态。整个过程可抽象为如下步骤：负载均衡算法会将静态
  权重load_rq、调度域参数、历史权重cpu_load[i]等信息，按照一定的规则计算出一个动态权重load
- 有了动态权重后，就可以用来判别系统负载是否均衡了：记当前cpu动态权重值为load_current，目标
  cpu动态权重值为load_target，判别规则类似如下：imbalance_pct *load_current <
  100*load_target(try_to_wake_up()函数有用到这个判断)其中，imbalance_pct为当前cpu的某个调度
  域参数，例如物理和核域为125；判别结果为真表示目标cpu负载过重，就会从目标cpu迁移任务至当前
  cpu。
- 算法均衡的时机共有定时均衡、闲时均衡、唤醒均衡和创建时均衡四种。时钟中断时，会根据条件触
  发均衡软中断（内核为其设置了专门的软中断），相应均衡函数为load_balance()，此函数只在调度
  组中的第一个cpu或第一个idle状态cpu上作均衡；此称为定时均衡（又可分为定时忙均衡和定时闲均
  衡）。任务调度时，会检查当前cpu是否空闲，如果空闲则进行负载均衡，相应均衡函数为
  idle_balance()，与定时均衡相比，只要拉到任务就会返回属于轻量级均衡算法(但实时系统中，此为
  主要均衡时机，因为发生频率较高)；此称为闲时均衡（又称NEW_IDLE均衡）。唤醒任务时，会在
  try_to_wake_up()中根据当前cpu的调度域参数决定是否进行负载均衡；此称为唤醒均衡（唤醒均衡倾
  向于将被唤醒任务迁移至当前cpu所在的调度域中的某个cpu上，如果系统中存在过于频繁的唤醒，此
  操作在某些NUMA系统中反而会造成负载不均衡）。创建任务时，会在sched_fork()/sched_exec()中，
  将此任务迁移至一个相对空闲的CPU上运行。称此为创建时均衡(包括fork和execve)。
- 任务迁移包括拉任务(一次操作多个任务)和推任务(一次操作一个任务)。拉任务指的是从目的CPU迁移
  任务至当前CPU，对应实现为pull_task()；定时均衡、闲时均衡和唤醒均衡对应的是拉任务。推任务
  指的是从当前CPU迁移任务至目的CPU，内核在每个运行队列中存放一个内核线程负责处理推任务请求；
  定时均衡多次失败时，置推任务标志，唤醒相应内核线程；创建时均衡(sched_exec)，发推任务请求，
  唤醒相应内核线程；设置任务的cpu亲和掩码时，发推任务请求，唤醒相应内核线程。实际负载均衡过
  程中，主要为拉任务操作。
- 任务迁移也要符合一定的规则，不同优先级按照由高到低、同种优先级按照LIFO的顺序进行选择任务。
  但对如下几种类型的任务不能被迁移：1) 任务的cpu亲和（见1.9）不允许被迁移到当前cpu；2) 为目
  标cpu上的当前运行任务；3) 任务具有cache亲和性(参考try_to_wake_up)；4) 任务被迁移到当前
  cpu上会产生新的失衡。但如果任务迁移到当前cpu后优先级为最高，这时就考虑让优先级高的任务先
  运行；但如果要迁移的是目标cpu上的最高优先级任务，且最高优先级任务只有一个，也不允许迁移。
- 每一个任务都会关联一个cpu亲和属性，该属性值的每一位表示此任务运行及迁移所允许的cpu集合
  （置位表示允许），cpu亲和影响均衡操作中的任务迁移（见1.8）。cpu亲和的设置可通过线程绑定和
  排它绑定来实现（见2.2和2.3）。
- sched_domain->span:表示调度域包含的CPU集合.sched_domain->last_balance记录了上次做定时均衡
  的时间点，单位为tick，只在定时均衡中更新，和balance_interval一起用于计算下次做定时均衡的
  时间；sched_domain->balance_interval也只在定时均衡中更新，单位为毫秒，用于计算下次做定时
  均衡的时间：last_balance+msecs_to_jiffies(balance_interval)，最终根据情况将结果更新记录于
  运行队列rq->next_balance中；定时均衡成功迁移任务后，意味着此时系统的负载倾向于不均衡，将
  balance_interval置为min_interval；定时均衡没有发生任务迁移，意味着此时系统的负载倾向于均
  衡，将balance_interval加倍，但不超过max_interval(如果当前CPU的任务都已绑定，则不超过
  MAX_PINNED_INTERVAL，定义为512ms)；sched_domain->busy_factor只用于定时均衡，如果当前CPU不
  是空闲的(定时忙平衡)，就将变量interval间隔乘上此字段，这样在当前CPU忙时不会太频繁进入负载
  均衡算法，避免负载均衡算法本身带来较大的开销。sched_domain->imbalance_pct用作均衡判别，见
  1.5节；此值增大可放宽负载失衡的判别条件，从而降低负载均衡的频率；
  sched_domain->cache_nice_tries，sched_domain->nr_balance_failed用于定时均衡中连续迁移任务
  失败后，发起推任务操作，判断条件为nr_balance_failed > (cache_nice_tries+2)；busy_idx,
  idle_idx, newidle_idx, wake_idx, forkexec_idx用作下标选择运行队列中的历史权重cpu_load[]数
  组中的元素
- SD_LOAD_BALANCE表示做负载均衡；做负载均衡就会做定时均衡，定时均衡不能单独关闭；
- SD_BALANCE_NEWIDLE表示做闲时均衡；
- SD_BALANCE_EXEC表示做创建时均衡中的execve均衡；
- SD_BLANCE_FORK表示做创建时均衡中的fork均衡；
- SD_WAKE_IDLE表示开启超线程后在try_to_wake_up()中将被唤醒任务迁移至一个空闲CPU的上运行(如
  果有的话)；
- SD_WAKE_AFFINE表示在try_to_wake_up()中考虑任务的cache亲和，如果存在cache亲和就不将此任务
  拉至当前CPU；
- SD_WAKE_BALANCE功能类似于SD_WAKE_AFFINE，只是算法判别准则不同；
- SD_SHARE_CPUPOWER表示共享CPU的处理能力，主要用于同一个核的超线程CPU间；从而让操作系统感知
  到超线程硬件特性予以特殊处理；
- SD_SHARE_PKG_RESOURCES用于初始化调度组的CPU能力；
- SD_SERIALIZE主要用于不同CPU间的负载均衡算法的串行化(增加了一把自旋锁)，一般NUMA域可设置此
  选项；
- BALANCE_FOR_MC_POWER和BALANCE_FOR_PKG_POWER用于节能，负载均衡可以和CPU的变频技术以及S5节
  能技术结合起来，用以将空闲的CPU降频或休眠，
- 任务的亲和性计算是指在被唤醒时选择一个可运行的CPU，该CPU最好保存有任务以前的cache内容。任
  务的亲和性计算在try_to_wake_up中进行，计算内容是在任务所属CPU和执行唤醒操作的本地CPU之间
  选择一个运行，选择的依据是CPU的cache内容是否失效和CPU的负载情况。Linux 2.6最初的亲和性计
  算是为每个调度域设一个cache失效的时间值，当被唤醒任务的睡眠时间超过失效时间时就认为任务与
  CPU之间已失去亲和性。这种算法的缺点是无法准确估计cache是否失效，因为如果一个时间片很长的
  任务一直在运行那么cache失效的可能性就很小。当前Linux 2.6的亲和性计算是根据CPU的任务切换频
  率来估计cache的失效可能性，如果有很多时间片很短的任务在CPU上运行，那么cache失效的可能性就
  很大。
- blog.chinaunix.net/uid-7295895-id-2941412.html有关于任务亲和性的计算过程
- 如果目的CPU和任务之间具有亲和性，即cond1和cond2条件同时为假，那么应该选择目的CPU。这时目
  的CPU上cache失效的可能性小且任务加在本地CPU上以后会造成负载失衡。对于任务加在本地CPU上造
  成负载失衡的原因，可以分为三种情况：i.  本地CPU负载小于目的CPU，且处于失衡状态。但任务权
  重很大，在本地CPU加上任务以后打破了失衡，使本地CPU负载反大于目的CPU而产生新的失衡；但如果
  将任务加载目的CPU上反而会扩大失衡。ii.  本地CPU负载接近目的CPU，处于均衡状态。本地CPU权重
  加上任务权重以后造成负载的失衡。iii.  本地CPU负载大于目的CPU，且处于失衡状态。这时本地
  CPU加上任务以后使失衡更加扩大。对于情况a)鼓励将任务加在本地CPU上；对于情况b)在考虑cache有
  效性的因素下鼓励任务加在目的CPU上；对于情况c)鼓励任务加在目的CPU上。因此在调度域设置允许
  的情况下需要对本地CPU和目的CPU的负载进行判断，判断的条件如下：imbalance*this_load <=
  100*load等价于(imbalance/100)*this_load <= load其中this_load是本地CPU的权重值；imbalance
  是负载平衡系数；load是目的CPU的权重值。(imbalance/100)恒大于1，因此可以看出，如果条件成立
  那么目的CPU与本地CPU的权重值之比超过负载平衡系数，而产生负载失衡。如果以上的条件成立则出
  现了情况a)，这时应选择本地CPU；否则应选择目的CPU。
- 然而实时性系统多为实时任务，实时任务的权重值固定为177522（见1.2节），再加上某些业务过高的
  网络软中断的影响，导致内核原有的亲和算法不能很好地发挥作用，反而会起到副作用。1. 在电信级
  业务的场景中，NUMA域默认参数存在两个问题。a)产品会伴随着大量的网络收发包，进而产生大量的
  网口中断和软中断，频繁的网络软中断就会产生大量的唤醒任务操作，导致唤醒均衡算法不停地将任
  务拉至中断所在的NUMA节点的CPU上；b)同时很多业务对系统中每一个CPU的利用率比较敏感，要求各
  个CPU利用率严格均衡。所以需要更改NUMA域的默认配置参数来解决这两个问题：第1个需要去掉NUMA
  域的唤醒均衡；第2个需要配上NUMA域的闲时均衡。2. 超线程域默认参数配有SD_WAKE_AFFINE算法。
  此参数在某些业务场景会导致系统性能发生抖动，进而影响系统性能；但与SD_WAKE_BALANCE算法相比
  影响相对较小，在出现开启硬件超线程特性后如果出现了性能抖动，就可以考虑去除此参数。
- 2.6.12的内核好像只是把运行队列里的进程数乘SCHED_LOAD_SCALE就用负载了，与进程优先级没有关
  系。
- source_load()取rq->cpu_load与当前负载的最小值，target_load()取rq->cpu_load与当前负载的最
  大值，值越大进程数越多。
- 同步唤醒就是唤醒后不马上执行。这种情况下把本地CPU的负载减小被唤醒进程的负载量,就是说用假
  设被唤醒进程加到当前CPU后的负载量所算出的当前的本地CPU的负载量来进行以后的计算。
- 若task->cpu的负载小于SCHED_LOAD_SCALE/2且本地CPU的负载大于SCHED_LOAD_SCALE/2就说明
  task->cpu的负载小而本地CPU的负载大。
- task->last_ran只在schedule()与task->timestamp赋相同的值。
- task_hot()用rq->timestamp_last_tick(运行队列的上一次切换时间)减去被唤醒进程在rq里被切换出
  去的时间所得到的时间差与调度域的cache_hot_time比，若后者大就说明进程的cache亲和力不大了，
  可以拉到本地CPU上运行。在做这个判断之前要先确定SD_WAKE_AFFINE已经设置。
- 用imbalance*this_load <= 100*load来判断不均衡度是否超过了所要求的，在这判断之前要确定是否
  设置了SD_WAKE_BALANCE.
- 调用wake_idle()处理设置了SD_WAKE_IDLE标志的情况。
- 进程要移出task->cpu时，就要解原rq的锁和给新的rq加上锁，这里有个时间窗口，在这个时间窗口里
  可能会修改进程的状态使它与参数state不相符或将task->array不为空以示进程在运行状态，这些都
  要做相应的处理，但无论是否出现这两种情况，进程已经是被移到其它CPU上了。但在状态不相符的情
  况下是把task->cpu修改了，但没有将进程的task->array赋予相应CPU的运行队列（但这没有问题，若
  在加上锁时state被其它进程调用try_to_wake_up()改成了TASK_RUNNING的时候也会把task->array改
  成相应的运行队列，若在加上锁时已被改成了TASK_INTERRUPTIBLE(与参数的state不相同)，那么不用
  给task->array赋值。所以加上锁之后发现task->state若与参数的state不相同就不用管task->array
  了，也不用设置task->state了。
- http://linux.cn/thread/2082/1/1/
- rq->nr_uninterruptible有什么用呢？ULK：Number of processes that were previously in the
  runqueue lists and are now sleeping in TASK_UNINTERRUPTIBLE state(only the sum of these
  fields across all runqueues is meaningful)为什么说要加起来才有用效呢？因为从
  try_to-wake_up()可以看出：如果进程已经确定要拉到本地CPU上执行了，也就是改了task->cpu，这
  时的rq(假设是rq1)与之前的rq(假设是rq2)不一样了，所以进程在进入TASK_UNINTERRUPTIBLE时是在
  rq1上对nr_uninterruptible上做自加操作的，而拉到rq2之后就在rq2上对nr_uninterruptible做自减
  操作。
- task->activated改为-1有什么用呢？
** static int wake_idle(int cpu, task_t *p)
*** kernel/sched.c:
- 注释：wake_idle() will wake a task on an idle cpu if task->cpu is not idle and an idle
  cpu is available.
- 在扫描所有域的CPU时要注意：域的SD_WAKE_IDLE是否有设置，域所包含的cpu要在cpu_online_map里，
  将拉的到的CPU要在task->cpus_allowed里,若CPU满足这些条件且该CPU也是idle就返回这个cpu.
** static void recalc_task_prio(task_t *p, unsigned long long now)
*** kernel/sched.c:
- task->timestamp:Time of last insertion of the process in the runqueue, or time of last
  process switch involving the process
- task->activated为-1表示是从TASK_UNINTERRUPTIBLE唤醒的，在try_to_wake_up()里可以看出。
- ULK:Checks whether the process is not a kernel thread, whether it is awakening from the
  TASK_UNINTERRUPTIBLE state (p->activated field equal to -1; see step 5 in the previous
  section), and whether it has been continuously asleep beyond a given sleep time
  threshold. If these three conditions are fulfilled, the function sets the p->sleep_avg
  field to the equivalent of 900 ticks
- ULK:The sleep time threshold depends on the static priority of the process;休眠时间的阀值
  依赖进程的静态优先级。
- 这个函数前面一大段是算出合适的task->sleep_avg,最后调用effective_prio()算出task->prio.而前
  面一段就注意两种情况：1.进程不是内核线程(task->mm)且进程不是从TASK_UNINTERRUPTIBLE唤醒且
  这一次的休眠时间已大于阀值就把task->sleep_avg设为最大的值；2.进程从TASK_UNINTERRUPTIBLE唤醒且进程
  不是内核线程且task->sleep_avg大于或等于阀值或task->sleep_avg加上这一次的休眠时间大于或等于阀值。
- 从这个函数可以看出进程的动态优先级是在这里修改的。
** asmlinkage void __sched schedule(void)
*** kernel/sched.c:
- profile_hit()在这里调用了
- 如果是一个进程直接调用schedule()说明调度的原因是想要的资源不能得到满足吗？
- 在这里,prev->timestamp一定是prev被切回来执行的时间，所以用sched_clock()返回的当前时间减去
  prev->timestamp就是prev这次的执行时间(run_time)，不管执行过程中产生了多少中断。
- CURRENT_BONUS()返回的值是0-10，算出这个运行时间只是最终用task->sleep_avg减去它再赋给
  task->sleep_avg
- 好像无论是EIXT_DEAD还是EIXT_ZOMBIE都是PF_DEAD这个标志，为什么在这里会有PF_DEAD的标志呢？
  为什么为PF_DEAD时要设置EXIT_DEAD
- 若prev状态是TASK_INTERRUPTIBLE且有信号挂起，那么就要把进程状态改为TASK_RUNNING,但好像在
  schedule()函数里没有看到把进程的放到运行队列里啊?
- 只有在prev进程的状态不是TASK_RUNNING且可以抢占prev(PREEMPT_ACTIVE没设)才会调用且状态不是
  TASK_INTERRUPTIBLE和有信号挂起同时存在时的会调用deactivate_task()把进程从运行队列里除掉
- idle_balance()只在schedule()里调用,注释：idle_balance is called by schedule() if
  this_cpu is about to become idle.在rq->nr_running为空就是没有可以运行的进程时才调用
  idle_balance()
- rq->exipired_timestamp:Insertion time of the eldest process in the expired lists.如果
  nr_running为空那么就要把expired_timestamp置0.
- smt:Simple Multi-Threading，SMT（同时多线程，也称超线程）
- rq->active和rq->expires是在这里调换的，调换之后rq->expired_timestamp设为0，
  rq->best_expired_prio设为MAX_PRIO
- task->activated的注释： 0- was TASK_RUNNIG, 1- was TASK_INTERRUPTIBLE and TASK_STOPPED
  awaked by system call or kernel thread 2- was TASK_INTERRUPTIBLE and TASK_STOPPED awaked
  by interrupt handler or a deferrable function, -1- was TASK_UNINTERRUPTIBLE state and it
  is being awakened.注意是was而不是is，就是说表示的是上一个状态。而不是现在的状态，现在的状态在state里。
- 如果next进程的上一个状态是TASK_INTERRUPTIBLE或TASK_UNINTERRUPTIBLE，那么在schedule()里被
  切回来的时候才会计算它的优先级.为什么计算之前要把它从运行队列里删除呢？处理完这个
  task->activated之后就把它改回0
- 为什么切换之前要把prev的TIF_NEED_RESCHED给清掉吗？不可以设置吗？
- 因为prev执行schedule()之前已经是执行了一段时间了，所以task->sleep_avg要减去执行的时间换算的一个值。
- 在这里prev的timestamp和last_ran都改成了现在的时间。
** static inline void wake_sleeping_dependent(int this_cpu, runqueue_t *this_rq)
*** kernel/sched.c:
- 这个函数只在支持多线程的情况下才使用。
- 函数的作用是把参数this_cpu所在的域的所有超线程CPU都检查是否正在执行idle进程且运行队列还有
  其它的进程，若是就要求切换那个CPU的idle进行调度。
- 在访问运行队列时是在NUMA一级，还是在物理CPU一级，还是在核一级，还是在超线程一级呢？其实在
  超线程一级
- 从blog.chinaunix.net/uid-7295895-id-2941412.html的描述来看线程域不是只包含一个线程逻辑
  CPU，它至少包含一个核的里所有的线程逻辑CPU，所以SD_SHARE_CPUPOWER应该是在线程域里指定的，
  表示共享CPU的处理能力，从而让操作系统感知到超线程硬件特性予以特殊处理。
- 所以wake_sleeping_dependent()会在一开始就判断this_rq所在的域的SD_SHARE_CPUPOWER有没有设置，
  设置了就表明this_rq->sd域是一个线程域，这时this->rq->span里的CPU就是同一个核里的逻辑CPU。
- 因为最低一层的线程域包含的是一个核里的所有逻辑CPU，所以逻辑CPU不能自已单独成为一个域了，
  但是线程域里的调度组可以是一个逻辑CPU的。在这个函数里会用循环遍历线程域里所有的逻辑CPU的
  运行队列，可以看出per_cpu变量会用到逻辑CPU一级的。所以一个双核四线程的per_cpu变量大小是4.
- 这个函数会释放this_rq的锁后又给它加上锁，所以退出这个函数的时候可能一些情况发生了变化。
** static inline int dependent_sleeper(int this_cpu, runqueue_t *this_rq)
*** kernel/sched.c:
- SD_SHARE_CPUPOWER也要被检查
- 从代码上看，进程p是当前逻辑CPU的下一个要进行执行的进程，就是将要被切换的进程。
- 返回1给schedule()时会让它切换到idle进程。防止切换之后会与另一个逻辑CPU的进程抢CPU资源
- 在什么情况下返回0呢？1.某个逻辑CPU正在执行的进程的时间片比进程p的小且某个逻辑CPU的进程不
  是实时进程;2.p是内核线程；3.某个逻辑CPU正在执行的进程是内核线程;4.p是实时进程。以上条件满
  足一个就返回0。
- 因为schedule()调用了这个函数，所以可以看出就算运行队列里有进程也不一定切换到它们而是切换
  到idle.
** static inline void finish_task_switch(task_t *prev)
*** kernel/sched.c:
- 这个函数的主要功能就是释放一下mm和task_struct.
- 如果进程是PF_DEAD才会调用put_task_struct(),但不一定会释放进程描述符。而mm就不是了，调一次这个
  函数就调一次mmdrop()，要释放mm结构体还要等到计数为0。
** static struct sched_group * find_busiest_group(struct sched_domain *sd, int this_cpu, unsigned long *imbalance, enum idle_type idle)
*** kernel/sched.c:
- BALANCE_FOR_MC_POWER和BALANCE_FOR_PKG_POWER用于节能，负载均衡可以和CPU的变频技术以及S5节
  能技术结合起来，用以将空闲的CPU降频或休眠，具体可参考find_busiest_group()函数的实现，基本
  思想就是开启节能选项后(选项开关可通过sched_mc_power_savings和sched_smt_power_savings控制
  选中SD_POWERSAVINGS_BALANCE)将负载轻的CPU的任务迁移至负载重的CPU，直至系统中产生空闲的
  CPU。
- 这个函数首先会找出负载最重的组再以节能算法找出真正的负载最重的组。
- 从imbalance参数返回的是要移多少个进程。
** static runqueue_t *find_busiest_queue(struct sched_group *group)
*** kernel/sched.c:
- 遍历组里所有CPU找出负载最重的CPU。
** static int load_balance(int this_cpu, runqueue_t *this_rq, struct sched_domain *sd, enum idle_type idle)
*** kernel/sched.c:
- 先调用find_busiest_group()找出最忙的组，再调用find_busiest_queue()找出组中最忙的CPU.
- 调用move_tasks()批量移进程
- 若move_tasks()一个进程也没有移成，那么就要增加nr_balance_failed计数给can_migrate_task()用
- sched_domain->last_balance记录了上次做定时均衡的时间点，单位为tick，只在定时均衡中更新，
  和sched_domain->balance_interval一起用于计算下次做定时均衡的时间。在这个函数里，如果虽然
  不平衡行但move_tasks()也没有成功移过一个进程，那么就要增加下一次的定时平衡时间，使得不要
  那么频繁进行对这个域做平衡。
- 如果是move_tasks()没有移成功过一个进程且失败率大于sched_domain->cache_nice_tries+2那就要
  设置忙运行队列的active_balance(Flag set if some process shall be migrated from this
  runqueue to another).如果之前active_balance之前没有设置过就要唤醒最忙运行队列的迁移进程的
  内核线程(sched_domain->migration_thread)
- 如果move_tasks()有成功移过进程，那么就把nr_balance_failed给清零，并把balance_interval设为
  min_interval
- 如果没有找到最忙的组或最忙的运行队列或最忙的队列是本地队列，同时balance_interval比
  max_interval要小，那么就要将balance_interval翻倍。
** static void double_lock_balance(runqueue_t *this_rq, runqueue_t *busiest)
*** kernel/sched.c:
- 不同运行的队列的锁是否有某种关系呢？为什么在busiest的锁trylock不成功且busiest小于this_rq
  时会先解this_rq->lock这个锁再加锁呢？busiest比this_rq的地址小说明了什么问题呢？
** static int move_tasks(runqueue_t *this_rq, int this_cpu, runqueue_t *busiest, unsigned long max_nr_move, struct sched_domain *sd, enum idle_type idle)
*** kernel/sched.c:
- 参数sd只是给做一些调度统计用的，和传给can_migrate_task()用的。
- 先考虑移expired链表上的，若expired上没有进程就再考虑移active里的。若是从busiest里的
  expired移，就要移到this_rq的expired上，active同理。
- 这里的goto用得好恐怖啊
- 这个函数就是把指定的CPU的运行队列里的进程放到指定的其它CPU运行队列里。
- 从skip_bitmap这个点开始下面的几行代码来看，看从active里的最高优先级进程开始移，接着再从
  expired的最高优先级开始,直到把参数指定要移的进程数max_nr_move已经移完。
** static inline int can_migrate_task(task_t *p, runqueue_t *rq, int this_cpu, struct sched_domain *sd, enum idle_type idle)
*** kernel/sched.c:
- 在什么情况下不移呢：进程正在运行；进程不允许在那个CPU上执行；被移的进程所在的CPU对于该进
  程是cache-hot的；但是如果其它的sliblings CPU都是idle的或太多平衡操作失败了，就算进程是
  cache-hot的也要移。
** static inline void pull_task(runqueue_t *src_rq, prio_array_t *src_array, task_t *p, runqueue_t *this_rq, prio_array_t *this_array, int this_cpu)
*** kernel/sched.c:
- 把进程在原来运行队列已经不运行的时间改成在新运行队列已经不运行的时间，做法就是用进程上一
  次被切换出去的时间戳(task->timestamp)减去原来运行队列所记录的上一次时间中断的时间
  (timestamp_last_tick)(这个时间差有可能是正数(进程的上一次切换是自愿的且在切换的之后没有发
  生过时钟中断)，也有可能是负数))再加上本地运行队列的timestamp_last_tick.
- 在这个函数里如果发现被移的进程的优先级比当前的进程还高，那么就设置当前进程的调度标志。
** asmlinkage long sys_nice(int increment)
*** kernel/sched.c:
- increment是一个增量,这个进程优先级的增量。
- nice的范围是-20到19
- 进程的优先级是0到140的
- 如果nice为负数，就是增量为负，那么要超级用户才可以操作。
** void set_user_nice(task_t *p, long nice)
*** kernel/sched.c:
- 要加上运行队列的锁，因为进程可能正在另一个CPU调度中被调度。
- 如果是一个实时进程，那么就调用PRIO_TO_NICE来修改它的优先级,因为NICE_TO_PRIO是这样定义
  的#define NICE_TO_PRIO(nice) (MAX_RT_PRIO + (nice) + 20)，所以调用之后原来是实时的进程就
  不再是实时进程了。但是有这样的注释：but as expected it wont have any effect on
  scheduling until the task is not SCHED_NORMAL.
- 因为会修改进程优先级，所以要先把进程从运行队列里删除，修改完之后再插入到合适的优先级。
- 其实nice值是可以通过静态优先级来换算的，所以进程描述符里没有类似nice这样的成员。修改nice
  值就是修改进程的静态优先级。
- 通过nice值修改进程的动态优先级和静态优先级的时是不同的，但是从代码的结果看修改的结果是一
  样的啊，为什么要多此一举呢？修改后动态优先级和静态优先级是一样的。
- nice()系统调用是修改当前进程的.但这个函数是可以指定不同的进程的.所以在sys_nice()系统调用
  里调用的set_user_nice()是以current作为参数调用的.
- 如果被修改的进程的动态优先级被改小了,那么就要把被修改进程所在的运行队列正在运行的的进程调
  度一下而不管被修改的进秵的动态优先级是否比当前执行的进程的优先级高.或如果动态优先级被改大
  了同时被修改的完程正在执行那么也对自已进行调度.
** #define rt_task(p)		(unlikely((p)->prio < MAX_RT_PRIO))
*** include/linux/sched.h:
- 通过优先级来判断就可以了，就是那么简单。
** asmlinkage long sys_getpriority(int which, int who)
*** kernel/sys.c:
- PRIO_PROCESS:Selects the processes according to their process ID,找出指定进程的nice,范围
  是40-1.
- PRIO_PGRP:Selects the processes according to their group ID,找出指定进程所在的进程组的
  nice值最大的进程.
- PRIO_USER:Selects the processes according to their user ID,因为用户的进程没有用链表链起
  来,所以要遍整个进程链表才可以找出所有的进程.
** asmlinkage long sys_setpriority(int which, int who, int niceval)
*** kernel/sys.c:
- 结构上与sys_getproirity()类似,只是不是比较而是调用了set_one_prio()修改nice.
** static int set_one_prio(struct task_struct *p, int niceval, int error)
*** kernel/sys.c:
- 若进程的用户id(uid)不等于当前进程的有效用户id(euid)且进程的有效用户id(euid)不等于当前进程
  的有效用户id(euid),那就不允许修改除非是超级用户.
- 最后还是与sys_nice()一样调用set_user_nice()来修改进程的nice.
** asmlinkage long sys_sched_getaffinity(pid_t pid, unsigned int len, unsigned long __user *user_mask_ptr)
*** kernel/sched.c:
- 主要是调用了sched_getaffinity()
** long sched_getaffinity(pid_t pid, cpumask_t *mask)
*** kernel/sched.c:
- 找出进程pid可以被调度到的CPU的掩码,就是task->cpus_allowed.
** asmlinkage long sys_sched_setaffinity(pid_t pid, unsigned int len, unsigned long __user *user_mask_ptr)
*** kernel/sched.c:
- 就是调用sched_setaffinity()
** long sched_setaffinity(pid_t pid, cpumask_t new_mask)
*** kernel/sched.c:
- 主要是调用cpuset_cpus_allowed()找出允许执行该进程的CPU，再用参数掩码相与,最后的结果给
  set_cpus_allowed()
** int set_cpus_allowed(task_t *p, cpumask_t new_mask)
*** kernel/sched.c:
- 新的掩码不在cpu_online_map里的话就返回。
** asmlinkage long sys_sched_getscheduler(pid_t pid)
*** kernel/sched.c:
- 就只是返回task->policy.
- SCHED_FIFO, SCHED_RR , or SCHED_NORMAL (the latter is also called SCHED_OTHER)
** asmlinkage long sys_sched_getparam(pid_t pid, struct sched_param __user *param)
*** kernel/sched.c:
- 获取的是实时进程的优先级，就是task->rt_priority,把task->rt_priority放到参数的
  param->sched_priority.
** 
- ULK:Child process stopped or terminated, or got signal if traced.所以在停止或终于或被跟踪
  时收到。
- 常规信号与中断一样是不会排队的，但是POSIX的实时信号是会排队的。
- 信号在处理时是不可以被其它的信号中断的
- 从用户态切到内核态时会检查是否有信号在挂起
- ULK:If a process receives a signal while it is being traced, the kernel stops the
  process and notifies the tracing process by sending a SIGCHLD signal to it. The tracing
  process may, in turn, resume execution of the traced process by means of a SIGCONT
  signal.被跟踪的进程收到信号时将会停止进程接着会发SIGCHLD给跟踪进程，若要恢复被跟踪进程的
  执行，那么跟踪进程就要发SIGCONT给被跟踪进程。
- 进程描述符的ptrace的PT_PTRACED为0就说明进程没有被跟踪。
- 信号的阻塞和怱略是不一样的，ULK:A signal is not delivered as long as it is blocked; it
  is delivered only after it has been unblocked. An ignored signal is always delivered,
  and there is no further action.原来怱略的信号总是被递送的。
- 多线程间，信号的处理函数要共享，但有各自的挂起和阻塞掩码，调用kill()和sigqueue()时，按
  POSIX的要求要把信号发给线程组里所有的进程；发给一个线程的信号可以任意选择一个线程,但若信
  号是致命的那么要杀死线程组中所有的线程。
- 信号的处理函数结构体只在进程描述符里，没有在signal_sturct里，因为多线程里信号处理函数是可
  以共享的，线程的进程描述符的信号处理函数指针指向领头线程的就可以了。
- 关于进程描述符的notifier的说明是:Pointer to a function used by a device driver to block
  some signals of the process.
- 进程描述符的real_blocked的解释是Temporary mask of blocked signals (used by the
  rt_sigtimedwait( ) system call)。
- 几个信号相关的结构体之间的关系：进程描述符struct task_struct(1)包含struct
  sigpending(2),struct signal_struct(3),struct sighand_struct(4);struct sigpending(2)不包含
  struct sigqueue(5)但是它的list成员是链到struct sigqueue的;struct signal_struct(3)包含
  struct sigpending(2);struct sighand_struct(4)包含struct k_sigaction(6),struct
  sigqueue(5)包含struct siginfo_t结构体
- 一个信号处理时可以指定是否mask该信号，在k_sigaction的sa_flags域的SA_NODEFER, SA_NOMASK标
  志。但同样是k_sigaction还一个类似的成员sa_mask指定当运行信号处理程序时被mask的信号。
- pending的信号是用struct sigpending链起来的.
- struct sigpending里的signal成员是sigset_t类型的，只是一个挂起信号的掩码，具体的挂起信号的
  信息还要通过struct sigpending里的list成员链起来的sigqueue的si_signo来标识具体是那个信号。
** static inline int signal_pending(struct task_struct *p)
*** include/linux/sched.h:
- 检查了TIF_SIGPENDING标志。
- 这个函数不会检查进程描述符的pending和signal->shared_pending之类的东西的，这些在
  recalc_sigpending_tsk()里进程。
** fastcall void recalc_sigpending_tsk(struct task_struct *t)
*** kernel/signal.c:
- 为什么要检查进程描述符的signal->group_stop_count呢？
- 还检查进程描述符的pending和signal->shared_pending，从检查的方法可以知道
  PENDING(&t->pending, &t->blocked)，被block的信号是可以放在sigpending的队列里的，但是要看
  这个信号是否真的pending中，就要与block掩码相对比。
** static int rm_from_queue(unsigned long mask, struct sigpending *s)
*** kernel/signal.c:
- 从这个函数可以看出siginfo_t->si_signo不是以从1-32这32个数字来标识信号的，而是用位掩码的方
  式来标识的。
- 要删除一个挂起的信口，就要删除signal_struct->signal里的掩码，把相应的sigqueue从链表里删除，
  释放相应的sigqueue结构体。
** static void flush_sigqueue(struct sigpending *queue)
*** kernel/signal.c:
- 与rm_from_queue的相类似。
** void flush_signals(struct task_struct *t)
*** kernel/signal.c:
- 这个函数会清掉进程的TIF_SIGPENDING，再对进程的pending调用flush_sigqueue(),再对进程的
  signal->shared_pending调用flush_sigqueue().
** int send_sig(int sig, struct task_struct *p, int priv)
*** kernel/signal.c:
- 以priv是否等于0作为info的参数调用send_sig_info()
** int send_sig_info(int sig, struct siginfo *info, struct task_struct *p)
*** kernel/signal.c:
- 以相同的参数调用specific_send_sig_info()
- 关于specific_send_sig_info()的info的参数：ULK:0 means that the signal has been sent by a User Mode process, 1
  means that it has been sent by the kernel, and 2 means that is has been sent by the
  kernel and the signal is SIGSTOP or SIGKILL.
** void force_sig(int sig, struct task_struct *p)
*** kernel/signal.c:
- 以1的info调用force_sig_info()
- 这是对进程私有的信号的处理，不是共享的信号.
- 如果信号是被阻塞的或信号是被怱略的(SIG_IGN)，那么就要把信号的处理函数改为SIG_DFL且改成不
  阻塞，接着还要设置TIF_SIGPENDING。最后还是以相同的参数调用specific_send_sig_info().
** void force_sig_specific(int sig, struct task_struct *t)
*** kernel/signal.c:
- 和force_sig()差不多，有点不同的是force_sig()会在信号被阻塞的情况下也把信号的处理程序改成
  SIG_DFL。
- 和force_sig()最主要的区别是调用specific_send_sig_inof()时是以info为2来调用的。
** int send_group_sig_info(int sig, struct siginfo *info, struct task_struct *p)
*** kernel/signal.c:
- 以相同的参数调用group_send_sig_info()
** int kill_pg(pid_t pgrp, int sig, int priv)
*** kernel/signal.c:
- 以priv是否为0的结果作为参数调用kill_pg_info()
** int kill_pg_info(int sig, struct siginfo *info, pid_t pgrp)
*** kernel/signal.c:
- 以相同的参数调用__kill_pg_info()
** int kill_proc(pid_t pid, int sig, int priv)
*** kernel/signal.c:
- 以priv是否为0的结果作为参数调用kill_proc_info()
** int kill_proc_info(int sig, struct siginfo *info, pid_t pid)
*** kernel/signal.c:
- 先用find_task_by_pid()再用找到的进程描述符调用group_send_sig_info()
** static int specific_send_sig_info(int sig, struct siginfo *info, struct task_struct *t)
*** kernel/signal.c:
- 若info是真实分配空间的且si_code是SI_TIMER那么就要把返回值设成
  info->si_sys_private,SI_TIMER表示信号的发送者是Timer expiration
- 若信号是被怱略就退出，但返回值可能在SI_TIMER的情况下被修改了。同样在信号已在挂起的时候也
  退出。
- 用send_signal()把信号加上.
- 若信号没有被block就调用signal_wake_up()
- 在调用signal_wake_up()时,若信号是SIGKILL那么就以resume为1的参数调用signal_wake_up().
- 若成功给进程发送了一个信号,那么就会把这个进程唤醒,让它处于可执行的状态,但是
  signal_wake_up()里调用的kick_process()有一个不足,就是若被唤醒的进程是本地CPU的进程且被唤
  醒之后的优先级比current的高,但这时不会进行进程切换.而不是本地CPU的进程时就不是这样处理,而
  是不管优先级都会给所在的进程发一个切换中断.
** static int sig_ignored(struct task_struct *t, int sig)
*** kernel/signal.c:
- 被跟踪(PT_PTRACED)的情况下无论如何是不能怱略信号的，优先级最高.
- 如果私有的信号block掩码有标上该信号,那么也是不能忽略的.
- 只有在处理函数指定是SIG_IGN或是SIG_DFL且默认的处理是怱略的时候才可以怱略该信号.
** void signal_wake_up(struct task_struct *t, int resume)
*** kernel/signal.c:
- 进入这个函数就一定会设置TIF_SIGPENDING
- 一定会唤醒在TASK_INTERRUPTIBLE状态的进程的
- 若信号是SIGKILL,那么参数resume就是1,这时也会唤醒在TASK_STOPPED和TASK_TRACED状态下的进程的.
- 调用wake_up_state()之前不判断进程的状态,因为会出现竞争,所以没有必要.
** int fastcall wake_up_state(task_t *p, unsigned int state)
*** kernel/sched.c:
- 就直接调用try_to_wake_up(),若try_to_wake_up()返回0,那么表示进程已经是可以运行的了.
** void kick_process(task_t *p)
*** kernel/sched.c:
- 这个函数主要的功能就是若进程t不是本地CPU的进程且又不是正在运行的进程,那么就给它所在的CPU
  发一个调度的中断.不是这样理解的,而是若进程t不是本地CPU的进程且又是正在运行的进程,那么就给
  它所在的CPU发一个调度中断,因为ULK:each process checks the existence of pending signals
  when returning from the schedule( ) function, the interprocessor interrupt ensures that
  the destination process quickly notices the new pending signal.
** static int send_signal(int sig, struct siginfo *info, struct task_struct *t, struct sigpending *signals)
*** kernel/signal.c:
- 若info是2,那么表示信号是SIGKILL或SIGSTOP或是由force_sig_specific()发送的,这时就不用作插入
  sigqueue的工作,但也要把sigpending->signal相应的位掩码给标上.因为:It is important to let
  the destination process receive the signal even if there is no room for the
  corresponding item in the pending signal queue. Suppose, for instance, that a process is
  consuming too much memory. The kernel must ensure that the kill( ) system call succeeds
  even if there is no free memory;
- SI_USER:0:sent by kill(), sigsend(), raise()
- SI_KERNEL:0x80:sent by the kernel from somewhere
- SI_QUEUE:-1:sent by sigqueue()
- 如果调用__sigqueue_alloc()时的最后一个参数为1那么就允许超过限制分配sigqueue.在这个函数里
  调用时是信号是常规信号且info小于2(这里不可能等于2，因为之前已作判断了，信号不是内核发送的
  SIGKILL或SIGSTOP)或info是分配了空间时信号是由kill(),sigsend(),raise()或内核发送的
  (info->si_code>=0就是SI_USER和SI_KERNEL),但是想不通的是为什么由用户态和内核态发送的常规信
  号可以超过限制。什么情况下才不能超过限制呢？其实超过限制也不一定说明分配就会失败。
- 如果成功分配了sigqueue,那么就给它的info初始化，若info的参数是0或1，就要单独初始化info里的
  成员，注意在info为1的情况下si_pid和si_uid是0的.
- 关于返回EAGAIN的情况：ULK：an item will not be added to the signal pending queue,
  because there are already too many pending signals, or there is no free memory for the
  sigqueue data structure, or the signal is immediately enforced by the kernel. If the
  signal is real-time and was sent through a kernel function that is explicitly required
  to queue it, the function returns the error code -EAGAIN。注意在这里info不可能为2.不是在
  SI_USER的情况下发的实时信号就要求重试。
** int group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)
*** kernel/signal.c:
- 主要是调用__group_send_sig_info().
- 为什么要在p->sighand不为空时才会执行相应的动作呢？
** static int check_kill_permission(int sig, struct siginfo *info,
*** kernel/signal.c:
- ULK:The signal is delivered only if at least one of the following conditions holds: The
  owner of the sending process has the proper capability (usually, this simply means the
  signal was issued by the system administrator; see Chapter 20). The signal is SIGCONT
  and the destination process is in the same login session of the sending process. Both
  processes belong to the same user.
- 以下这个是保证发这个信号的肯定是用户态下发的而不是在内核态发的，不管它是不是SIGKILL或
  SIGSTOP.如果真的是SIGKILL或SIGSTOP那调用了这个函数的__group_send_sig_info()该怎么办呢？
 #+BEGIN_EXAMPLE
(!info || ((unsigned long)info != 1 &&
				   (unsigned long)info != 2 && SI_FROMUSER(info)))
 #+END_EXAMPLE
- 以下的保证信号是SIGCONT且是同一个会话才允许发送。
 #+BEGIN_EXAMPLE
  ((sig != SIGCONT) ||
  (current->signal->session != t->signal->session))
 #+END_EXAMPLE
- 当前的进程euid和uid分别与别进程t的suid和uid比较。
 #+BEGIN_EXAMPLE
  (current->euid ^ t->suid) && (current->euid ^ t->uid)
  && (current->uid ^ t->suid) && (current->uid ^ t->uid)
 #+END_EXAMPLE
- http://zhumeng8337797.blog.163.com/blog/static/10076891420112410364978/ set-user-ID
  (suid)/set-group-ID(sgid):设置用户ID,这是相对于文件来说的.设置了set-user-ID位的可执行程
  序,执行时,进程的effective user ID与saved set-uesr-ID都为程序文件所属用户的ID. 时real
  user ID与effective user ID就不一定相等了.这类程序称之为SUID程序,这类程序有特殊的用途.典型
  的例子:passwd程序,ping程序等.如果一个文件被设置了SUID或SGID位，会分别表现在所有者或同组用
  户的权限的可执行位上。
- 除了一般的user id 和group id外，还有两个称之为effective 的id，就是有效id，上面的四个id表
  示为：uid，gid，euid，egid。内核主要是根据euid和egid来确定进程对资源的访问权限。一个进程
  如果没有SUID或SGID位，则euid=uid egid=gid，分别是运行这个程序的用户的uid和gid。例如kevin
  用户的uid和gid分别为204和202，foo用户的uid和gid为 200，201，kevin运行myfile程序形成的进程
  的euid=uid=204，egid=gid=202，内核根据这些值来判断进程对资源访问的限制，其实就是kevin用户
  对资源访问的权限，和foo没关系。如果一个程序设置了SUID，则euid和egid变成被运行的程序的所有
  者的uid和gid，例如kevin用户运行myfile，euid=200，egid=201，uid=204，gid=202，则这个进程具
  有它的属主foo的资源访问权限。SUID的作用就是这样：让本来没有相应权限的用户运行这个程序时，
  可以访问他没有权限访问的资源。passwd就是一个很鲜明的例子。SUID的优先级比SGID高，当一个可
  执行程序设置了SUID，则SGID会自动变成相应的egid。
- 而进程的suid是用来因执行一个设置了suid程序文件而用来保存进程的euid的。
** int __group_send_sig_info(int sig, struct siginfo *info, struct task_struct *p)
*** kernel/signal.c:
- 调用的send_singal()是把信号放在共享挂起信号上的.接着调用__group_complete_signal()来选择一
  个线程处理相应的信号,若是致命信号就调会给所有的线程发SIGKILL信号.
** static void handle_stop_signal(int sig, struct task_struct *p)
*** include/linux/sched.h:
- 是什么样的进程才会有SIGNAL_GROUP_EXIT标志呢？是发起组退出的那个进程吗？
- 用sig_kernel_stop()检查信号是不是停止信号，若是先把线程组里的共享SIGCONT信号删掉，再把其
  它线程的私有SIGCONT给删除.所以可以看出给一个组发停止信号就会把所有的线程都continue.
- SIGNAL_STOP_CONTINUED只是在handle_stop_signal()里赋值，当正在进行组停止的时候收到一个
  SIGCONT信号。SIGNAL_STOP_CONTINUED有什么用呢？
- 正在组退出的时候如果信号是SIGCONT且不是被跟踪的话为什么调用do_notify_parent_cldstop()的时
  候参数是p->group_leader呢？好像也不必要知道为什么，只要记住是这样就可以了：如果一个进程要
  用SIGCHLD通知它的父进程的话，且这个进程没有被跟踪，那么要通知的进程将是该进程所在的进程组
  的领头进程的父进程，而不是该进程的父进程。
- 为什么在信号是SIGCONT且在组退出的时候会把p->signal->group_stop_count清零呢？
- group_stop_count是记录当前线程组中未停止的线程的个数，在通过tkill系统调用的时候发送给某个
  线程停止信号（SIGSTOP，SIGTSTP，SIGTTIN，SIGTTOU），这会使得线程组中的所有线程停止。在进
  程fork线程的时候，如果flag是CLONE_THREAD(复制线程)，多个线程会共用一个signal_struct,如果
  tkill一个信号到某个线程，但是这个tkill会把信号发送到线程task_struct的pending中，这个字段
  是属于线程的私有信号结构。按照道理，当这个线程由于系统调用，异常，中断等返回用户态后，会
  处理线程中的信号，无论是私有的还是共有的信号都会处理。那些信号都是私有信号，那么就tkill对
  应的线程会，只有那一个线程会采取stop操作。但是实际上，内核中对停止信号（SIGSTOP，SIGTSTP，
  SIGTTIN，SIGTTOU），会采取特殊的处理方式，就是stop信号会使得线程组中其他线程将状态转换为
  TASK_STOPED,使得整个线程组停止。就是说一个线程的停止会导致整个线程组的停止。这个特性靠
  signal_struct的group_stop_count变量来实现，对于一个线程组来说每个线程的使用的都是同一个
  signal_struct,这个变量来记录，还有多少个线程还未stop。这样可以想到逻辑就是：如果一个线程
  收到了SIGSTOP信号，group_stop_count的值就是线程组中的线程个数，同时这个线程会通过
  signal_wake_up来告诉线程组中的线程有信号需要处理，那么线程组中的每个线程都会调用信号处理
  函数，每个线程都会首先处理stop信号，判断group_stop_count如果大于0就代表当前线程需要被停止
  了，停止之前将group_stop_count减一，如果线程组中的线程都停止了，group_stop_count=0,那么就
  通知父进程告诉他子线程组停止了.http://blog.chinaunix.net/uid-27767798-id-3496692.html. 所
  以在这里把group_stop_count清零表示还没有停止的进程就不要再停止了。接着在下面调用的
  do_notify_parent_cldstop()是为了用SIGCHLD通知父进程子线程组停止了，父进程可以去做其它的事
  情了。
- stop类的信号是对整个线程组起作用的，一个线程收到了stop信号会迫使线程组中其它的线程也停止，
  如果在这停止线程组过程中收到了SIGCONT信号，那么group stop会立刻停止，使得线程组继续执行，
  已经停止的进程应该是没有办法通过一个SIGCONT信号来唤醒已经停止的其他线程了，只能tkill每一
  个线程，发送SIGCONT信号，来唤醒停止的线程。
  http://blog.chinaunix.net/uid-27767798-id-3496692.html
- group_stop_count等于1说明其他的线程都已经结束了，还差当前进程未结束。
  http://blog.chinaunix.net/uid-27767798-id-3496692.html
- 如果在group stop的过程中，或者get_signal_to_deliver函数已经从信号挂起队列中已经dequeue出
  了一个stop信号，线程组中的一个线程收到了SIGCONT信号，意味着线程组不用在继续stop了，这时需
  要将gourp_stop_count变为0。不让线程组再继续stop了，这里前面讲到的signal_struct标志位的
  SIGNAL_STOP_DEQUEUED，就起到了作用，在dequeue出stop信号的时候，会加上这个标志位。在执行
  do_signal_stop的之前，也就是第一个stop的进程处理的时候，也会判断这个SIGNAL_STOP_DEQUEUED
  标志位，这时如果收到了SIGCONT信号，清除标志位SIGNAL_STOP_DEQUEUED是有必要了，起到了避免了
  收到了SIGCONT，而继续停止第一个线程的作用.  http://blog.chinaunix.net/uid-27767798-id-3496692.html
- tsk->signal->flags的标志就4个而已:SIGNAL_STOP_STOPPED:job control stop in
  effect.SIGNAL_STOP_DEQUEUED:stop signal dequeued.SIGNAL_STOP_CONTINUED:SIGCONT since
  WCONTINUED.SIGNAL_GROUP_EXIT:group exit in progress
- SIGNAL_STOP_STOPPED说明所有的线程都已经停止了
  http://blog.chinaunix.net/uid-27767798-id-3496692.html
- SIGNAL_STOP_DEQUEUED，这个标志位的意思是，从信号队列中已经dequeue了一个stop的信号.
- SIGNAL_STOP_CONTINUED:当线程组收到SIGCONT信号时,无论是中组停止的过程中还是最近完成了一次
  组停止(会置上SIGNAL_STOP_STOPPED),那会设置SIGNAL_STOP_CONTINUED.这上可以从这个函数里看出
  来.这个标志要经过这样一个过程.
- CLD_CONTINUED:这个基本只在handle_stop_signal()调用do_notify_parent_cldstop()时使用,所以可
  以从这个使用上找出它有什么用,使用它的情况是当线程组接到一个SIGCONT时且之前线程组已完成了
  所有线程停止(SIGNAL_STOP_STOPPED)而不是在组停止的过程中时就用do_notify_parent_cldstop()就
  会使用.那么是不是想用这个标志说明从一个所有线程停止的状态里收到了一个SIGCONT信号.
- CLD_STOPPED:是在收到SIGCONT信号时且组停止还在进行时调用do_notify_parent_cldstop()时使用,
  所以它的意思是想告诉父进程线程组已完成停止且是组停止被中断的情况下完成的.
- 在收到一个SIGCONT时,就算正在组停止中,也要把线程组中的共享stop信号给删掉,再逐个逐个把线程
  组中的所有的线程的stop类信号给删掉.不管线程私有信号蔽避是否有蔽避SIGCONT,如果组收到
  SIGCONT信号时就一定唤醒线程组里的所有在TASK_STOPPED线程,若SIGCONT自定义了处理函数且没有被
  蔽避,那么也会唤醒TASK_INTERRUPTIBLE的进程
** static void do_notify_parent_cldstop(struct task_struct *tsk, struct task_struct *parent, int why)
*** kernel/signal.c:
- 这个函数是在收到一个SIGCONT时调用的。这个函数的一个主要目的就是给父进程发一个SIGCHLD信号，
  而在发之前要根据不同的情况给info初始化,同时还唤醒父进程。是不是想给父进程发SIGCHLD信号时
  都会调用这个函数呢？
- 说明一下这个不好理解的struct siginfo结构体，si_code成员不只ULK上介绍的，除了
  SI_USER（kill(),raise()）,SI_KERNEL(Generic kernel
  function),SI_QUEUE(sigqueue()),SI_TIMER(Timer expiration),SI_ASYNCIO (Asynchronous I/O
  completion),SI_TKILL(tkill( ) and tgkill( ))还有可以是
  SIGCHLD,SIGILL,SIGFPE,SIGSEGV,SIGBUS,SIGPOLL,SIGTRAP信号产生是的相应的si_code代码,如
  SIGILL就有ILL_ILLOPC, ILL_ILLOPN, ILL_ILLADR等的si_code值，SIGCHLD的就有CLD_TRAPPED
  CLD_STOPPED CLD_CONTINUED等。到这里想起之前有一个判断si_code是否大于0的情况，现在发现只有
  SI_KERNLE和信号为SIGILL,SIGFPE,SIGSEGV,SIGBUS,SIGTRAP,SIGCHLD,SIGPOLL的时候都是大于0的.其
  实发现struct siginfo结构体里那个联合体也针对不同的类型的si_code作的。所以若
  info->si_signo是那些特殊的信号的时候就要给info->si_code赋相应的特定的值且还要给那个联合体
  的相对应的成员做赋值。
- 在info->si_signo为SIGCHLD的时候可以看这个函数来具体认清相应的联合体里每个成员的作用。
  si_utime是赋予了子进程的utime,si_stime也是类似。si_status的注释说是exit
  code,CLD_CONTINUED时是SIGCONT,CLD_STOPPED时是signal->group_exit_code,CLD_TRAPPED时是
  tsk->exit_code.
- 在handle_stop_signal()里若信号是SIGCONT时会以CLD_STOPPED来调用do_notify_parent_cldstop()
  的,这是不是可以推出若进程是在停止状态收到SIGCONT时就会用通知父进程上子进程的上一个状态是
  停止的。如果上一个状态不是在被跟踪的情况下(CLD_STOPPED)停止的话那么就会把si_status赋予
  tsk->signal->group_exit_code,这也可以看CLD_STOPPED就是为在组停止时收到SIGCONT信号时服务的。
- 如果SIGCONT信号是被父进程怱略的或SIGCONT信号设置了SA_NOCLDSTOP(Applies only to SIGCHLD;
  do not send SIGCHLD to the parent when the process is stopped),那么就调
  用__group_send_sig_info()给父进程所在的整个线程组发一个SIGCHLD信号，用之前的初始化的
  info(si_signo是SIGCONT),给整个线程组发信号是不是想用共享的处理函数的意思呢？
- 无论有没有把信号发成功，都会唤醒父进程。
- CLD_TRAPPED是在
- group_exit_code ULK的注释：Process termination code for the thread group.为什么是
  CLD_STOPPED的时候就要把group_exit_code赋给info->si_status呢？停止时收到SIGCONT信号与组的
  退出代码有什么关系吗？
** static void __group_complete_signal(int sig, struct task_struct *p)
*** kernel/signal.c:
- EXIT_ZOMBIE, EXIT_DEAD, TASK_TRACED, TASK_STOPPED状态的进程是不可以接收信号的,但是在
  TASK_STOPPED和TASK_TRACED状态时若信号是SIGKILL把可以接收.
- wants_signal()就是看p是不是可以作为处理信号的的进程.与进程的信号阻塞有关，与进程的状态有
  关，与进程是否在退出有关(PF_EXITING和EXIT_DEAD、EXIT_ZOMBIE)进程是否在执行有关，和进程是
  否有挂起信号有关。
- 若线程组里只有一个线程，那就不用选了，就用参数进程p.若不是就要扫描整个线程组。若还是没有
  一个进程想处理这个信号的，那么就直接退出这个函数，这样做有什么后果呢？
- tsk->real_blocked这个成员好像只有在这个函数和sys_rt_sigtimedwait()里使用。
- 这个函数不像ULK上说的那么简单，仅仅是收到一个致命信号还不足以杀掉整个线程组，还有很多条件
  要满足：若是正在组退出那不用管了；不能是real_blocked的成员（这个real_blocked有什么用
  呢？）；若不是SIGKILL这个致命信号时那么进程就不能被跟踪。
 #+BEGIN_EXAMPLE
  (sig_fatal(p, sig) && !(p->signal->flags & SIGNAL_GROUP_EXIT) &&
	    !sigismember(&t->real_blocked, sig) &&
	    (sig == SIGKILL || !(t->ptrace & PT_PTRACED)))
 #+END_EXAMPLE
- 因为dump的处理方式的信号也会杀死进程，所以在杀死整个组的情况下会考虑信号是不是dump,若是就
  要转存。
- sig_fatal()检查信号是否为致命信号不仅仅是检查信号的默认行为是不是dump或terminal，还要检查
  它的处理方式是不是默认的。
- 若不能满足作致命信口的处理，那么就用选中的进程调用signal_wake_up()，调用的另一个参数就用
  信号与SIGKILL比较。
- 如果不是dump信号，那么就要设置tsk->signal->flags的SIGNAL_GROUP_EXIT。所以设置这个标志之后
  说明要进行线程组退出了。同时这里还把tsk->signal->group_exit_code退出码置成了信号，看来作
  用不止一个啊。还把tsk->signal->group_stop_count组停止计数给清零了，为什么要这样做呢？是不
  是让还在进行组停止的进程组不要再进行组停止了。为什么要把所有的线程都停止了呢？有这样的注
  释：We make all threads other than the chosen one go into a group stop so that nothing
  happens until it gets scheduled, takes the signal off the shared queue, and does the
  core dump.
- SIGNAL_GROUP_EXIT在do_group_eixt(),在__group_complete_signal(),在zap_other_threads()被置
  位。
- 处理这个core dump信号的进程还是被选中的进程。
- group_exit_task是有两种含义的，注释也说了:overloaded:notify group_exit_task when ->count
  is equal to notify_count; everyone except group_exit_task is stopped during signal
  delivery of fatal signals, group_exit_task processes the signal.一个是用于组退出结束后通
  知group_exit_task进程http://blog.chinaunix.net/uid-27767798-id-3492389.html， 另一个是在递
  送一个致命信号时希望group_exit_task是停止的，后者的这种情况是在这个函数出现的。
** int get_signal_to_deliver(siginfo_t *info, struct k_sigaction *return_ka, struct pt_regs *regs, void *cookie)
*** kernel/signal.c:
- 这个函数会用处理所有的信号.
- ULK:the do_signal( ) function is usually only invoked when the CPU is going to return in
  User Mode.不是在进程在切换时做的。
- 在满足正在进行组停止时才会调用handle_group_stop(),若没有进行组退出，那么是不用调用
  handle_group_stop()的，为什么有这个特殊的条件呢？这也是为什么handle_group_stop()叫
  handle_group_stop了
- 每处理完一个信号就会判断一下是否有组停止,是否应该调用handle_group_stop().组停止切回来后还
  会马上处理其它所有的挂起信号.
- 若有信号要处理且信号不是SIGKILL(为什么要排除SIGKILL呢?)且被跟踪,那么就要作跟踪的处理，要
  调用ptrace_stop(),这个函数下面有讲。
- 调用ptrace_stop()之后，跟踪进程可能会取消进程的信号，取消的方法是把current->exit_code清零。
  若不取消那么就继续处理，否则就处理下一个信号。
- 若跟踪进程修改了信号，就要修改参数info,若进程没有阻塞信号就给它发信号，所以注意是先通知跟
  踪进程让它处理再根据跟踪进程的反馈让被跟踪进程作相应的处理。但被阻塞的情况下也还会有很多的处理。
- 若是发现信号的处理方法是SIG_IGN那么就处理下一个信号，若发现信号是SIG_DFL那么就不用与处理
  下一个信号了，直接退出函数。
- 因为被跟踪进程可能会修改跟踪进程的信号，所以return_ka和原来的不一样了。若发现不是默认的，
  且设了SA_ONESHOT那么就把它改回默认的。
- 之前都已经检查了信号的处理是否是默认的了，但为什么后面又用sig_kernel_ignore来检测呢？而且
  为真时还处理下一个信号。嗅，知道了，原来之前检查的是SIG_IGN和不为SIG_DFL的情况，而用
  sig_kernel_ignore()检查的是为SIG_DFL且默认的处理方式是怱略。
- 在这里判断进程是否是进程1。
- 一个进程组不是孤儿进程组的条件是，该组中有一个进程，其父进程在属于同一个会话（session）的
  另一个组中.所以若信号的默认处理方式是停止但信号不是SIGSTOP且进程所在的进程组是一个孤儿进
  程组，那么就不处理这个信号，处理下一个信号。为什么SIGSTOP这个信号要作特殊处理呢？难道
  是SIGSTOP时就一定是执行do_signal_stop()不管进程是否在孤儿进程。
- do_signal_stop()(这个函数后面有讲)是在什么情况下返回0表示没有停止的呢？
  1.SIGNAL_STOP_DEQUEUED没有设置时，2.在想开始一个新的组停止而进行解加锁时被其它的纯程收了
  另一个停止信号或SIGCONT时。若返回0就开始处理下一个信号，但无论返回什么，都不能往下执行了，
  往下执行的情况只有在处理方式是默认的且处理方式是dump或terminate.
- 若处理方式是默认的且处理方式是dump或terminate，那么设置PF_SIGNALED，原来PF_SIGNALED是这样
  用的。若是一个dump信号就调用do_coredump(),若不是就调用do_group_exit()
** static inline int handle_group_stop(void)
*** kernel/signal.c:
- 对于get_signal_to_deliver()来说，这个函数返回1表示不用处理任何的信号了。
- 这个函数的只有被get_signal_to_deliver()调用，调用这个函数时已经表明是在进行了组停止。
- 当发现tsk->signal->group_exit_task为current时，有这样的注释：Group stop is so we can do
  a core dump, We are the initiating thread, so get on with it.表明组停止已经结束了，就差
  current这个进程了。但为什么要正在进行组退出的时候才这样做呢？有什么关系吗？
- 若group_exit_task为current，无论是否是在进行组退出，都要处理信号，还要把group_exit_task清零。
- 若group_exit_task不为current，但current->signal->flags置了SIGNAL_GROUP_EXIT，表示current
  所在的进程组在进行组退出，且处录地组退出的那个进程不是current,这种情况下也要返回0表示要处
  理信号。所以可以得出只要线程组在进行组退出，那么就要处理信号，不管是否是在进行组停止.
- 若没有进行组退出且在进行组停止，那么就不能处理信号。
- 从代码可以看出，组停止是在这里处理的，这也是为什么这个函数叫handle_group_stop,从代码来看，
  正在进行组停止时处理停止信号很简单，就是把当前进程的退出代码(current->exit_code)设置为组
  的退出代码(current->signal->group_exit_code)，再设置进程为TASK_STOPPED。为什么要设置退出
  代码呢？停止跟退出代码有什么关系呢？
- 结合get_signal_to_deliver()来看，因为是在一开始在get_signal_to_deliver()调用
  handle_group_stop()的，所以若想进行一个组停止时，一旦把signal->group_stop_count设为不等于
  0，那么其它的线程就不会处理其它的信号了，因为是在get_signal_to_deliver()一开始调用
  handle_group_deliver()的。
- 因为处理了一个线程的停止信号，所以current->signal->group_stop_count要自减，若为0了就表明
  所有的线程都已进行了一次停止，所以要设置SIGNAL_STOP_STOPPED.
- 如果一个进程已经处理了自已停止，那么就调用finish_stop()
** static void finish_stop(int stop_count)
*** kernel/signal.c:
- 在这里参数也有可能会为负？这个函数是handle_group_stop()和do_signal_stop()调用的,而
  handle_group_stop()在被get_signal_to_deliver()调用之前已经验证了group_stop_count大于0，所
  以在handle_group_stop()里调用finish_stop()时一定可以保证stop_count参数不为负数，所以出现
  负数的情况应该是在do_signal_stop()里调用。
- 这个函调用do_notify_parent_cldstop()时都是用CLD_STOPPED的，且都是在stop_count(其实就是
  group_stop_count)不为正，所以可能看出CLD_STOPPED是在所有的线程都完成了停止之后才可以使用
  的吗？不是的，可以从handle_stop_signal()函数看出，在信号是SIGCONT且正在进行组停止
  （group_stop_count>0）的时候也会把CLD_STOPPPED传给do_notify_parent_cldstop()的，所以
  CLD_STOPPED这个信号的作用应该是组停止结束后就以CLD_STOPPED调用do_notify_parent_cldstop()
  通知父进程，不管是不是在停止过程中有被SIGCONT中断，若是中断了也算是组停止结束。
- 为什么要在group_stop_count为负数的时候给current->parent发通知呢?在被踪跟的情况下发还可以理解.
- group_stop_count为0表示组停止结束,所以给current->group_leader->real_parent发信号.不是
  current->group_leader->parent.
- 调用这个函数就一定会调度,因为调用了schedule().这个函数被handle_group_stop()调
  用,handle_group_stop()是被get_signal_to_deliver()在进行组停止时调用的.所以进行组停止的时
  候若其中一个线程在执行的时候,处理信号时就会调用get_signal_to_deliver() ->
  handle_group_stop()(没有进行组退出的情况) -> finish_stop() -> schedule().
- 等从schedule()切换回来的时候把current->exit_code给清掉了.exit_code有在
  handle_group_stop()被设置成current->signal->group_exit_code的,所以这个exit_code应该是给
  schedule()使用的.
** static void ptrace_stop(int exit_code, int nostop_code, siginfo_t *info)
*** kernel/signal.c:
- 这个函数主要功能就是想以CLD_TRAPPED调用do_notify_parent_cldstop()，通知跟踪进程被跟踪的进
  程已收到一个信号。
- 通知完之后马上调用schedule()进行切换。
- 若正在进行组停止，那么就要停止数减1，从被get_signal_to_deliver()的角度看，就是若处理一个
  非SIGKILL信号且被跟踪时且正在进行组退出，那么就要把group_stop_count自减1。虽然减1了，但是
  不一定会调用do_notify_parent_cldstop(),就算group_stop_count是0也不会以CLD_STOPPED调用
  do_notify_parent_cldstop().
- 要想以CLD_TRAPPED调用do_notify_parent_cldstop()还要先满足一大堆的乱七八糟的条件。不满足就
  不调用do_notify_parent_cldstop(),不进行进程的切换，不修改进程的执行状态。
- 把current->last_siginfo设为info和把current->exit_code设为exit_code可能是给
  do_notify_parent_cldstop()用的。若不能调用do_notify_parent_cldstop()那么
  current->exit_code要改成nostop_code.在get_signal_to_deliver()调用这个函数的时候exit_code
  参数和nostop_code参数都是信号编号。
- tsk->last_siginfo是给跟踪用的。
** static int do_signal_stop(int signr)
*** kernel/signal.c:
- 关于这里为什么要检查SIGNAL_STOP_DEQUEUED信号，上面有讲。
- 若进程所在的线程组正在进行一个组退出，那么就不用再启动下一个组退出了。
- 在进行组退出的时候，是不会发一个停止信号给所有线程的而是直接停掉它，所以在个函数里发现有
  停止信号不是因为有组停止所引起的。
- 在这里处理组停止的情况与handle_group_stop()函数里后面的类似，就是group_stop_count自减，为
  0就设置SIGNAL_STOP_STOPPED，把current->signal->group_exit_code赋给current->exit_code，设
  置set_current_state(TASK_STOPPED)。handle_group_stop()是只在get_signal_to_deliver()一开始
  处理信号时被调用.而handle_group_stop()在一开始是检查了有没有进行组退出。
- 若进程没有正在进行一个组退出，且线程组只有一个线程，那么也把current->exit_code和
  current->signal->group_exit_code赋予信号，为什么要这样做呢？不是不如果进程因一个信号的处
  理方式是停止的话都会这样设置exit_code和group_exit_code.也把进程状态设置为TASK_STOPPED，也
  设置SIGNAL_STOP_STOPPED.
- 若没有在进行组退出，且线程组只有这个线程，那么j说要开始一个新的组停止了。因为
  tasklist_lock程sighand->siglock要按顺序获得，所以要先放了siglock锁，所以在这个时间窗口里
  有可能产生一个SIGCONT信号和另一个停止信号，因为在收到一个SIGCONT的时候会设置
  SIGNAL_STOP_DEQUEUED，这个时候就要直接退出了。在这个函数里还要判断group_stop_count是否为
  0就是因为刚才解锁后重新上锁可能导致另一个线程开始了一个组停止，若是这样的话说把
  group_exit_code提出来并以current线程的名义给group_stop_count自减，否则就开始一个新的组停
  止，开始一个组停止时要把group_exit_code赋以signr,所以从这里可以看出进行一个组停止时
  group_exit_code是一个因它而停止的信号。接着把exit_code赋以了signr,所以一个进程若因一个组
  停止而停止，那么它的current->exit_code的值就是存放因它而停止的信号，从这个函数可以看出
  current->exit_code不一定与group_exit_code相同的吗？这里还要检查该线程是不是最后一个停止的
  线程是因为可能在之前的解锁再加锁的过程中产生了另一停止信号而使得开始了另一个组进停止并使
  得该线程是最后一个停止的线程。
** int fastcall do_signal(struct pt_regs *regs, sigset_t *oldset)
*** arch/i386/kernel/signal.c:
- 这个函数会调用get_signal_to_deliver(),返回非零表示有信号被deliver了。在
  get_signal_to_deliver()函数里可能会改变info和ka。若有信号被deliver,那么就调用
  handle_signal()
- do_signal()调用一次只处理一个信号，ULK：Notice how do_signal( ) returns after having
  handled a single signal. Other pending signals won't be considered until the next
  invocation of do_signal( ). This approach ensures that real-time signals will be dealt
  with in the proper order.
- 一开始要判断是否是一个系统调用，若是就要设置相应的重启方式。
- 关于如何在内核态和用户态之间切换ULK有说：A nonblocked signal is sent to a process. When
  an interrupt or exception occurs, the process switches into Kernel Mode. Right before
  returning to User Mode, the kernel executes the do_signal( ) function, which in turn
  handles the signal (by invoking handle_signal( )) and sets up the User Mode stack (by
  invoking setup_frame( ) or setup_rt_frame( )). When the process switches again to User
  Mode, it starts executing the signal handler, because the handler's starting address was
  forced into the program counter. When that function terminates, the return code placed
  on the User Mode stack by the setup_frame( ) or setup_rt_frame( ) function is
  executed. This code invokes the sigreturn( ) or the rt_sigreturn( ) system call; the
  corresponding service routines copy the hardware context of the normal program to the
  Kernel Mode stack and restore the User Mode stack back to its original state (by
  invoking restore_sigcontext( )). When the system call terminates, the normal program can
  thus resume its execution.
** asmlinkage long sys_kill(int pid, int sig)
*** kernel/signal.c:
- si_signo就是sig,si_code是SI_USER，si_pid是当前进程的tgid,si_uid是uid.si_pid是比较特殊的。
- 调用kill_something_info().
** static int kill_something_info(int sig, struct siginfo *info, int pid)
*** kernel/signal.c:
- ULK:pid = 0 ,The sig signal is sent to the thread group of the process whose PID is
  equal to pid .所以是以kill_pg_info()的方式调用的。
- ULK:pid = -1,The signal is sent to all processes, except swapper (PID 0), init (PID 1),
  and current.但是看代码时不是这样的，有一个这样的判断:p->tgid != current->tgid,所以
  current所在的线程组里的所有线程都不能接收信号。这样做是不是因为若给current的线程组发信号，
  最终可能会在该组中选择current来处理信号。
- ULK:pid < -1,he signal is sent to all thread groups of the processes in the process
  group -pid.
** asmlinkage long sys_tkill(int pid, int sig)
*** kernel/signal.c:
- 这个函数的si_pid也是赋以current->tgid,为什么是tgid呢？
- si_code是SI_TKILL
** asmlinkage long sys_tgkill(int tgid, int pid, int sig)
*** kernel/signal.c:
- ULK:The sys_tgkill( ) service routine performs exactly the same operations as sys_tkill( ),
  but also checks that the process being signaled actually belongs to the thread group
  tgid. This additional check solves a race condition that occurs when a signal is sent to
  a process that is being killed.与sys_tkill()的不同就是多了一个p->tgid == tgid的判断，
** static inline unsigned int task_timeslice(task_t *p)
*** kernel/sched.c:
- 这个函数是通过静态优先级来计算进程的时间片的
** static inline runqueue_t *task_rq_lock(task_t *p, unsigned long *flags)
*** kernel/sched.c:
- 这个函数三个目的：禁止本地变量，获取rq->lock，获取运行队列。但这三个操作要一致性。
** static inline runqueue_t *this_rq_lock(void)
*** kernel/sched.c:
- 这个函数不像task_rq_lock()要用循环使得一致性，因为这个
** static void dequeue_task(struct task_struct *p, prio_array_t *array)
*** kernel/sched.c:
- 把进程从array这个链表里删除，array可以是active或expires.
- 相关的东西有nr_active, run_list, array->queue, p->prio, array->bitmap.
** static void enqueue_task(struct task_struct *p, prio_array_t *array)
*** kernel/sched.c:
- 把一个进程插入array链表，不判断位图是否设置了，因为这会耗掉时间。
- 还会把进程的array设成array.
** static void requeue_task(struct task_struct *p, prio_array_t *array)
*** include/linux/list.h:
- 把一个进程从原来链表里删除再插到其它的链表。
** static inline void enqueue_task_head(struct task_struct *p, prio_array_t *array)
*** include/linux/list.h:
- 这个函数与enqueue_task_head()的差别是一个插入头一个插入尾。
** static int effective_prio(task_t *p)
*** kernel/sched.c:
- 根据静态优先级和bonuse来计算优先级。
** static inline void __activate_task(task_t *p, runqueue_t *rq)
*** kernel/sched.c:
- 调用enqueue_task()把进程插入active,并自增nr_running,nr_running与nr_active是不一样的。
** static inline void __activate_idle_task(task_t *p, runqueue_t *rq)
*** kernel/sched.c:
- 这个函数与__activate_task()的区别是调用enqueue_task_head()把进程插入到active
** static void activate_task(task_t *p, runqueue_t *rq, int local)
*** kernel/sched.c:
- 这个函数主要作用是更新进程的动态优先级(recalc_task_prio()),更新task->activated状态，更新
  task->timestamp,调用__activate_task()把进程插入运行队列的激活链表里.
- 这个函数被try_to_wake_up()和__migrate_task()调用.
- 要用sched_clock()来获取时间
- 若是多核的话且进程不在本地CPU的话就要结合timestamp_last_tick来调整时间。
** static void deactivate_task(struct task_struct *p, runqueue_t *rq)
*** kernel/sched.c:
- 就只是调用dequeue_task()把进程从运行链表里删除,不管是不是active链表还是expires链表.
- 这个函数没有activate_task()那么复杂.
- 因为task->timestamp是与插入队列有关的，但是与删除没有关系。所以不用管。
** static void resched_task(task_t *p)
*** kernel/sched.c:
- 还挻多函数调用它的。
- 这个函数有单多核版，单核版的只是调用set_tsk_need_resched(), 而多核版的话除了设置
  TIF_NEED_RESCHED之外还要给所在的CPU发一个调度中断。
** static int migrate_task(task_t *p, int dest_cpu, migration_req_t *req)
*** kernel/sched.c:
- 这个函数里有一段检查代码很奇怪，p->array为空了难道task_running()还有可能是真吗？如果条件
  为真的话就直接调用set_task_cpu()设置进程的cpu,task->thread_info->cpu.若条件为假，那么就用
  completion原语来实现进程的迁移。
 #+BEGIN_EXAMPLE
	if (!p->array && !task_running(rq, p)) {
 #+END_EXAMPLE
** void wait_task_inactive(task_t * p)
*** kernel/sched.c:
- 这个函数只被在跟踪时调用两次。
- 这个函数就是等待进程从运行队列里删掉，这个函数有做优化，就是若被等待的进程没有正在运行那
  么就调用yeild()让进程等待时间长一点。
** static inline unsigned long source_load(int cpu)
*** kernel/sched.c:
- 计算负载的方式就是rq->nr_running乘以一个常量。
** int fastcall wake_up_process(task_t * p)
*** kernel/sched.c:
- 就是调用try_to_wake_up(),唤醒所有在停止状态的进程，包括
  TASK_STOPPED,TASK_TRACED,TASK_INTERRUPTIBLE,TASK_UNINTERRUPTIBLE的进程,但不是同步唤醒。
** int fastcall wake_up_state(task_t *p, unsigned int state)
*** kernel/sched.c:
- 以state调用try_to_wake_up()
** void fastcall sched_fork(task_t *p)
*** kernel/sched.c:
- 只给copy_process()调用。
- 注释说了，这个函数是为一个新fork的进程做调度相关的设置。
- 注释有说，这个函数里把进程标为正在执行的，但是并没有实际插入到运行队列，这可以保证没有谁
  可以让它运行，就算是一个信号或其它的外部事件不能唤醒和把它插入到一个运行队列。
- 因为fork一个进程时，那个新的进程其实是有一个自旋锁的，所以要给thread_info->preempt_count
  补尝1。
- 上面有关于时间片父进程如何共享的说明。
- 因为如果current->time_slice的时间为1，那么分享之后的时间就是0了，这时要把它改回1(为什么要
  改回1呢？)并调用scheduler_tick()
** unsigned long nr_running(void)
*** kernel/sched.c:
- 计算所有的CPU的进程数的总和
** unsigned long nr_uninterruptible(void)
*** kernel/sched.c:
- 计算所有的CPU的不可中断的进程的总和
** unsigned long long nr_context_switches(void)
*** kernel/sched.c:
- 计算所有CPU的切换次数。
** unsigned long nr_iowait(void)
*** kernel/sched.c:
- 计算所有CPU的io等待次数。
** static int find_idlest_cpu(struct task_struct *p, int this_cpu, struct sched_domain *sd)
*** kernel/sched.c:
- 先找出p->cpus_allowed与sd->span的cpu的交集CPU中最低负载CPU，但这个CPU还不是找到的那个，还
  要和当前CPU用一个公式比较
** static void sched_migrate_task(task_t *p, int dest_cpu)
*** kernel/sched.c:
- 若目的CPU不是允许的CPU(cpus_allowed)就不能移。
- 移进程到一个CPU是用线程和completion实现的，migrate_task()里有init_completion(),这个函数有
  wait_for_completion()，migration 线程里应该有complete().migration线程就只有这种时候被用到
  吗？
** void sched_exec(void)
*** kernel/sched.c:
- 注释说了，执行sched_exec()的时候是平衡的时机，因为进程有最小的有效内存和cache覆盖面。
- 先找标有SD_BALANCE_EXEC的域,再从中用find_idlest_cpu()找出最不忙的cpu,找到了就调用
  sched_migrate_task().
** static int load_balance_newidle(int this_cpu, runqueue_t *this_rq, struct sched_domain *sd)
*** kernel/sched.c:
- 这个函数由idle_balance()调用idle_balance()又由schedule()调用.注释说了,当this_rq将成为
  idle时(NEWLY_IDLE)是被schedule()调用.
- 这个函数可以说是load_balance()的简化版,先用find_busiest_group()找出最忙的组,再用
  find_busiest_queue找出最忙的运行队列,最后调用move_tasks()移进程.
** static inline void idle_balance(int this_cpu, runqueue_t *this_rq)
*** kernel/sched.c:
- 注释说了,这个函数在this_rq快成为idle时由schedule()调用,所以要检查SD_BALANCE_NEWIDLE.
** static void active_load_balance(runqueue_t *busiest_rq, int busiest_cpu)
*** kernel/sched.c:
- 把busiest_rq里的进程移到一个idle CPU里。
- 注释说了,这个函数被migration线程调用,是在rq->active_balance设置时被调用.
- 因为SD_LOAD_BALANCE的标志有继承性,所以若有发现这个标志没有设置就不用再向更高的层去搜索了.
- 这个函数主要是三层循环，最外层是循环调度域的，次外层是循环一个调度里的所有组的，最内层是
  循环一个组内的所有的CPU的。
- 因为组间的CPU的交集可能不为空，所以要记录下以访问过的CPU（visited_cpus）
- 最主要的就是找到CPU之后调用move_tasks().
** static void rebalance_tick(int this_cpu, runqueue_t *this_rq, enum idle_type idle)
*** kernel/sched.c:
- 这个函数是遍历所有的域，不是单独某个域。
- 发现原来rq->cpu_load只在这里更新的，且更新的方法是用现在的负载和旧的负载求平均。
- 没有设置SD_LOAD_BALANCE就不用再做处理了。
- 这个函数最主要的就是调用load_balance()，调用load_balance()前要确定调用的时间间隔有没有到，
  就是sched_domain->balance_interval,但sched_domain->balance_interval不能直接使用，要结合
  SCHED_IDLE、sched_domain->busy_factor、sched_domain->last_balance. 若有调用
  load_balance()那么就要更新sched_domain->last_balance.
** asmlinkage void __sched preempt_schedule(void)
*** kernel/sched.c:
- 这个函数里会判断current是否禁止抢占或有禁止中断，若是返回不能调用schedule()
- 在调用schedule()之前会用PREEMPT_ACTIVE来禁止抢占。
** void __devinit init_idle(task_t *idle, int cpu)
*** kernel/sched.c:
- 初始化idle进程，可以看出一个idle只允许在一个CPU上执行，把运行队列的idle赋以了idle进程。
** int set_cpus_allowed(task_t *p, cpumask_t new_mask)
*** kernel/sched.c:
- 这个函数的主要功能就是设置进程的cpus_allowed
- 设置前要注意新的CPU mask与cpu_online_map有没有交集
- 若发现进程当前所在的CPU不包含在新设置的mask里的时候就要调用migrate_task()把进程移过去。
** static void __migrate_task(struct task_struct *p, int src_cpu, int dest_cpu)
*** kernel/sched.c:
- 把进程从src_cpu中移到dest_cpu里去
- 移之前要判断进程是不是被移走了，是否是不允许移到那个CPU
- 如果之前进程已经是插入到一个运行链表里，那么就要重新计算task->timestamp了，还要把进程从原
  来的运行链表删除再插入到新的运行链表.
- 移了之后若发现进程可以抢占current，那么就调用resched_task()
** static int migration_thread(void * data)
*** kernel/sched.c:
- 这个函数是一个线程函数。处理本地进程所要求的迁移到其它CPU的任务.
- 这个函数处理migration_req_t迁移进程的任务链表,若都处理完了就调度。
- 若发现CPU offline了，那么就不断地调用schedule()
- 若运行队列要求迁移进程(rq->active_balance)，那么就调用active_load_balance(),把rq运行队列
  的进程移到域的其它idle cpu里.除了会调用active_load_balance()之外，还会处理迁移进程任务链表里的任务。
- 迁移任务有两种一个是REQ_MOVE_TASK，这个会调用__migrate_task(),还有一个类型是
  REQ_SET_DOMAIN，这个只是设置运行队列里的调度域。
** static int migration_call(struct notifier_block *nfb, unsigned long action, void *hcpu)
*** kernel/sched.c:
- 迁移进程是以SCHED_FIFO的调度策略执行的,且是以实时进程的优先级运行。
** void __devinit cpu_attach_domain(struct sched_domain *sd, int cpu)
*** kernel/sched.c:
- 为什么是本地CPU的时候就不用任务链表来改调度域呢？
** #define pte_none(x)		(!(x).pte_low)
*** include/asm-i386/pgtable-2level.h:
** static inline int pte_none(pte_t pte)
*** include/asm-i386/pgtable-3level.h:
- 这个要比较高低位。
- 只有在打开PAE时才有高低之分。所以在x86里使用3级的页表的时候就是打开了PAE
- 对于32位页表项（没有打开PAE）是有12位的标志位，剩下的20位是表示一个页框的物地址，要记住这
  个页表项的20位地址是如何和线性地址结合来找到一个页框的。
- 要知道page table(pte)和page offset是不一样的
- 所谓的扩展分页就是offset从12位(4K)变成了22位(4M)
** #define pmd_clear(xp)	do { set_pmd(xp, __pmd(0)); } while (0)
*** include/asm-i386/pgtable.h:
** #define set_pmd(pmdptr, pmdval) (*(pmdptr) = (pmdval))
*** include/asm-i386/pgtable-2level.h:
** #define pte_same(a, b)		((a).pte_low == (b).pte_low)
*** include/asm-i386/pgtable-2level.h:
** #define	pmd_bad(x)	((pmd_val(x) & (~PAGE_MASK & ~_PAGE_USER)) != _KERNPG_TABLE)
*** include/asm-i386/pgtable.h:
- 
** #define PAGE_MASK	(~(PAGE_SIZE-1))
*** include/asm-i386/page.h:
- 所以有效位是0的，无效位是1的。
** #define _KERNPG_TABLE	(_PAGE_PRESENT | _PAGE_RW | _PAGE_ACCESSED | _PAGE_DIRTY)
*** include/asm-i386/pgtable.h:
- 当前在内存，可读写，已被访问，已被修改的页就是_KERNPG_TABLE了
** #define	pmd_bad(x)	((pmd_val(x) & (~PAGE_MASK & ~_PAGE_USER)) != _KERNPG_TABLE)
*** include/asm-i386/pgtable.h:
- (~PAGE_MASK & ~_PAGE_USER)根据离散里的De Morgan’s laws可以改成~(PAGE_MASK | _PAGE_USER)
  这样就好看多了
** static inline int pte_file(pte_t pte)		{ return (pte).pte_low & _PAGE_FILE; }
*** include/asm-i386/pgtable.h:
- ULK:Reads the Dirty flag (when the Present flag is cleared and the Dirty flag is set,
  the page belongs to a non-linear disk file mapping;
** #define mk_pte_huge(entry) ((entry).pte_low |= _PAGE_PRESENT | _PAGE_PSE)
*** include/asm-i386/pgtable.h:
- Sets the Page Size and Present flags of a Page Table entry
** static inline pte_t pte_wrprotect(pte_t pte)	{ (pte).pte_low &= ~_PAGE_RW; return pte; }
*** include/asm-i386/pgtable.h:
** static inline pte_t pte_rdprotect(pte_t pte)	{ (pte).pte_low &= ~_PAGE_USER; return pte; }
*** include/asm-i386/pgtable.h:
** static inline pte_t pte_exprotect(pte_t pte)	{ (pte).pte_low &= ~_PAGE_USER; return pte; }
*** include/asm-i386/pgtable.h:
** #define pgd_index(address) (((address) >> PGDIR_SHIFT) & (PTRS_PER_PGD-1))
*** include/asm-i386/pgtable.h:
- 找到页框所对应的global page table的下标.
** #define pgd_offset(mm, address) ((mm)->pgd+pgd_index(address))
*** include/asm-i386/pgtable.h:
- 从这里可以看出页表和内存描述符结合起来了。
- 产生的是address对应的pgd的页表项的所在的线性地址。
** #define pgd_offset_k(address) pgd_offset(&init_mm, address)
*** include/asm-i386/pgtable.h:
- 其实就是pgd_offset(init_mm, address)
** #define pgd_page(pgd)				(pud_page((pud_t){ pgd }))
*** include/asm-generic/pgtable-nopud.h:
- ULK:Yields the page descriptor address of the page frame containing the Page Upper
  Directory referred to by the Page Global Directory entry pgd . In a two- or three-level
  paging system, this macro is equivalent to pud_page() applied to the folded Page Upper
  Directory entry.
- pgd的页表项是pgd_t类型的还是pud_t类型的。
** #define pmd_offset(pud, address) ((pmd_t *) pud_page(*(pud)) + pmd_index(address))
*** include/asm-i386/pgtable-3level.h:
- pud_page()返回的是pud的页表项,也可以看出pud的页表项是pmd_t类型的。
** #define mk_pte(page, pgprot)	pfn_pte(page_to_pfn(page), (pgprot))
*** include/asm-i386/pgtable.h:
- 用页描述符的线性地址和指定的权限来生成一个pte.
** #define page_to_pfn(page)	((unsigned long)((page) - mem_map))
*** include/asm-i386/page.h:
- 这个函数的作用是把某个页的页描述符转为所对应的页号。
- ULK:All page descriptors are stored in the mem_map array.所以page是某个页框的的页描述符的
  物理地址。
** #define pfn_pte(pfn, prot)	__pte(((pfn) << PAGE_SHIFT) | pgprot_val(prot))
*** include/asm-i386/pgtable-2level.h:
- 为什么无论是pfn_pte()还是pfn_pmd()，都要移PAGE_SHIFT位呢？因为页表项的最高20位表示一个页
  框的物理地址，剩下的12位就是PAGE_SHIFT的大小，这个在设计时就要求一定是一致的，因为
  PAGE_SHIFT是12，所以至少要32-12位才可以表示所有的页框。
** #define pmd_page(pmd) (pfn_to_page(pmd_val(pmd) >> PAGE_SHIFT))
*** include/asm-i386/pgtable-2level.h:
- 根据参数pmd里的20位页框号来得相应的页框的页描述符的地址。至于这个页描述符的地址是线性地址
  还是物理地址，就要看mem_map是线性的还是物理的了。但根椐下面的pmd_page_kernel()使用
  了__va()可以看出是线性地址。所以返回的是页描述符的线性地址。
** #define pfn_to_page(pfn)	(mem_map + (pfn))
*** include/asm-i386/page.h:
- 根据页框号获得页描述地址。
** #define pmd_page_kernel(pmd) ((unsigned long) __va(pmd_val(pmd) & PAGE_MASK))
*** include/asm-i386/pgtable-2level.h:
- 注意与pmd_page()的不同，为什么要分pmd_page()和pmd_page_kernel()呢？
  http://bbs.csdn.net/topics/330112122 所以这个函数是获得内存小于896时，获得内核页表的.
- 那么是不是pmd_page()是给内存大于896的时候用的,而pmd_page_kernel()是给小于896内存用的呢？
- pgd/pud/pmd/pte_index/offset/page, mk_pte(), pte_offset_kernel(),pte_offset_map()
** #define __va(x)			((void *)((unsigned long)(x)+PAGE_OFFSET))
*** include/asm-i386/page.h:
- 就只是加上一个PAGE_OFFSET，那么x是物理地址呢还是线性地址呢？
- ULK:The __pa macro is used to convert a linear address starting from PAGE_OFFSET to the
  corresponding physical address, while the _ _va macro does the reverse.所以从这句话可以看
  出pmd_t的高20位的地址表示的是物理地址,ULK也是这么说的。__pa()和__va()是给内存小于896时用的。
** #define pte_offset_kernel(dir, address) ((pte_t *) pmd_page_kernel(*(dir)) +  pte_index(address))
*** include/asm-i386/pgtable.h:
- ULK:Yields the linear address of the Page Table that corresponds to the linear address
  addr mapped by the Page Middle Directory dir . Used only on the master kernel page
  tables.
- 这个宏用了pmd_page_kernel(),说明pte_offset_kernel()也是用于内存小于896的。
- ULK:The kernel maintains a set of page tables for its own use, rooted at a so-called
  master kernel Page Global Directory. After system initialization, this set of page
  tables is never directly used by any process or kernel thread; rather, the highest
  entries of the master kernel Page Global Directory are the reference model for the
  corresponding entries of the Page Global Directories of every regular process in the
  system.所以是不是说明了pmd_page_kernel()和pte_offset_kernel()这些使用PAGE_OFFSET的函数就
  是访问主内核页表的呢？
- ULK:If PAE is enabled, the kernel uses three-level paging. When the kernel creates a new
  Page Global Directory, it also allocates the four corresponding Page Middle Directories;
  these are freed only when the parent Page Global Directory is released.这句话是不是也暗含
  着如果没有打开PAE的话是不使用3级页表的呢？
- ULK:When two or three-level paging is used, the Page Upper Directory entry is always
  mapped as a single entry within the Page Global Directory.
** pgd_t *pgd_alloc(struct mm_struct *mm)
*** arch/i386/mm/pgtable.c:
- 这个函数分配一个pgd并初始化。
- 为什么PTRS_PER_PMD为1时不用初始化最后那USER_PTRS_PER_PGD个项呢？因为PTRS_PER_PMD == 1表示
  不使用PMD，所以不用给pgd的项从pmd_cache那分配pmd,那么PTRS_PER_PMD == 1时是不是要给pgd的项
  分配pte呢？
- 从以下代码可以看出PAGE_OFFSET的好处了，因为页表项的高20位是物理地址，而从pmd_cache里分配
  的内存是在内核范围内的，所以可以通过__pa(pmd)来获得pmd的物理地址，如果不使用PAGE_OFFSET的
  话而使用普通的虚拟地址那么就很麻烦。
 #+BEGIN_EXAMPLE
		set_pgd(&pgd[i], __pgd(1 + __pa(pmd)));
 #+END_EXAMPLE
** void pgd_free(pgd_t *pgd)
*** arch/i386/mm/pgtable.c:
- 有注释说打开PAE的情况下，用户的pgd项在使用前会被填充重写，就是最后的那1G的空间。所以要释
  放pgd时要先释放它们。那除了这些项之外的其它项在哪里被释放了呢？
** static inline pud_t *pud_alloc(struct mm_struct *mm, pgd_t *pgd, unsigned long address)
*** include/linux/mm.h:
- 这个是在有4级页表的时候才使用的.
- 如果pgd参数所指的pgd项为空的话就分配pud，
** static inline int pgd_none(pgd_t pgd)		{ return 0; }
*** include/asm-generic/pgtable-nopud.h:
- 相应的项是0那么就返回1，就是说如果对应的项是空的那么就返回1，就是说参数pgd为0的话就返回1。
  pgd_none的检查对象是pgd.
** #define pud_none(pud)				0
*** include/asm-i386/pgtable-3level.h:
- pud为0返回1。
** #define pmd_alloc_one(mm, address)		NULL
*** include/asm-generic/pgtable-nopmd.h:
** #define pmd_alloc_one(mm, addr)		({ BUG(); ((pmd_t *)2); })
*** include/asm-i386/pgalloc.h:
- 这是不打开PAE的时候才可以用的。为什么是2的指针呢？
** static inline pmd_t *pmd_alloc_one (struct mm_struct *mm, unsigned long addr)
*** include/asm-x86_64/pgalloc.h:
- 这个就会调用get_zeroed_page()了，所以可以看出是分配一个页的做为一个pmd页表的。
- 返回的类型是pmd_t的指针,说明一个pmd页表里的项的类型是pmd_t的。
** pmd_t fastcall *__pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
*** mm/memory.c:
- 这个函数只有一个实现，是不未定义__PAGETABLE_PMD_FOLDED时才可以调用的。这个
** static inline pmd_t *pmd_alloc(struct mm_struct *mm, pud_t *pud, unsigned long address)
*** include/linux/mm.h:
- 这个是在不定义__ARCH_HAS_4LEVER_HACK时才可以调用的。
- 这个函数调用了__pmd_alloc()
** pte_t fastcall *pte_alloc_map(struct mm_struct *mm, pmd_t *pmd, unsigned long address)
*** mm/memory.c:
- 这个函数是用于从kmap那里分配一个页表.
- 先用pmd_present()来判断Present标志看pmd所指向的page middle页表项所描述的page table是否存
  在。若存在就没有必要再分配了，但是如果被回收了又怎么处理呢？难道被回收ULK：If the entry of a Page
  Table or Page Directory needed to perform an address translation has the Present flag
  cleared, the paging unit stores the linear address in a control register named cr2 and
  generates exception 14: the Page Fault exception.原来访问一个不存在页时分页单元是这样来处
  理来发出Page Fault。
- mm->nr_ptes只在这里增加，说明要给进程分一个页表就要通过这个函数。
- 在ULK里没有介绍struct page_state结构体,这个结构体被声明成了一个per-cpu变量,所以会对每个
  CPU的所有页的状态进行跟踪,但是页不会属于特定的CPU,那么这种跟踪有什么用呢?而这个
  pte_alloc_map()是用来分配一个page table页表的,所以每分配一个页表就会増加
  nr_page_table_pages.这就可以通过把所有CPU的这个值加起来得到.
- pmd_populate(mm, pmd, new)就是把一个值赋给pmd,mm没有用到,对于new是由alloc_pages()分配的，
  应该是一个页描述符的虚拟地址，但是可以通过page_to_pfn()来找到它的页框号。对于
  pmd_populate()是用_PAGE_TABLE作为它的标志，就是(_PAGE_PRESENT | _PAGE_RW | _PAGE_USER |
  _PAGE_ACCESSED | _PAGE_DIRTY)，所以这个是使用了_PAGE_USER标志，也就是说使用
  pte_alloc_map()函数分配的页表是给用户空间使用的。
- 最后还调用了pte_offset_map()，就是调用kmap_atomic()(如果没有使用高端内存就使用
  page_address())，所以就是作为高端内存映射的。
** struct page *pte_alloc_one(struct mm_struct *mm, unsigned long address)
*** arch/i386/mm/pgtable.c:
- 这个函数只有pte_alloc_map()调用。
- 这个函数是分配一个页作为一个page table.
- 只是调用了alloc_pages(),用__GFP_HIGHMEM表示可以在高端内存分配，本应该这样的，因为页表是属
  于进程的。用了__GFP_REPEAT表示可能会休眠。用了__GFP_ZERO表示会清零。
** pte_t fastcall * pte_alloc_kernel(struct mm_struct *mm, pmd_t *pmd, unsigned long address)
*** mm/memory.c:
- 这个与pte_alloc_map()不同了，不是通过高端内存来分配,所以调用了pte_alloc_one_kernel()
** pte_t *pte_alloc_one_kernel(struct mm_struct *mm, unsigned long address)
*** arch/i386/mm/pgtable.c:
- 使用了__get_free_page()来获取一个页表，所以不能分配高端内存，这也是因为内核不能访问高端内
  存.所以这个函数返回的内存是内核可以访问的。
** #define pmd_populate_kernel(mm, pmd, pte) set_pmd(pmd, __pmd(_PAGE_TABLE + __pa(pte)))
*** include/asm-i386/pgalloc.h:
- 这个函数被pte_alloc_kernel()调用
- 这个函数用来设置pmd项的值的。
- 与pmd_populate()的主要的不同是物理地址的计算不同了，因为是低端地址，所以可以用__pa()来获
  取物理地址。
** static inline void pte_free(struct page *pte)
*** include/asm-i386/pgalloc.h:
- 这个是调用__free_page()的，那么被释放的内存应该是不分高低端内存的。
** static inline void pte_free_kernel(pte_t *pte)
*** include/asm-i386/pgalloc.h:
- 这个函数调用了free_page(),转而调用free_pages(),转而调用__free_pages(),里面有一个用
  virt_to_page()的调用
** #define virt_to_page(kaddr)	pfn_to_page(__pa(kaddr) >> PAGE_SHIFT)
*** include/asm-i386/page.h:
- 这个宏的意思就是把一个地址转为struct page *的地址, 而被转的地址是一个低端地址.
** void __init paging_init(void)
*** arch/i386/mm/init.c:
- 这个函数是用来初始化主内核全局页表的，主要就是调用pagetable_init()
** static void __init pagetable_init (void)
*** arch/i386/mm/init.c:
- 为什么要在打开PAE的情况下要把页表项指向empty_zoro_page呢?
- cpu_has_pse检测pse,是从boot_cpu_data那里来的,转而从new_cpu_data那里来的,有注释这样说:cpu
  data as detected by the assembly code in head.S.若设置这个标志那么就设置cr4寄存器的
  PSE位.pse的全称是page size extern.
- 与cpu_has_pse相似,有一个cpu_has_pge,但是为什么会设置_PAGE_GLOBAL而不设置_PAGE_GLOBAL呢?
- 然后调用kernel_physical_mapping_init()
** static void __init kernel_physical_mapping_init(pgd_t *pgd_base)
*** arch/i386/mm/init.c:
- 这个函数会把主内核页表里的全局页表从虚拟地址PAGE_OFFSET开始的页表项到最后一个页表项都分配
  相应的pud,pmd和pte(只要存在相应的级页表存在)。
- 注释这样说它的作用: This maps the physical memory to kernel virtual address space, a
  total of max_low_pfn pages, by creating page tables starting from address PAGE_OFFSET.
- max_low_pfn : Page frame number of the last page frame directly mapped by the kernel
  (low memory)
- 因为要映射的线性地址是从PAGE_OFFSET开始的,所以要用pgd_index()来找到对应的项的下
- 为什么会出现pfn不小于max_low_pfn后再continue的情况呢？在max_low_pfn小于1G(896M)的时或在使
  用3级页表时(因为只要1个pgd页表的项就可以表示1G了)，但是因为还要给后面的pgd页表项分配pmd页
  表，所以还要continue来调用one_md_table_init().
- 如果在80x86（32bit）上使用扩展页表，那么只使用一级一页表，就是pgd,因为offset要用22位才可
  以达到4M的页框，如果80x86上使用扩展页表且打开PAE,那么页框是2M的，使用的是2级页表外加一个
  PDPT。
- 这个is_kernel_text()在虚拟地址是大于PAGE_OFFSET且小于__init_end的时候才返回1。
** static pmd_t * __init one_md_table_init(pgd_t *pgd)
*** arch/i386/mm/init.c:
- 在这里可以看出对于打开了PAE的32位x86来说是使用了3级页表的,而不打开PAE的32位x86来说是使用
  了2级页表.所以在打开了PAE的时候这个函数里会分一个pmd页表, 而使用3级页表的时候的
  pud_offset()是直接把参数pgd转换pud_t类型后返回的.在不打开PAE的时候因为pud_offset()和
  pmd_offset()都是返回参数的,所以也不用分配什么页表,所以到最后整个函数返回的就是参数pgd.所
  以在调用这个函数的kernel_physical_mapping_init()里的最外层的for循环的pmd在使用2级页表的时
  候就是pgd的值
** static pte_t * __init one_page_table_init(pmd_t *pmd)
*** arch/i386/mm/init.c:
- pmd_page_kernel(pmd)函数返回pmd页表项pmd参数所对于page talbe的页表的页的虚拟地址。
- pte_offset_kernel(pmd, 0)的意思是从pmd页表项pmd所对应的page talbe页表里找出第二个参数所对
  应的pte页表项。
- 总结一下...page...类的函数返回的是参数所对应的页表项的下一级页表的页框；...offset...类的
  函数返回的是把第一个参数作为页表项找出下一级页表的页框(使用...page...)后再用第二参数作为
  虚拟地址从页表中找到对应的页表项(使用...index()函数)。
** static void __init page_table_range_init (unsigned long start, unsigned long end, pgd_t *pgd_base)
*** arch/i386/mm/init.c:
- 以参数pgd_base为全局页表的首地址，把start到end之间的虚拟地址都分配相应的pmd页表和pte页表。
- 如果没有使用pud这一级页表，那么PUD_SHIFT等于PGDIR_SHIFT，又如果没有使用pmd这一级页表，那
  么PMD_SHIFT等于PUD_SHIFT。所以在这个函数里如果一共使用了2级页表，那么vaddr += PMD_SIZE就
  是vaddr += PGDIR_SIZE.
** static void __init permanent_kmaps_init(pgd_t *pgd_base)
*** arch/i386/mm/init.c:
- 这个函数用来初始化(给所有的永久映射虚拟地址分配页表和页框(就是调用了
  page_table_range_init))永久映射页表的。最后把基虚拟地址映射的页框赋给了pkmap_page_table.
- 先是调用page_table_range_init()来把从PKMAP_BASE开始长度为PAGE_SIZE*LAST_PKMAP的虚拟地址的
  页表全分配了，再连续调用pud_offset(),pmd_offset(),pte_offset_kernel()来找出PKMAP_BASE地址
  对应的
- 永久映射使用的页表是主内核的页表(swapper_pg_dir)，只使用一个pte页表，所以大小就是一个pte
  页表所能表示的范围。
- 永久映射也作为内核映射高端内存的其中一种机制。
** void zap_low_mappings (void)
*** arch/i386/mm/init.c:
- 为什么要保存原来的swapper_pg_dir页表保存在swsusp_pg_dir呢？
- 把swapper_pg_dir里从0到USER_PTRS_PER_PGD的所有页表项那清零了，所以到最后主内核页表的前
  USER_PTRS_PER_PGD都是空的。




- 内核可以采用三种不同的机制将页框映射到高端内存；分别叫做永久内核映射、临时内核映射以及非
  连续内存分配。
- http://www.linuxidc.com/Linux/2012-02/53457.htm
- 永久内核映射允许内核建立高端页框到内核地址空间的长期映射。使用主内核页表中一个专门的页表，
  其地址存放在变量pkmap_page_table中，那是不是说明了永久内核映射所能映射的范围是2的22次方的
  大小呢？（如果Page Table是10位，offset是12位）。页表中的表项数由LAST_PKMAP宏产生。
- pkmap_count数组包含LAST_PKMAP个计数器，pkmap_page_table页表中的每一项都有一个。
- 这里由定义可以看出永久内存映射为固定映射下面的4M/2M空间(用了PMD_MASK), 注意是在FIXADDR_BOOT_START处地址减的
 #+BEGIN_EXAMPLE
#define PKMAP_BASE ((FIXADDR_BOOT_START - PAGE_SIZE * (LAST_PKMAP + 1)) \   
            & PMD_MASK)
 #+END_EXAMPLE
- 高端映射区逻辑页面的分配结构用分配表(pkmap_count)来描述，它有1024项，对应于映射区内不同的
  逻辑页面。当分配项的值等于0时为自由项，等于1时为缓冲项，大于1时为映射项。映射页面的分配基
  于分配表的扫描，当所有的自由项都用完时，系统将清除所有的缓冲项，如果连缓冲项都用完时，系
  统将进入等待状态。
- 为了记录高端内存页框与永久内核映射包含的线性地址之间的联系，内核使用了
  page_address_htable散列表。该表包含一个page_address_map数据结构，用于为高端内存中的每一个
  页框进行当前映射。而该数据结构还包含一个指向页描述符的指针和分配给该页框的线性地址。
- page_address()函数返回页框对应的线性地址
- 有注释这样说：
 #+BEGIN_EXAMPLE
/*
 * Ordering is:
 *
 * FIXADDR_TOP
 * 			fixed_addresses
 * FIXADDR_START
 * 			temp fixed addresses
 * FIXADDR_BOOT_START
 * 			Persistent kmap area
 * PKMAP_BASE
 * VMALLOC_END
 * 			Vmalloc area
 * VMALLOC_START
 * high_memory
 */
 #+END_EXAMPLE

** void *kmap_atomic(struct page *page, enum km_type type)
*** arch/i386/mm/highmem.c:
- 这个函数用来映射临时内存映射(fix map)的.
- 为什么要禁止抢占呢？有注释说:even !CONFIG_PREEMPT needs this, for in_atomic in
  do_page_fault
- 对于每个类型的内存，不同的CPU都是映射到相同的内存，为什么要这样做呢？
** #define __fix_to_virt(x)	(FIXADDR_TOP - ((x) << PAGE_SHIFT))
*** include/asm-i386/fixmap.h:
- 从这个函数可以看出，参数x是一个km_type的一个下标，一个下标表示一个页框（因为用了
  PAGE_SHIFT），因为用减法，所以可以看出下标越大那么它的虚拟地址越小，
- FIXADDR_TOP是一个宏，所以是固定的，是0xfffff000，因为大于0xc0000000，所以是在内核虚拟地址
  内的。但是为什么不是0xffffffff呢？要把最后的4k的大小的虚拟内存出来呢？
- 这个宏在ULK里的实现时使用了编译器的特点做到了检查参数。
** static void set_pte_pfn(unsigned long vaddr, unsigned long pfn, pgprot_t flags)
*** arch/i386/mm/pgtable.c:
- 这个函数的作用就是通过vaddr找到相应的page talbe页表项，然后再用参数页框号pfn和标志初始化
  找到的页表项。
** void __set_fixmap (enum fixed_addresses idx, unsigned long phys, pgprot_t flags)
*** arch/i386/mm/pgtable.c:
- 这个用idx通过__set_fixmap()找到相应的虚拟地址，找到之后再用set_pte_pfn()来设置找到的虚拟
  地址所对应的Page Talbe页表项。
** void *page_address(struct page *page)
*** mm/highmem.c:
- ULK：returns the linear address associated with the page frame, or NULL if the page
  frame is in high memory and is not mapped.
- 这个函数只由内核用来查找一个内核使用的线性地址所对应的页框的页描述符。
- 通过查页描述符的PG_highmem来判断该页是否在高端内存里。
- 如果是低端内存那么就调用lowmem_page_address()使用PAGE_SHIFT,PAGE_OFFSET,_va()这些东西找到
  虚拟地址。
- 虽然是内核使用的，但内核也可以使用高端内存，因为有内核有3种方式可以将内核虚拟地址映射到高
  端内存。但是这个函数在找高端内存时只能找永久映射部分的，那么其它的两种方式就找不到了吗？
- 如果是高端内存，那么就用page_slot()找到该页对应的散列项，找到之后再对找到散列项链表里的所
  有结点进行遍历找到相应的结点，找到结点之后就返回结点里的virtual成员，这个成员就是要找的虚
  拟地址，结点还有一个page成员表示该结点对应哪个页描述地址。
** void fastcall *kmap_high(struct page *page)
*** mm/highmem.c:
- 这个函数的作用就是把参数page指向的高端内存页框映射到永久虚拟地址里去。
- 先用page_address()来找出page所指向的虚拟地址，如果这个地址没有映射，那么就会返回0，每次调
  用这个函数都会把这个页指向的虚拟地址的使用计数加1，如果小于2，那么就BUG()。如果为0，那么
  就说明这个页没有被映射，那么就调用map_new_virtual(),
** static inline unsigned long map_new_virtual(struct page *page)
*** mm/highmem.c:
- 这个函数的作用就是为参数page从永久映射区里找一个线性地址来关连起来，并设置相应的页表项。
- last_pkmap_nr是用来保存上一次调用这个函数时找到的合适的虚拟地址的,累加时使用
  LAST_PKMAP_MASK相与，所以可以自动处理回绕。
- 如果last_pkmap_nr为0表示回绕了，可以刷新一下不使用的页表项了，用flush_all_zero_pkmaps()刷
  新
- 如果LAST_PKMAP个项都扫描过了,那么就要休眠一下等其它的进程释放内存,这也说明这个函数会休眠,不
  能作原子操作.
- 等待释放内存有一个专门等待队列叫pkmap_map_wait,把等待对象插入之后就调度,调度回来就把对象
  从等待队列里删除,这时还要检查是不是对象又被分配出去了,如果是又被分配出去的话就不用再等待
  了而是把现在的虚拟地址返回,为什么不用再等待了,难道再次被其它进程分配的就是想要的吗?的确是
  我们想要的因为调用这个函数就是想把一个页框找一个永久映射的虚拟地址把它给关联起来,所以调用
  page_address()时发现这个页框已有虚拟地址与它关联,那么就返回这个地址就可以了.
- 如果休眠回来发现页框没有被映射,那么就再用原来的步骤扫描一次.
- 若找到了相应的虚拟地址,就用页框号和相应的kmap_prot标志初始化页表项,把在pkmap_count相应的
  项设为1,再调用set_page_address()在相应的散列表项里加上这个页框相应的结点.
** static void flush_all_zero_pkmaps(void)
*** mm/highmem.c:
- 会用循环遍历所有的永久映射页表项来清除不使用的页表项.
- 页表项对应的pkmap_count项不为1表示还使用,为1表示没有使用了.为1时就设回0.
- 用pte_clear()直接把对应的页表项清零就行了.
** void set_page_address(struct page *page, void *virtual)
*** mm/highmem.c:
- 这个函数用于永久内存映射的.ULK:insert a new element in the page_address_htable hash
  table
- 有个一叫page_address_pool的内存池,是struct page_address_map的内存池
- 如果参数virtual不为空,那么就从page_address_pool里取一个struct page_address_map,然后用参数
  page和virtual初始化struct page_address_map,最后把struct page_address_map加到散列表里去.
- 如果参数virtual为空,那就以参数page为标准从相应的散列表里找到相应的结点struct
  page_address_map,找到了之后就从散列表里删除这个结点,再把这个结点加到page_address_pool内存
  池里,但是没有把删除的结点清零.
** void fastcall kunmap_high(struct page *page)
*** mm/highmem.c:
- 先用page_address()和参数page找到相应的虚拟地址,再用找到虚拟地址和PKMAP_NR()找到对应的下
  标,再用找到的下标和pkmap_count数组找到使用计数减1,为1就会调用witqueue_active()看
  pkmap_map_wait队列是否为空,若不为空,就调用wake_up()把pkmap_map_wait把唤醒队列里的进程.

** void *kmap_atomic(struct page *page, enum km_type type)
*** arch/i386/mm/highmem.c:
- 临时映射(fix-map)不是把某个页框一直映射到一个页表项来达到不休眠的,而是有一个内核模块使用
  来达到的,所以不使用时被映射的页框会被释放.
- 之前对enum fixed_address的理解有错误,以为只对高端内存有用,原来还有其它的使用,就是除了
  CONFIG_HIGHMEM,还有CONFIG_ACPI_BOOT,CONFIG_X86_IO_APIC这类的东西.而enum km_type里的成员才
  是一个成员对应一个模块的,那么enum fixed_address里的所有可能的成员的地址的大小是不是是
  128M呢?如果是这样子就有疑问了,高端内存只有一个内核模块使用吗?现在又明白一点了,不是高端内
  存只有一个内核模块使用,而是高端内存有多个内核模块使用,要想知道有谁使用,不是看enum
  fixed_address,而是看enum km_type,km_type里面的东西才是说明有哪些内核模块使用的,如
  KM_USER0,KM_PTE0,KM_IRQ0等等.


** static void __init kmap_init(void)
*** arch/i386/mm/init.c:
- 这个函数被paging_init()调用,paging_init()被setup_arch()调用,在paging_init()里是先调用
  pagetable_init()再调用kmap_init()的.
- 调用完pagetable_init()之后,fix-mapping的线性地址(对于特定的模块已经是固定的了)已经指向了
  特定的Page Table.所以以虚拟地址调用kmap_get_fixmap_pte()(用pte_offset_kernel()实现)返回就
  是对应的页表项了.
- kmap_pte指向开头的Page Table页表项,这个在kmap_atomic()里使用,kmap_prot为所有fix-mapping页
  表项的权限,这个也在kmap_atomic()里使用.这个函数就只是初始化这两个变量而已.而相应的页表项
  在paging_init()里被初始化了.
** void *kmap_atomic(struct page *page, enum km_type type)
*** arch/i386/mm/highmem.c:
- 这个函数为什么要禁止抢占呢?在这里禁止抢占而在kunmap_atomic()使能抢占.
- 因为fix-mapping的页表paging_init()里被初始化,所以在这个函数里只要把页框号填到相应的页表项
  就可以了,而不用分配页表项,所以这个函数执行起来也很快.

- mem_map包含了所有的页框,而每个zone包含了其中的一段,zone结构体里的zone_mem_map指向了所包含
  的那段的开头的下标,size表示包含段的大小.free_area是一个结构体,而zone结构体里的free_area成
  员是一个free_area结构体数组.
- zone结构体里的free_area数组包含了11个不同页大小的链表.页表描述符的lru把链表里的页描述符连
  起来.
- struct free_zone结构体里的free_list成员是页描述符的链表头.
- 页描述符的private表明了该页被链在哪个free_area元素下,就是下标.
- 从pte_offset_map()的实现来看，kmap_atomic()返回的是一个pte页表的起始地址,因为传给
  kmap_atomic()的page是pmd_page()返回的地址。
** static struct page *__rmqueue(struct zone *zone, unsigned int order)
*** mm/page_alloc.c:
- 这个函数从伙伴系统里分配页.
- 如果对应order的那个链表没有结点了的话那么就要从其它的更大的order的链表里拆分了.拆分之后的
  合并在expand()函数里处理.
- 因为是在一组连续的页框里的第一个页框设置了页描述符的private成员为order而其它的页没有设置,所
  以要把找到的连续的页框的第一个页框的页描述符的private给设为0,还要把该页的页描述符的flags
  标志的PG_private给清掉.
- zone->nr_free减1
- zone->free_pages要减去order对应的页的大小,而不管是否从其它更大的order里拆出页来.
** static inline struct page *expand(struct zone *zone, struct page *page,int low, int high, struct free_area *area)
*** mm/page_alloc.c:
- 这个函数被__rmqueue()调用来处理因不满足分配大小的要求而拆其它更大的页组.
- 用循环来处理拆页合页操作,low是要求的order,high是实际被分到的order,要循环high-low次.
** static inline void __free_pages_bulk (struct page *page, struct zone *zone, unsigned int order)
*** mm/page_alloc.c:
- 一个order为x的链表里的一个结点就是一个页描述符,而这个页描述符对应的页框之后连续2^x个页框
  就是一组的,那么结点的那个页的页框地址是不是一定是2^x*2^12的倍数呢?而在这个页的后面连续页
  框就不能是这个的倍数了.这也就是为什么合并两个块时要ULK说的这个条件:The physical address
  of the first page frame of the first block is a multiple of 2 x b x 2^12.
- 还要注意一个问题,就是被链接到free_area的页描述符所对应的页框号的最低11位不能全为0
- 每个循环里page局部变量只会减小或不变,不会增大因为combined_idx不会比page_idx大,但这样的设
  计是有问题的吧?因为这样的设计只能让page为头的这一block不能与之前的block合并,只能与后面的
  合并.这句话中的后面那部分说错了,page的确是只能减小不能增大,
- 在ULK里很好的说明了buddy这个变量,但是没有说明combined_idx这个变量.
- 其实在合并时就两种情况:
 #+BEGIN_EXAMPLE
  1, page_idx的order位为0时有以下情况

  __find_combined_index()返回的值等于page_idx,返回的值是赋给combined_idx的,所以在后面执行
  page=page+(combined_idx-page_idx)之后page其实不变.

  __page_find_buddy()返回的值是page+order_size就是page+(1<<order),返回的值是赋给buddy的,所
  以在执行完page=page+(combined_idx-page_idx)之后,page < buddy的.所以也可以得出如果两个块合
  并,那么是物理地址最小的那页作为块头被链到块链表里.

  这种情况就是与比page地址大的地址合并.

  --------------------------------------------------------------------
  2, page_idx的order位为1时有以下情况

  __find_combined_index()返回的值小于(这个与情况1不同)page_idx,返回的值是赋给
  combined_idx的,所以在后面执行page=page+(combined_idx-page_idx)之后page变小,小了
  (1<<order)(与1不同).

  __page_find_buddy()返回的值是page-order_size(这个与情况1不同)就是page-(1<<order),返回的值
  是赋给buddy的,所以在执行完page=page+(combined_idx-page_idx)之后,page = buddy的.所以也可以
  得出如果两个块合并,那么是物理地址最小的那页作为块头被链到块链表里.

  这种情况就是与比page地址小的地址合并.

  -----------------------------------------------------------------
  这两种情况下的page都会指向该块的最小的地址.

 #+END_EXAMPLE
** static int rmqueue_bulk(struct zone *zone, unsigned int order,unsigned long count, struct list_head *list)
*** mm/page_alloc.c:
- 这个函数的作用就是从zone区里分配count个大小为2^order的块,把这些块插到list链表里去,是用
  page->lru链起来的.
** static struct page *buffered_rmqueue(struct zone *zone, int order, unsigned int __nocast gfp_flags)
*** mm/page_alloc.c:
- 如果order是大于0的话,那么就和调用__rmqueue()一样,如果order为0那么就是从per-CPU page
  frame caches里获取了,
- 虽然__rmqueue()也可以分配order为0的页框,但是在分配时有可能会拆更大的块,而且在释放时很可
  能又会把这页框合并,如果是是频繁分配和释放的话就会很浪费时间.所以需要这样的单页cache.
** static void fastcall free_hot_cold_page(struct page *page, int cold)
*** include/linux/page-flags.h:
- 这个函数是把页释放到per-CPU caches里,所以order肯定是0.
- 这个函数的会增加page_state->pgfree
- per-CPU cache是放在zone->pageset链表里的,而且是每个CPU都有,每个zone都有自已的pageset.
- struct per_cpu_cpuset里就只有一个struct per_cpu_pages 类型的2元数组pcp,一个是hot cache一
  个是cold cache.
- 这个函数会调用free_pages_bulk()来释放所有的页框.但是这个函数不只是释放order为0的块.
** static int free_pages_bulk(struct zone *zone, int count, struct list_head *list, unsigned int order)
*** mm/page_alloc.c:
- 这个函数会把count个order为order的块放到zone里,而这些链表是放在list链表里的.
- zone->pages_scanned在这里清零,为什么要这样做呢?pages_scanned的解释ULK:Counter used when
  doing page frame reclaiming in the zone.
- zone->pages_unreclaimable也清0,ULK:Flag set when the zone is full of unreclaimable
  pages.

- per-CPU cache分配器所得到的页是以order为0调用buffered_rmqueue()来分配的,转而调用
  rmqueue_bulk(),再转而调用__rmqueue();zone分配器是通过__alloc_pages()来分得页的,转而以
  order可能不为0调用buffered_rmqueue(),转而有可能调用__rmqueue()或rmqueue_bulk().所以可以得
  出这两个分配器最终都是调用__rmqueue()分配页的.

- zone->pages_low:Low watermark for page frame reclaiming; also used by the zone allocator
  as a threshold value.

** int zone_watermark_ok(struct zone *z, int order, unsigned long mark, int classzone_idx, int can_try_harder, int gfp_high)
*** mm/page_alloc.c:
- 这个函数返回1表示有足够的页.
- can_try_harder为1表示可以在更难的情况下来分配,所以min会比较大,gfp_high表示min可以小一点.
- 首先要确的如果真的分配了(1<<order)+1个页之后,那么保留页框(zone->lowmem_reserve[])和
 #+BEGIN_EXAMPLE
	if (free_pages <= min + z->lowmem_reserve[classzone_idx])
 #+END_EXAMPLE
- 但是第二个条件是什么意思呢?ULK里的看不明白,到底与zone->pages_low有什么关系呢?
** struct page * fastcall __alloc_pages(unsigned int __nocast gfp_mask, unsigned int order, struct zonelist *zonelist)
*** mm/page_alloc.c:
- 第一次扫描zone->zonelist里的区时不用can_try_harder和gfp_high,所以第一次扫描时调用的
  zone_watermark_ok()的最后两个参数为0.
 #+BEGIN_EXAMPLE
		if (!zone_watermark_ok(z, order, z->pages_low,
				       classzone_idx, 0, 0))
 #+END_EXAMPLE
- 扫描完第一次之后如果没有可用的页就要唤醒所有zone的回收页框线程了。
- 第二次扫描的时候调用zone_watermark_ok()时比第一次加上can_try_harder和__GFP_HIGH.还有一个
  和第一次扫描不一样的地方是加多一个是等待的判断，不知道这个判断有什么用。
- ULK：and it is trying to reclaim page frames (either the PF_MEMALLOC flag or the
  PF_MEMDIE flag of current is set)。TIF_MEMDIE只有select_bad_process()和__alloc_pages()读
  取来判断，只有__oom_kill_task()设置。
 #+BEGIN_EXAMPLE
	if (((p->flags & PF_MEMALLOC) || unlikely(test_thread_flag(TIF_MEMDIE)))
 #+END_EXAMPLE
- 若进程想回收页框且不在中断(软硬)中就执行第三次扫描，buffer_rmqueue()这个函数是不休眠的.但
  是第三次的扫描是不管wait的,但是已经不用zone_watermark_ok()了。zone_watermark_ok()只有第一
  次和第二次的时候使用。
- 第三次扫描完之后如果不能等待那么不再做什么了，而是直接退出。唤醒进程是在第一次扫描时。
 #+BEGIN_EXAMPLE
	if (!wait)
		goto nopage;
 #+END_EXAMPLE
- 第三次扫描完之后如果可以等待就马上调用cond_resched()看是不是可以调度以其它进程可以回收内
  存。
 #+BEGIN_EXAMPLE
  rebalance:
	cond_resched();
 #+END_EXAMPLE
- 调度回来之后就设置PF_MEMALLOC，表明该进程开始回收内存了,同时也要设置一个reclaim_state,之
  后就调用try_to_free_pages()
 #+BEGIN_EXAMPLE
	p->flags |= PF_MEMALLOC;
	reclaim_state.reclaimed_slab = 0;
	p->reclaim_state = &reclaim_state;
	did_some_progress = try_to_free_pages(zones, gfp_mask, order);
	p->reclaim_state = NULL;
	p->flags &= ~PF_MEMALLOC;
 #+END_EXAMPLE
- try_to_free_pages()之后还要还要看看是不是需要调度。
 #+BEGIN_EXAMPLE
	cond_resched();
 #+END_EXAMPLE
- 调度回来之后若try_to_free_pages()返回成功就再以第二次相同的方法扫描。
 #+BEGIN_EXAMPLE
	if (likely(did_some_progress)) {
 #+END_EXAMPLE
- 若try_to_free_pages()没有成功返回，且__GFP_FS设置了，__GFP_NORETRY没设，那么就以第一次扫
  描的方式来扫描。且在这种情况下若没找到内存，那么就回到这个函数的开头重新执行。

** int __sched cond_resched(void)
*** kernel/sched.c:
- 如果TIF_NEED_RESCHED设置了，那么就会禁止抢点然后调度再使能中断，如果调度回来之后发现
  TIF_NEED_RESCHED还设置，那么再禁止抢点然后调度再使能中断。
- 如果__GFP_NORETRY没有设置且order不大于3或__GFP_REPEAT设置了，那么就调用
  blk_congestion_wait()来等待。如果__GFP_NORETRY没有设置且__GFP_NOFAIL设置了，那么就调用也
  blk_congestion_wait()
- 这个函数一共有5个地方来扫描zone.
** fastcall void __free_pages(struct page *page, unsigned int order)
*** mm/page_alloc.c:
- 如果是保留的页就不用释放页，那么保留页怎么处理呢？
- 如果是page->_count减1之后不为0，那么也不能释放页，因为还有其它进程在使用。
- 如果order为0，那么调用free_hot_page()放到per-CPU cache里。
- 如果order不为0，那么就调用__free_pages_ok()
** void __free_pages_ok(struct page *page, unsigned int order)
*** mm/page_alloc.c:
- 要修改page_state->pgfree.
- 为什么使用MMU才会递减page->_count计数器呢？
 #+BEGIN_EXAMPLE
#ifndef CONFIG_MMU
	if (order > 0)
		for (i = 1 ; i < (1 << order) ; ++i)
			__put_page(page + i);
#endif
 #+END_EXAMPLE
- 要把这个块插到一个空的链表头里去。
 #+BEGIN_EXAMPLE
	list_add(&page->lru, &list);
 #+END_EXAMPLE
- 最后调用free_pages_bulk()把页释放。



- slab allocator是属于Memory Area Management这一部分的。
- 在slab分配器里一个cache有多个slab，一个slab有多个object
- ULK:A first cache called kmem_cache whose objects are the cache descriptors of the
  remaining caches used by the kernel. The cache_cache variable contains the descriptor of
  this special cache.
- 关于对齐因子的解释，ULK：The objects managed by the slab allocator are aligned in
  memorythat is, they are stored in memory cells whose initial physical addresses are
  multiples of a given constant, which is usually a power of 2. This constant is called
  the alignment factor.最大的对齐因子是一个页的大小
- ULK:microcomputers access memory cells more quickly if their physical addresses are
  aligned with respect to the word size (that is, to the width of the internal memory bus
  of the computer).Thus, by default, the kmem_cache_create( ) function aligns objects
  according to the word size specified by the BYTES_PER_WORD macro.
- ULK:When creating a new slab cache, it's possible to specify that the objects included in it
  be aligned in the first-level hardware cache. To achieve this, the kernel sets the
  SLAB_HWCACHE_ALIGN cache descriptor flag. The kmem_cache_create( ) function handles the
  request as follows: If the object's size is greater than half of a cache line, it is
  aligned in RAM to a multiple of L1_CACHE_BYTESthat is, at the beginning of the
  line. eOtherwise, the object size is rounded up to a submultiple of L1_CACHE_BYTES; this
  ensures that a small object will never span across two cache lines.
- Slab中引用着色机制是为了提高L1缓冲的效率。我们知道linux是一个兼容性很高的平台，但现在处理
  器的缓冲区方式多种多样，有的每个处理器有自己的独立缓存。有的很多处理器共享一个缓存。有的
  除了一级缓存（L1）外还是二级缓存（L2），因此，linux为了更好的兼容处理平台，只优化了一级缓
  存.http://blog.chinaunix.net/uid-9543173-id-1988997.html
- 使用着色的原因的一个是让不相同大小的对象尽量不放在同一个cache line里，另一个原因是

** void __init kmem_cache_init(void)
*** mm/slab.c:
- 这个函数的注释我喜欢，先来个总的说明，
- cache_chain这个是所有kmem_cache_t的链表头。cache_cache是第一个结点。
  https://www.ibm.com/developerworks/cn/linux/l-linux-slab-allocator/ 有图说明
** kmem_cache_t *kmem_cache_create (const char *name, size_t size, size_t align, unsigned long flags, void (*ctor)(void*, kmem_cache_t *, unsigned long), void (*dtor)(void*, kmem_cache_t *, unsigned long))
*** mm/slab.c:
- size的大小是指object的大小。
- 开始要调整参数size,不足一个字的要补足一个字
 #+BEGIN_EXAMPLE
	if (size & (BYTES_PER_WORD-1)) {
		size += (BYTES_PER_WORD-1);
		size &= ~(BYTES_PER_WORD-1);
	}
 #+END_EXAMPLE
- 处理对齐的那段代码还是看不明白。
- 要在cache的大小小于512M或内部碎片的有足够的空间放slab描述符时才放在外部，这时会设置CFLGS_OFF_SLAB.
 #+BEGIN_EXAMPLE
Slab descriptors can be stored in two possible places:
External slab descriptor
Stored outside the slab, in one of the general caches not suitable for ISA DMA pointed to by
cache_sizes (see the next section).
Internal slab descriptor
Stored inside the slab, at the beginning of the first page frame assigned to the slab.
The slab allocator chooses the second solution when the size of the objects is smaller than 512MB or
when internal fragmentation leaves enough space for the slab descriptor and the object descriptors
(as described later)inside the slab. The CFLGS_OFF_SLAB flag in the flags field of the cache descriptor
is set to one if the slab descriptor is stored outside the slab; it is set to zero otherwise.
 #+END_EXAMPLE
- 为什么要在对象的大小大于页大小的1/8时就把slab描述符放在外部呢？难道是因为对象太大以至于没
  有多余的碎片了吗？有一个这样的注释:Size is large, assume best to place the slab management obj off-slab
  (should allow better packing of objs).所以这样做也只是一个猜想的结果。
 #+BEGIN_EXAMPLE
	/* Determine if the slab management is 'on' or 'off' slab. */
	if (size >= (PAGE_SIZE>>3))
		/*
		 * Size is large, assume best to place the slab management obj
		 * off-slab (should allow better packing of objs).
		 */
		flags |= CFLGS_OFF_SLAB;
 #+END_EXAMPLE
- cache->gfporder: Logarithm of the number of contiguous page frames included in a single
  slab. ULK:If the slab cache has been created with the SLAB_RECLAIM_ACCOUNT flag set, the
  page frames assigned to the slabs are accounted for as reclaimable pages when the kernel
  checks whether there is enough memory to satisfy some User Mode requests.就是说设置了
  SLAB_RECLAIM_ACCOUNT标志,那么表示这个slab里的页框在当内核检查是否有足够的内存来满足用户模
  式请求时可以回收这个slab.
- 在SLAB_RECLAIM_ACCOUNT设置且对象的大小小于页大小时,就把cache->gfporder设为0,这样做是不是
  因为页可以被轻易回收所以就把gfporder设置尽量小,且又因为一个对象的大小小于一个页大小,所以
  可以把cache->gfporder设成最小0.
- 第一个do{}while循环主要的作用是确定cache->gfporder,cache->num和内部碎片(left_over局部变
  量).cache->gfporder一开始是0,在这个循环的过程中会不断地增加.

 #+BEGIN_EXAMPLE
			cachep->gfporder++;
 #+END_EXAMPLE
  如果cache->gfporder在增加的过程中超过了MAX_GFP_ORDER,那么就要退出循环

 #+BEGIN_EXAMPLE
			if (cachep->gfporder >= MAX_GFP_ORDER)
				break;
 #+END_EXAMPLE
  在这里cache->num一定不能等于0

 #+BEGIN_EXAMPLE
			if (!cachep->num)
				goto next;
 #+END_EXAMPLE
  关于offslab_limit的注释：Max number of objs-per-slab for caches which use off-slab
  slabs. Needed to avoid a possible looping condition in cache_grow().所以有做这样的处理：
 #+BEGIN_EXAMPLE
			if (flags & CFLGS_OFF_SLAB &&
					cachep->num > offslab_limit) {
				/* This num of objs will cause problems. */
				cachep->gfporder--;
				break_flag++;
				goto cal_wastage;
			}
 #+END_EXAMPLE
- slab_break_gfp_order就只有0或1这两种值，所以一个cache里最多只能有1个页或2个页。
 #+BEGIN_EXAMPLE
			if (cachep->gfporder >= slab_break_gfp_order)
				break;
 #+END_EXAMPLE
- 可允许的内部碎片是这样子计算的
 #+BEGIN_EXAMPLE
			if ((left_over*8) <= (PAGE_SIZE<<cachep->gfporder))
				break;	/* Acceptable internal fragmentation. */
 #+END_EXAMPLE
- 如果设了CFLGS_OFF_SLAB且内部碎片不小于slab描述符和所有对象描述符的大小,那么就清掉CFLGS_OFF_SLAB

 #+BEGIN_EXAMPLE
	/*
	 * If the slab has been placed off-slab, and we have enough space then
	 * move it on-slab. This is at the expense of any extra colouring.
	 */
	if (flags & CFLGS_OFF_SLAB && left_over >= slab_size) {
		flags &= ~CFLGS_OFF_SLAB;
		left_over -= slab_size;
	}
 #+END_EXAMPLE
- 内部和外部的存放不同那么计算slab_size的大小也不一样。
 #+BEGIN_EXAMPLE
	slab_size = ALIGN(cachep->num*sizeof(kmem_bufctl_t)
				+ sizeof(struct slab), align);
	if (flags & CFLGS_OFF_SLAB) {
		/* really off slab. No need for manual alignment */
		slab_size = cachep->num*sizeof(kmem_bufctl_t)+sizeof(struct slab);
	}

 #+END_EXAMPLE
- cache->colour_off，ULK:Basic alignment offset in the slabs.
- cache->colour,ULK：Number of colors for the slabs 
- cache->colour_off就是cache_line_size()，就是cache行的大小。
  http://hi.baidu.com/zengzhaonong/item/52a529a5371a86248919d38b
 #+BEGIN_EXAMPLE
	cachep->colour_off = cache_line_size();
 #+END_EXAMPLE
- 原来cache->colour是这样计算的，就left_over直接除cache->colour_off.
 #+BEGIN_EXAMPLE
	cachep->colour = left_over/cachep->colour_off;
 #+END_EXAMPLE
- cache->slab_size就是slab描述符加上所有的object描述符，如果是在内部，那么还要加上最后一个
  对象的后面的对齐字节
 #+BEGIN_EXAMPLE
	slab_size = ALIGN(cachep->num*sizeof(kmem_bufctl_t)
				+ sizeof(struct slab), align);
 #+END_EXAMPLE
 #+BEGIN_EXAMPLE
	if (flags & CFLGS_OFF_SLAB) {
		/* really off slab. No need for manual alignment */
		slab_size = cachep->num*sizeof(kmem_bufctl_t)+sizeof(struct slab);
	}
 #+END_EXAMPLE
- cache->flags,ULK:flags Set of flags that describes permanent properties of the cache.
- cache->gfpflags,ULK:gfpflags Set of flags passed to the buddy system function when
  allocating page frames.在这个函数里只会设置GFP_DMA
 #+BEGIN_EXAMPLE
	if (flags & SLAB_CACHE_DMA)
		cachep->gfpflags |= GFP_DMA;
 #+END_EXAMPLE
- cache->slabp_cache,ULK:Pointer to the general slab cache containing the slab descriptors
  (NULL if internal slab descriptors are used;)
 #+BEGIN_EXAMPLE
	if (flags & CFLGS_OFF_SLAB)
		cachep->slabp_cache = kmem_find_general_cachep(slab_size,0);
 #+END_EXAMPLE
- 看到最后一个函数alloc_slabmgmt的最后一段注释，说效果甚微，这是为什么呢？这其实就是最终撤
  销slab着色的原因之一。我们看到如果slab的数目只有cachep->colour个的话，这个slab着色的效果
  就 太好了，但是这往往不太现实，slab的数目有时是相当大的，这样的话，slab着色实际上只是帮了
  一点点小忙而已，它仅仅保证了最开始的几个slab不会map到同一个cpu cache line，但是待slab逐渐
  增加以后，后面的slab将还是会无情的打仗，从而造成cpu访问cache频频失效，这种能救几个算几个
  的思想可能对于人类救灾是有效的，毕竟生命高于一切(当然不包括三氯氰胺事件)，但是对于系统设
  计，这种效果的机制不如不要，因为我们用大量的代码维持了一个效果甚微的方案，这是不值得的，
  软件设计就是这样，每笔账都要算清，赔本的生意绝对不做，内核开发者的慧眼识别出了这个滥竽充
  数的所谓的巧妙算法，绝然地移除了它，在分配器从slab发展到slub以后，这个问题相对简单了许多，
  slub的思想就是简单，不要那么多花里胡哨的算法，就是简单，简单就是美，这确实是一句真理，冲
  突就冲突呗，只要我们带来的益处超过了冲突带来的麻烦，这就是值得的，鸵鸟算法在这种情况下就
  是有效的，确实是这样。hi.baidu.com/zengzhaonong/item/52a529a5371a86248919d38b


** static void cache_estimate(unsigned long gfporder, size_t size, size_t align, int flags, size_t *left_over, unsigned int *num)
*** mm/slab.c:
- 一开始cache->gfporder是0,因为cache是在这个函数分配的且分配之后马上清零.所以cal_wastage处
  的循环一开始的cache->gfporder是0的.
- http://oss.org.cn/kernel-book/ch06/6.3.3.htm 说明了着色区和补尝着色区是如何分布的,这篇文
  章对我理解着色的具体实现有很大的帮助。对于一个cache里的第一个slab,它是这样分布的：一开始
  一个cache->colour_off的大小(在alloc_slabmgmt()可以看出),紧接着是slab描述符，紧接着是一连串的
  object描述符，这些object描述符中间不用添加对齐字节，紧接着是因为要对齐cpu cache line所添
  加的字节，紧接着是cache->colour_off*cache->colour_next个字节，紧接着是一堆连续的object，
  但是这些object之间要添加上对齐字节。cache->colour_off是一个cpu cache line所占的字节数，对
  于第一个slab，cache->colour_next是0，第二个就是1，以此类推，cache->colour_next在
  cache_grow()里自增。
 #+BEGIN_EXAMPLE
	while (i*size + ALIGN(base+i*extra, align) <= wastage)
		i++;
 #+END_EXAMPLE
- cache_estimate:指定slab的大小后，返回slab中的对像个数以及剩余空间数
- 因为对象的描述符是放在slab描述符之后的,所以如果slab描述符放在内部,那么对象的结构体也是放
  在内部的.
 #+BEGIN_EXAMPLE
	if (!(flags & CFLGS_OFF_SLAB)) {
		base = sizeof(struct slab);
		extra = sizeof(kmem_bufctl_t);
	}
 #+END_EXAMPLE
- 

** static void *kmem_getpages(kmem_cache_t *cachep, unsigned int __nocast flags, int nodeid)
*** mm/slab.c:
- When the slab allocator creates a new slab, it relies on the zoned page frame allocator
  to obtain a group of free contiguous page frames. For this purpose, it invokes the
  kmem_getpages( ) function,所以这个函数是给
- 参数flags要加上cache->gfpflags的标志
- 用alloc_pages()分配页之后要用page_address()来获取虚拟地址。
- 如果设置了SLAB_RECLAIM_ACCOUNT，那么就增加slab_reclaim_pages,这个变量表示有多少个
  slab_pages页可以在页少时可以回收。
- page_state->nr_slab也要增加
- 还要修改page描述符的PG_slab。
** static void kmem_freepages(kmem_cache_t *cachep, void *addr)
*** mm/slab.c:
- 相对于kmem_getpages()来说，这个函数就是释放的。
- 因为通过virt_to_page()来获得页描述符，所以可以确定slab不会在高端内存。
- 如果current->reclaim_state不为空，表示current要回收页，所以要增加
  current->reclaim_state->reclaimed_slab.
- 这里会调用free_pages()释放页
- 若设了SLAB_RECLAIM_ACCOUNT,那么就要减slab_reclaim_pages

** static int cache_grow(kmem_cache_t *cachep, unsigned int __nocast flags, int nodeid)
*** mm/slab.c:
- the slab allocator assigns a new slab to the cache by invoking cache_grow( ) . 
- 这段代码计算了当前要分配的slab里的第一个对象要比第一个slab里的第一个对象的位置偏移多少.
 #+BEGIN_EXAMPLE
    offset = cachep->colour_next;
	cachep->colour_next++;
	if (cachep->colour_next >= cachep->colour)
		cachep->colour_next = 0;
	offset *= cachep->colour_off;
 #+END_EXAMPLE


** static struct slab* alloc_slabmgmt(kmem_cache_t *cachep, void *objp, int colour_off, unsigned int __nocast local_flags)
*** mm/slab.c:
- 如果cache->dtor不为空就要对每个对象调用dtor(),
 #+BEGIN_EXAMPLE
	if (cachep->dtor) {
		int i;
		for (i = 0; i < cachep->num; i++) {
			void* objp = slabp->s_mem+cachep->objsize*i;
			(cachep->dtor)(objp, cachep, 0);
		}
 #+END_EXAMPLE
- 释放一个slab也可以用rcu来释放
 #+BEGIN_EXAMPLE
	if (unlikely(cachep->flags & SLAB_DESTROY_BY_RCU)) {
		struct slab_rcu *slab_rcu;

		slab_rcu = (struct slab_rcu *) slabp;
		slab_rcu->cachep = cachep;
		slab_rcu->addr = addr;
		call_rcu(&slab_rcu->head, kmem_rcu_free);
 #+END_EXAMPLE
** void *kmem_cache_alloc(kmem_cache_t *cachep, unsigned int __nocast flags)
*** mm/slab.c:
- 调用__cache_alloc()
** static inline void *__cache_alloc(kmem_cache_t *cachep, unsigned int __nocast flags)
*** mm/slab.c:
- kmem_bufctl_t类型原来就是一个整型而已,因为它是存放一个地址的而已.
- cache->array的元素存放不同CPU的可用的空闲的对象

** static void *cache_alloc_refill(kmem_cache_t *cachep, unsigned int __nocast flags)
*** mm/slab.c:
- 若共享链表里有空闲的,那么就把空闲的对象移到相应的链表,而不是在共享链表里使用分配,就是说共
  享链表里的所有对象都是空闲的.拷贝过去的只是一个地址而已,也就是说在紧跟struct array_cache
  后面的是对象的地址,不是对象本身.
 #+BEGIN_EXAMPLE
			memcpy(ac_entry(ac), &ac_entry(shared_array)[shared_array->avail],
					sizeof(void*)*batchcount);
 #+END_EXAMPLE
- 无论如何,都可以通过以下的代码找到slab->list所指向的在slabs_partial或在slabs_free里的结点.
 #+BEGIN_EXAMPLE
		entry = l3->slabs_partial.next;
		if (entry == &l3->slabs_partial) {
			l3->free_touched = 1;
			entry = l3->slabs_free.next;
			if (entry == &l3->slabs_free)
				goto must_grow;
		}
		slabp = list_entry(entry, struct slab, list);
 #+END_EXAMPLE
- array_cache->avail的最大值是batchcount,说明紧跟array_cache后面的只有batchcount个对象地址.
- slab描述符是怎样被链起来的呢?应该是通过slab->list,又因为slab->list是kmem_list3结构体里的
  slabs_full或slabs_partail或slabs_free链表里的一个结点,又因为kmem_cache_t->lists是
  kmem_list3结构体,这样子就可以把kmem_cache_t和所有的slab关联起来了.
** static inline void __cache_free(kmem_cache_t *cachep, void *objp)
*** mm/slab.c:
- 若array_cache->avail还没有到限制(array_cache->limit)的话就把对象放到array_cache结构体的后
  面可以了,array_cache->limit一定是不小于array_cache->batchcount的.这是不是说明array_cache
  结构体后面的对象在地址上不一定是连续的呢?是不是也有可能后面的对象不在同一个slab上?
- array_cache结构体后的对象因为在空时才分配batchcount个,没有超过array_cache->limit,而会因为
  释放过多而超过array_cache->limit.
- 虽然每个CPU都有自已的array_cache结构体,但是不能保证回收到某个CPU的array_cache的对象一定是
  从该个CPU的array_cache分配出去的.因为进程会被迁移到不同的CPU执行.
- 若array->avail大于array_cache->limit就要调用cache_flusharray()来释放batchcount个对象。
 #+BEGIN_EXAMPLE
		cache_flusharray(cachep, ac);
 #+END_EXAMPLE
- 若array->avail小于array_cache->limit，那么就要把对象放回array_cache结构体后面。
 #+BEGIN_EXAMPLE
		ac_entry(ac)[ac->avail++] = objp;
 #+END_EXAMPLE
** static void cache_flusharray(kmem_cache_t *cachep, struct array_cache *ac)
*** mm/slab.c:
- 如果cache->lists->shared不为空且还有空余的空间来存放新的对象，就一定会把对象放到共享队列
  里，直到填满整个共享队列。但是因为共享队列剩余的空间可以小于batchcount，且在这种情况下是
  不会处理多余的对象，那么就可能释放的对象数小于batchcount。所以调用free_block()的
  batchcount参数不一定是array_cache->batchcount。
- 释放array_cache结构体后面的对象是紧跟在结构体后面的，而不是离结构体最远的那个。所以要把
  batchount之后的所有对象移到紧跟结构体后面。
** static void free_block(kmem_cache_t *cachep, void **objpp, int nr_objects)
*** mm/slab.c:
- Remember that the lru.prev field of the descriptor of the slab page points to the
  corresponding slab descriptor.
 #+BEGIN_EXAMPLE
		slabp = GET_PAGE_SLAB(virt_to_page(objp));
 #+END_EXAMPLE
- 这个函数是对一个对象一个对象独立释放的,不是一起释放的,这也说明了array_cache结构体后面的对
  象的地址不一定是连续的,更有可能的是这些对象不一定在同一个slab里.
- 传给这个函数的objpp是array_cache结构体后面的地址,因为objpp是指针的指针,所以是指向结构体后
  面连续的对象.所以可以用循环来释放
 #+BEGIN_EXAMPLE
	for (i = 0; i < nr_objects; i++) {
 #+END_EXAMPLE
- 现在才知道slab结构体后面的对象描述符原来是这个作用的,和slab->free结合在一起就实现了一个空
  闲对象的链表。slab->free存放的是第一个空闲的对象索引，而slab结构体后面的对象描述符是存放
  对于该对象下一个空闲对象的索引。就是如果slab->free是1，那么索引为1的对象是空闲的，而下一
  个空闲对象是这样获取的：因为slab->free为1，所以下一个空闲对象的索引存放在紧跟在slab描述符
  后面的第1个对象描述符里，注意对象描述符是一个整型而已。
 #+BEGIN_EXAMPLE
		slab_bufctl(slabp)[objnr] = slabp->free;
		slabp->free = objnr;
 #+END_EXAMPLE
- 以下的代码找到对象的索引
 #+BEGIN_EXAMPLE
		objnr = (objp - slabp->s_mem) / cachep->objsize;
 #+END_EXAMPLE
- 释放一个对象slab->inuse就要自减
- 当某个slab里的所有对象为空闲时
 #+BEGIN_EXAMPLE
		if (slabp->inuse == 0) {
 #+END_EXAMPLE
  且整个cache的空闲对象超过了上限，就要destroy这个slab了
 #+BEGIN_EXAMPLE
			if (cachep->lists.free_objects > cachep->free_limit) {
 #+END_EXAMPLE
  释放时要把free_objects减去cache->num(一个slab的对象数)
 #+BEGIN_EXAMPLE
				cachep->lists.free_objects -= cachep->num;
 #+END_EXAMPLE
  若没有超过上限，就要把这个slab插到slabs_free链表里。
 #+BEGIN_EXAMPLE
				list_add(&slabp->list,
				&list3_data_ptr(cachep, objp)->slabs_free);
 #+END_EXAMPLE
- 若inuse不为0，那就直接插到slabs_partial里
 #+BEGIN_EXAMPLE
			list_add_tail(&slabp->list,
				&list3_data_ptr(cachep, objp)->slabs_partial);
 #+END_EXAMPLE
** void *__kmalloc(size_t size, unsigned int __nocast flags)
*** mm/slab.c:
- 这个函数是用来分配2^n大小的slab对象的。
- 被这个函数分配的对象大小为2^n的cache是在kmem_cache_init()建立的。这个cache就general
  cache.
- 这个函数先调用__find_general_cachep()来找出应该哪个cache来分配。再调用__cache_alloc()来分
  配对象。
** static inline kmem_cache_t *__find_general_cachep(size_t size, int gfpflags)
*** mm/slab.c:
- 所有的general cache都存放在malloc_sizes里。
- 通过对比大小找到合适cache。
 #+BEGIN_EXAMPLE
	while (size > csizep->cs_size)
		csizep++;
 #+END_EXAMPLE
- 因为一个大小的cache有分DMA和非DMA两种对象，所以要通过gfpflags参数来判断
 #+BEGIN_EXAMPLE
	if (unlikely(gfpflags & GFP_DMA))
		return csizep->cs_dmacachep;
	return csizep->cs_cachep;
 #+END_EXAMPLE
** void kfree(const void *objp)
*** mm/slab.c:
- 这个函数对应kmalloc()的
** 
- ULK:a memory pool allows a kernel componentsuch as the block device subsystemto allocate
  some dynamic memory to be used only in low-on-memory emergencies.
- reserved page frames是与memory pool不一样的，前者用于满足中断处理程序和临界区请求的原子分
  配而建立的，而后者为某个特定的内核部分而保留的动态内存，memory pool 一般不使用。
- 从memory pool里分配内存可能是要等待的,在mempool_alloc()里休眠。
- memory pool只是做为slab对象里的一个缓冲区，memory pool的元素有可能是特殊slab里的对象，有
  可能是general cache里的对象。当是特殊slab时，那么mempool_t->alloc是
  mempool_alloc_slab(),mempool_t->free是mempool_free_slab(),里面对分别调用
  kmem_cache_alloc()和kmem_cache_free(), 这时mempool_t->pool_data指向的是cache 描述符。
** mempool_t * mempool_create(int min_nr, mempool_alloc_t *alloc_fn, mempool_free_t *free_fn, void *pool_data)
*** mm/mempool.c:
- mempool_t->min_nr是在创建时确定的，不是所有的mempool都是一样的。
- mempool_t结构体是用kmalloc()来分配的，没有用特殊的cache.
- mempool_t->elements原来是一个指针的指针，它指向的是一堆连续的指针
 #+BEGIN_EXAMPLE
	pool->elements = kmalloc(min_nr * sizeof(void *), GFP_KERNEL);
 #+END_EXAMPLE
- 所有在memory pool里的元素都是从参数pool_data那里来的。
 #+BEGIN_EXAMPLE
		element = pool->alloc(GFP_KERNEL, pool->pool_data);
 #+END_EXAMPLE
- 这个函数会创建mempool_t->min_nr个元素，对于mempool_t->min_nr，ULK是这样解释的：min_nr
  Maximum number of elements in the memory pool，注意是maximum
- 用一个类似array_cache结构体后面来存储对象地址的方式来实现mempool_t->element，那怎么正确的
  分配与释放的问题呢？
** static void add_element(mempool_t *pool, void *element)
*** mm/mempool.c:
- mempool_t->curr_nr就是内存池第一个空闲元素的位置。那是不是在被回收的元素就要是最近被分配
  的元素呢？原来理解错了mempool_t->curr_nr意思了，ULK:curr_nr Current number of elements
  in the memory pool.所以和array_cache->avail相似。
** void * mempool_alloc(mempool_t *pool, unsigned int __nocast gfp_mask)
*** mm/mempool.c:
- 要分配到内存池的元素，就要调用这个函数。
- 一开始分配元素时没有直接从内存池里分配的，所以一开始是调用mempoolt->alloc来分配的。
- 第一次调用mempool_t->alloc时是不想等待的，
 #+BEGIN_EXAMPLE
	gfp_temp = gfp_mask & ~(__GFP_WAIT|__GFP_IO);

repeat_alloc:

	element = pool->alloc(gfp_temp, pool->pool_data);
 #+END_EXAMPLE
- 每次休眠回来都是先从mempool_t->alloc里分配的，
 #+BEGIN_EXAMPLE
	prepare_to_wait(&pool->wait, &wait, TASK_UNINTERRUPTIBLE);
	smp_mb();
	if (!pool->curr_nr)
		io_schedule();
	finish_wait(&pool->wait, &wait);

	goto repeat_alloc;
 #+END_EXAMPLE
  且休眠回来之后调用mempool_t->alloc可能会阻塞
 #+BEGIN_EXAMPLE
	/* We must not sleep in the GFP_ATOMIC case */
	if (!(gfp_mask & __GFP_WAIT))
		return NULL;

	/* Now start performing page reclaim */
	gfp_temp = gfp_mask;
 #+END_EXAMPLE
- 若从mempool_t->alloc里分配不到元素时就会从内存池里分配
 #+BEGIN_EXAMPLE
	if (likely(pool->curr_nr)) {
		element = remove_element(pool);
		spin_unlock_irqrestore(&pool->lock, flags);
		return element;
	}
 #+END_EXAMPLE
** void mempool_free(void *element, mempool_t *pool)
*** mm/mempool.c:
- 若内存池的元素数没有超过限制就把它放回内存池，并唤醒等待队列
 #+BEGIN_EXAMPLE
		if (pool->curr_nr < pool->min_nr) {
			add_element(pool, element);
			spin_unlock_irqrestore(&pool->lock, flags);
			wake_up(&pool->wait);
			return;
        }
 #+END_EXAMPLE
  否则就释放这个元素
 #+BEGIN_EXAMPLE
	pool->free(element, pool->pool_data);
 #+END_EXAMPLE

- 非连续内存的使用场合：Linux uses noncontiguous memory areas in several ways for
  instance, to allocate data structures for active swap areas (see the section "Activating
  and Deactivating a Swap Area" in Chapter 17), to allocate space for a module (see
  Appendix B), or to allocate buffers to some I/O drivers. Furthermore, noncontiguous
  memory areas provide yet another way to make use of high memory page frames (see the
  later section "Allocating a Noncontiguous Memory Area").
- 原来非连续内存的线性地址是PAGE_OFFSET到0xffffffff这段地址的。这段地址还夹杂着固定映段和临
  时映射的线性地址。关于这段地址的解释8.3.1节开头有解释。


** void *__vmalloc_area(struct vm_struct *area, unsigned int __nocast gfp_mask, pgprot_t prot)
*** mm/vmalloc.c:
- 这个函数是一个页一个页地分配给整个连续的线性地址的
 #+BEGIN_EXAMPLE
		area->pages[i] = alloc_page(gfp_mask);
 #+END_EXAMPLE
- vm_struct->size的大小是包括安全区的，所以要减去一页的大小
 #+BEGIN_EXAMPLE
	nr_pages = (area->size - PAGE_SIZE) >> PAGE_SHIFT;
 #+END_EXAMPLE
  但是vm_struct->nr_pages是没有包括安全区的。
- vm_struct->pages存放的是保存(nr_pages * sizeof(struct page *))个页的地址，
  vm_struct->pages是一个指针的指针。vm_struct->size是4095的倍数。
 #+BEGIN_EXAMPLE
	nr_pages = (area->size - PAGE_SIZE) >> PAGE_SHIFT;
	array_size = (nr_pages * sizeof(struct page *));
	/* Please note that the recursion is strictly bounded. */
	if (array_size > PAGE_SIZE)
		pages = __vmalloc(array_size, gfp_mask, PAGE_KERNEL);
	else
		pages = kmalloc(array_size, (gfp_mask & ~__GFP_HIGHMEM));
	area->pages = pages;
 #+END_EXAMPLE
   从kmalloc()分配到的内存有可能在高端吗？但是有分DMA区和非DMA区的，这个区是用GFP_DMA来区分
   的。所以这个函数的大部分代码就是为vm_struct->pages分配空间，再分配页框并把页框对应的页描
   述符地址放到vm_struct->pages指向的空间里。
- 分配好页之后，就调用map_vm_area()来把vm_struct->addr和这些页框关联起来，就是要建立页表。
** int map_vm_area(struct vm_struct *area, pgprot_t prot, struct page ***pages)
*** mm/vmalloc.c:
- 在这个函数里处理的是pgd页表一级的，通过调用vmap_pud_range()来处理pud一级的页表。
- 因为整个被映射的虚拟地址有可能超过一个pgd页表项所能表示的最大范围，所以要用循环来处理多个
  pgd页表项。
 #+BEGIN_EXAMPLE
	do {
		next = pgd_addr_end(addr, end);
		err = vmap_pud_range(pgd, addr, next, prot, pages);
		if (err)
			break;
	} while (pgd++, addr = next, addr != end);
 #+END_EXAMPLE
  任何一个页表里的项所映射的虚拟地址一定是连续的.就是说如果两个页表项是连续的,那么它们所表
  示的虚拟地址就是连续的.
** static inline int vmap_pud_range(pgd_t *pgd, unsigned long addr, unsigned long end, pgprot_t prot, struct page ***pages)
*** mm/vmalloc.c:
- 这个函数处理的是pud一级的页表.它调用vmap_pmd_range()来处理pmd一级的页表.
** static inline int vmap_pmd_range(pud_t *pud, unsigned long addr, unsigned long end, pgprot_t prot, struct page ***pages)
*** mm/vmalloc.c:
- 这个函数处理的是pmd一级的页表,这调用vmaap_pte_range()处理pte一级的页表.
** static int vmap_pte_range(pmd_t *pmd, unsigned long addr, unsigned long end, pgprot_t prot, struct page ***pages)
*** mm/vmalloc.c:
- 这个函数处理的是pte一级的页表,用set_pte_at()来填充pte页表项.
- 比较关键的一点就是如何处理pages这个参数了.
** void *vmap(struct page **pages, unsigned int count,unsigned long flags, pgprot_t prot)
*** mm/vmalloc.c:
- 这个函数就是先调用get_vm_area(),再调用map_vm_area().而vmalloc()调用__vmalloc(),转而调用
  get_vm_area()到这里与vmap()还是一样的,但是转而调用的__vmalloc_area()不一样
  了,再__vmalloc_area()里在调用map_vm_area()之前多做了一些事情,就是设置了
  vm_struct->nr_pages和vm_struct->pages,还分配了页框并把页框对应的描述符地址放到了
  vm_struct->pages里.所以在调用vmap()之前要把在__vmalloc_area()函数里调用map_vm_area()之前
  所做的事情做了才可以调用vmap().
- 所以vmap()的适用场合是:很多个页已经分配好了,这些页可以连续也可以不连续,而且这些的页的页描
  述地址是已放在参数pages里了,调用这个函数的目的就是把这些页映射到一段连续的线性地址去.
- 但要注意无论是vmap()还是vmalloc()都不会把vm_struct这个结构体返回给调用者,这个结构体只是内
  部使用,返回给调用者的是vm_struct->addr这个线性地址.在解除映射时也只接收虚拟地址,而不接收
  vm_strcut这个结构体,但是不是用container_of()这个宏来用vm_structt->addr找到vm_struct这个结
  构体的,而是用扫描vmlist这个链表来找到vm_struct的,在__remove_vm_area()里有这段代码
 #+BEGIN_EXAMPLE
	for (p = &vmlist ; (tmp = *p) != NULL ;p = &tmp->next) {
		 if (tmp->addr == addr)
			 goto found;
	}
 #+END_EXAMPLE
** void vfree(void *addr)
*** mm/vmalloc.c:
- 就是直接调用__vunmap(),但是第二个参数是以1调用的,
** void vunmap(void *addr)
*** mm/vmalloc.c:
- 也就是直接调用__vumap(),但是第二个参数是以0调用的,这个参数的作用是在__vumap()里体现的.
** void __vunmap(void *addr, int deallocate_pages)
*** mm/vmalloc.c:
- 无论是又于vfree()还是vunmap(),都要调用remove_vm_area()把虚拟地址对应的页表项给清掉.
- 接下来的工作是把页框给释放掉,但这个对于vunmap()是不用做的这个工作的,因为调用
  vmap()时,vmap()所使用的页框不是在vmap()内部分配的,是由外部分配后传给vmap()的.


- All memory descriptors are stored in a doubly linked list. Each descriptor stores the
  address of the adjacent list items in the mmlist field. The first element of the list is
  the mmlist field of init_mm,
- The mm_users field stores the number of lightweight processes that share the mm_struct
  data structure
- The mm_count field is the main usage counter of the memory descripto
- We'll try to explain the difference between the use of mm_users and mm_count with an
  example.Consider a memory descriptor shared by two lightweight processes. Normally, its
  mm_users field stores the value 2, while its mm_count field stores the value 1 (both
  owner processes count as one).If the memory descriptor is temporarily lent to a kernel
  thread (see the next section), the kernel increases the mm_count field. In this way,
  even if both lightweight processes die and the mm_users field becomes zero, the memory
  descriptor is not released until the kernel thread finishes using it because the
  mm_count field remains greater than zero.
- task_struct->mm_struct->mmap把该进程所有的内存区描述符vm_area_struct给链起来了.
- Figure 9-1说明是添加一个内存区时的两种不同插入方式(是否有相同的杈限),删除一个内存区时的两
  种不同的情况(在中间删除或在边上删除).
- We have already discussed two kinds of flags associated with a page: A few flags such as
  Read/Write, Present, or User/Supervisor stored in each Page Table entry.A set of flags
  stored in the flags field of each page descriptor.We now introduce a third kind of flag:
  those associated with the pages of a memory region.
- The initial values of the Page Table flags (which must be the same for all pages in the
  memory region, as we have seen) are stored in the vm_ page_ prot field of the
  vm_area_struct descriptor. When adding a page, the kernel sets the flags in the
  corresponding Page Table entry according to the value of the vm_ page_ prot field.
- When a User Mode process asks for dynamic memory, it doesn't get additional page frames;
  instead, it gets the right to use a new range of linear addresses, which become part of
  its address space. This interval is called a "memory region."

** struct vm_area_struct * find_vma(struct mm_struct * mm, unsigned long addr)
*** mm/mmap.c:
- 这个函数只是通过地址addr找合适的vm_area_struct而已,找不到也不会分配.
- 首先是看mm_struct->mmap_cache是不是要找的,因为这个是最近访问的.若不是就要搜索红黑树了.
- 这个函断要找的vm_area_struct是vm_end大于addr就可以了，与vm_start无关。
** static inline struct vm_area_struct * find_vma_intersection(struct mm_struct * mm, unsigned long start_addr, unsigned long end_addr)
*** include/linux/mm.h:
- 这个函数简单，就是直接用start_addr调用find_vma()来找到包含start_addr地址的区间，再看
  end_addr是不是在个区间内，若是就是找到区间，否则就是没有这样的区间。
** unsigned long arch_get_unmapped_area(struct file *filp, unsigned long addr, unsigned long len, unsigned long pgoff, unsigned long flags)
*** mm/mmap.c:
- 一些架构有自已的实现，如ARM。
- 这个函数是给用户进程分配用户态空间的区域的。
- 这个函数是从0x40000000的线性地址开始往上增加来分配的，与arch_get_unmapped_area_topdown()相反。
- mm_struct->mmap_cache是给find_vma()找区间用的，mm_struct->free_area_cache是给
  arch_get_unmapped_area()用的
- 先看addr到addr+len这一段是否是空的区间，
 #+BEGIN_EXAMPLE
	if (addr) {
		addr = PAGE_ALIGN(addr);
		vma = find_vma(mm, addr);
		if (TASK_SIZE - len >= addr &&
		    (!vma || addr + len <= vma->vm_start))
			return addr;
	}
 #+END_EXAMPLE
  不为0表示尽量在addr处开始获得空间，先是用find_vma()找一个vm_end比addr大的vm_area_struct,
  若找到且addr到addr+len这个区间内的地址都不在vm_area_struct里的话就从addr到addr+len这个区
  间里有空间可以使用了。
- 如果上一步没有找到或addr为0就要搜索整个进程的线性地址了。注意是从mm->free_area_cache这个
  vm_area_struct区间开始找的
 #+BEGIN_EXAMPLE
	start_addr = addr = mm->free_area_cache;

full_search:
	for (vma = find_vma(mm, addr); ; vma = vma->vm_next) {
 #+END_EXAMPLE
- 跳到下一个vm_area_struct的操作就是利用vm_area_struct->vm_next,这种方式的话能保证地址是连
  续搜索的，就是红黑树维护了这一点。
- 有了一个vm_area_struct，那么开始地址就是从vm_area_struct->vm_end开始的
 #+BEGIN_EXAMPLE
		addr = vma->vm_end;
 #+END_EXAMPLE
- 判断一个区间是否有存在就是用后一个vm_area_struct->start_addr的地址减去关前一个
  vm_area_struct->end_addr是否大于addr+len,若是，就表明存在区间了。
 #+BEGIN_EXAMPLE
		if (!vma || addr + len <= vma->vm_start) {
			/*
			 * Remember the place where we stopped the search:
			 */
			mm->free_area_cache = addr + len;
			return addr;
		}
 #+END_EXAMPLE
** int insert_vm_struct(struct mm_struct * mm, struct vm_area_struct * vma)
*** mm/mmap.c:
- vm_area_struct->vm_pgoff，ULK：Offset in mapped file (see Chapter 16). For anonymous
  pages, it is either zero or equal to vm_start/PAGE_SIZE (see Chapter 17).为什么要这样做呢？
  有一段这样的注释：The vm_pgoff of a purely anonymous vma should be irrelevant until its
  first write fault, when page's anon_vma and index are set.  But now set the vm_pgoff it
  will almost certainly end up with (unless mremap moves it elsewhere before that first
  wfault), so /proc/pid/maps tells a consistent story. By setting it to reflect the
  virtual start address of the vma, merges and splits can happen in a seamless way, just
  using the existing file pgoff checks and manipulations. Similarly in do_mmap_pgoff and
  in do_brk.
- 
** static void vma_link(struct mm_struct *mm, struct vm_area_struct *vma, struct vm_area_struct *prev, struct rb_node **rb_link, struct rb_node *rb_parent)
*** mm/mmap.c:
- 为什么要把vm_area_struct->vm_truncate_count设成
  vm_area_struct->vm_file->f_mapping->truncate_count呢？
- ULK:vm_truncate_count Used when releasing a linear address interval in a non-linear file
  memory mapping.
** static inline unsigned long do_mmap(struct file *file, unsigned long addr,unsigned long len, unsigned long prot,	unsigned long flag, unsigned long offset)
*** include/linux/mm.h:
- 主要调用了do_mmap_pgoff()
- 这个函数只是分配一个页大小的虚拟地址
** unsigned long do_mmap_pgoff(struct file * file, unsigned long addr, unsigned long len, unsigned long prot,unsigned long flags, unsigned long pgoff)
*** mm/mmap.c:
- ULK:the do_mmap( ) function creates and initializes a new memory region for the current
  process. However, after a successful allocation, the memory region could be merged with
  other memory regions defined for the process
- 对于匿名区,主要的工作有:分配vm_area_struct,初始化vm_area_struct里的成员(特别是vm_flags占
  了很多代码),把是匿名区就链接到vma链表(vma_link()),若页被锁就分配页
- 若file->f_op不为空表示文件操作函数不为空，但是file->f_op->mmap为空就会返回ENODEV
 #+BEGIN_EXAMPLE
		if (!file->f_op || !file->f_op->mmap)
			return -ENODEV;
 #+END_EXAMPLE 
- 通过调用get_unmapped_area()来获取空闲的区间,若不能获取就返回.
 #+BEGIN_EXAMPLE
	addr = get_unmapped_area(file, addr, len, pgoff, flags);
	if (addr & ~PAGE_MASK)
		return addr;
 #+END_EXAMPLE 
- mm->def_flags,VM_MAYREAD,VM_MAYWRITE, VM_MAYEXEC这几个标志一定是有的,
 #+BEGIN_EXAMPLE
	vm_flags = calc_vm_prot_bits(prot) | calc_vm_flag_bits(flags) |
			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
 #+END_EXAMPLE 
- ULK关于参数prot的说明：This parameter specifies the access rights of the pages included
  in the memory region.Possible flags are PROT_READ, PROT_WRITE, PROT_EXEC, and
  PROT_NONE. The first three flags mean the same things as the VM_READ, VM_WRITE , and
  VM_EXEC flags. PROT_NONE indicates that the process has none of those access rights.
- 关于flags参数有MAP_GROWSDOWN, MAP_LOCKED, MAP_DENYWRITE, and MAP_EXECUTABLE是与VM_有对应
  的，还有一些不是与VM_对应的，所以对应的那些要转为VM_
 #+BEGIN_EXAMPLE
	vm_flags = calc_vm_prot_bits(prot) | calc_vm_flag_bits(flags) |
			mm->def_flags | VM_MAYREAD | VM_MAYWRITE | VM_MAYEXEC;
  #+END_EXAMPLE 
  但MAP_LOCKED还是要特殊处理一下的.先是允许mlock操作才会设置VM_LOCKED,VM_LOCKED是不是最终会
  影响PG_locked呢?
 #+BEGIN_EXAMPLE
	if (flags & MAP_LOCKED) {
		if (!can_do_mlock())
			return -EPERM;
		vm_flags |= VM_LOCKED;
	}
 #+END_EXAMPLE 
  接着还要看有没有超限

 #+BEGIN_EXAMPLE
	if (vm_flags & VM_LOCKED) {
		unsigned long locked, lock_limit;
		locked = len >> PAGE_SHIFT;
		locked += mm->locked_vm;
		lock_limit = current->signal->rlim[RLIMIT_MEMLOCK].rlim_cur;
		lock_limit >>= PAGE_SHIFT;
		if (locked > lock_limit && !capable(CAP_IPC_LOCK))
			return -EAGAIN;
	}
 #+END_EXAMPLE 
- MAP_SHARED:The former flag specifies that the pages in the memory region can be shared
  among several processes; the latter flag has the opposite effect.
- 对于MAP_SHARED和MAP_PRIVATE也要做处理,对于file是否为空会做出不同的处理:
 #+BEGIN_EXAMPLE
	if (file) {
		switch (flags & MAP_TYPE) {
		case MAP_SHARED:
			if ((prot&PROT_WRITE) && !(file->f_mode&FMODE_WRITE))
				return -EACCES;

			/*
			 * Make sure we don't allow writing to an append-only
			 * file..
			 */
			if (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))
				return -EACCES;

			/*
			 * Make sure there are no mandatory locks on the file.
			 */
			if (locks_verify_locked(inode))
				return -EAGAIN;

			vm_flags |= VM_SHARED | VM_MAYSHARE;
			if (!(file->f_mode & FMODE_WRITE))
				vm_flags &= ~(VM_MAYWRITE | VM_SHARED);

			/* fall through */
		case MAP_PRIVATE:
			if (!(file->f_mode & FMODE_READ))
				return -EACCES;
			break;

		default:
			return -EINVAL;
		}
	} else {
		switch (flags & MAP_TYPE) {
		case MAP_SHARED:
			vm_flags |= VM_SHARED | VM_MAYSHARE;
			break;
		case MAP_PRIVATE:
			/*
			 * Set pgoff according to addr for anon_vma.
			 */
			pgoff = addr >> PAGE_SHIFT;
			break;
		default:
			return -EINVAL;
		}
	}
 #+END_EXAMPLE 
- 若是映射文件的,在MAP_SHARED情况下,设了PROT_WRITE但file->f_mode没有设FMODE_WRITE是错误的.不
  允许向一个只能append的文件设置FMODE_WRITE.该file不能有强制锁.
 #+BEGIN_EXAMPLE
			if ((prot&PROT_WRITE) && !(file->f_mode&FMODE_WRITE))
				return -EACCES;

			/*
			 * Make sure we don't allow writing to an append-only
			 * file..
			 */
			if (IS_APPEND(inode) && (file->f_mode & FMODE_WRITE))
				return -EACCES;

			/*
			 * Make sure there are no mandatory locks on the file.
			 */
			if (locks_verify_locked(inode))
				return -EAGAIN;
 #+END_EXAMPLE 
- 若file不为空,且设置了MAP_SHARED,那么就一定会设置VM_MAYSHARE,且如果设了FMODE_WRITE,那么就
  设置VM_SHARED,否则就不能设置VM_SHARED,还要把VM_MAYWRITE清掉.
 #+BEGIN_EXAMPLE
			vm_flags |= VM_SHARED | VM_MAYSHARE;
			if (!(file->f_mode & FMODE_WRITE))
				vm_flags &= ~(VM_MAYWRITE | VM_SHARED);
 #+END_EXAMPLE 
- 对file为空时处理MAP_SHARED就简单一点了,就是把VM_SHARED和VM_MAYSHARE给设置了,这个
  VM_MAYSHARE有什么用呢?如果是MAP_PRIVATE的做法有点不理解
 #+BEGIN_EXAMPLE
			/*
			 * Set pgoff according to addr for anon_vma.
			 */
			pgoff = addr >> PAGE_SHIFT;
 #+END_EXAMPLE 
- 不用find_vma_intersection()函数的原因是除了找到一个空区间,还想找出这个空区间的前一个
  vm_area_struct,所以没有用find_vma_intersection()因为find_vma_intersection()不能找出前一
  个_vm_area_struct.
 #+BEGIN_EXAMPLE
munmap_back:
	vma = find_vma_prepare(mm, addr, &prev, &rb_link, &rb_parent);
	if (vma && vma->vm_start < addr + len) {
		if (do_munmap(mm, addr, len))
			return -ENOMEM;
		goto munmap_back;
	}
 #+END_EXAMPLE 
- 对于为什么要做下的检查,ULK:Notice that the check is done here and not in step 1 with the
  other checks, because some memory regions could have been removed in step 4.
 #+BEGIN_EXAMPLE
	if (!may_expand_vm(mm, len >> PAGE_SHIFT))
		return -ENOMEM;
 #+END_EXAMPLE 
- ULK:VM_ACCOUNT Check whether there is enough free memory for the mapping when creating
  an IPC shared memory region (see Chapter 19). ULK:MAP_NORESERVE The function doesn't
  have to do a preliminary check on the number of free page
  frames.http://book.2cto.com/201302/16309.html : 下面介绍overcommit_memory的不同取值对应的
  不同虚拟空间分配。  OVERCOMMIT_GUESS overcommit_memory的默认值为OVERCOMMIT_GUESS。指定这
  个参数时，预测将空闲内存、页面缓存量、空闲交换区量、可回收slab（长字节）量等回收的页面数，
  虚拟空间要求分配的量比这个数小时，分配成功。  （请注意，在多个进程同时要求大量的虚拟空间
  时是无法正确预测的。下面所述的OVERCOMMIT_NEVER中就没有这种问题。） 在OVERCOMMIT_GUESS的情
  况下，可分配的虚拟空间大小基本就是物理内存大小和交换区大小的合计值。物理内存为2GB，交换区
  为2GB，当前消耗1GB时，还可以分配约3GB的虚拟空间。  OVERCOMMIT_ALWAYS 在OVERCOMMIT_ALWAYS
  的情况下，虚拟空间分配总是成功。即使对于过大的虚拟空间要求，也会分配虚拟空间。可以在与实
  际安装的物理内存量完全无关的形态下使用虚拟空间，如前面所述的散列表等。  OVERCOMMIT_NEVER
  在OVERCOMMIT_NEVER的情况下，对可分配虚拟空间量的管理更加严格。  首先，记录下整个系统内已
  分配的虚拟空间量。这个值严格由系统进行集中管理，在分配或释放虚拟空间时重新计算。这个值为
  /proc/meminfo的Committed_AS。  对于虚拟空间大小的计算也比其他参数严格。例如，在
  OVERCOMMIT_GUESS的情况下，对mmap系统调用设置了MAP_NORESERVE的虚拟空间量不添加到
  Committed_AS中。但是，在OVERCOMMIT_NEVER的情况下会添加到Committed_AS中。指定了
  MAP_NORESERVE的区域也作为可能分配物理内存的虚拟空间处理。
 #+BEGIN_EXAMPLE
	if (accountable && (!(flags & MAP_NORESERVE) ||
			    sysctl_overcommit_memory == OVERCOMMIT_NEVER)) {
		if (vm_flags & VM_SHARED) {
			/* Check memory availability in shmem_file_setup? */
			vm_flags |= VM_ACCOUNT;
		} else if (vm_flags & VM_WRITE) {
			/*
			 * Private writable mapping: check memory availability
			 */
			charged = len >> PAGE_SHIFT;
			if (security_vm_enough_memory(charged))
				return -ENOMEM;
			vm_flags |= VM_ACCOUNT;
		}
	}
 #+END_EXAMPLE 
 这个段代码最终影响的就是VM_ACCOUNT标志.在要求对页要做一个初步的检查或设置OVERCOMMIT_NEVER
 就作处理,在这种情况下若设置了VM_SHARED就设VM_ACCOUNT,表里作于IPC的内存时要检查是否有足够的
 内存,若是私有的可写的内存,就要用security_vm_enough_memory()检查,通过检查就设置
 VM_ACCOUNT.所以只有共享时或私有可写时才有可能设置VM_ACCOUNT.
- ULK:If the new interval is private (VM_SHARED not set) and it does not map a file on
  disk, it invokes vma_merge( ) to check whether the preceding memory region can be
  expanded in such a way to include the new interval.
 #+BEGIN_EXAMPLE
	/*
	 * Can we just expand an old private anonymous mapping?
	 * The VM_SHARED test is necessary because shmem_zero_setup
	 * will create the file object for a shared anonymous map below.
	 */
	if (!file && !(vm_flags & VM_SHARED) &&
	    vma_merge(mm, prev, addr, addr + len, vm_flags,
					NULL, NULL, pgoff, NULL))
		goto out;
 #+END_EXAMPLE 
  这面这段代码的注释也说了,检查VM_SHARED的必要的,因为shmem_zero_setup会为一个共享的匿名区创
  建一个文件对象,且因为映射文件的内存区是不可以被合并的.vma_merge()函数首先会检查能不能合并.
  
  到这里为止,之前的代码做的工作有:处理了vm_flags,找出了在有合适大小空闲内存区前的vm_area_struct,检查了合并.处理了pgoff
- ULK:If the MAP_SHARED flag is set (and the new memory region doesn't map a file on
  disk), the region is a shared anonymous region: invokes shmem_zero_setup( ) to
  initialize it. Shared anonymous regions are mainly used for interprocess communications;
  see the section "IPC Shared Memory" in Chapter 19.
 #+BEGIN_EXAMPLE
	} else if (vm_flags & VM_SHARED) {
		error = shmem_zero_setup(vma);
		if (error)
			goto free_vma;
	}
 #+END_EXAMPLE 
- 不知道这段代码是干什么的
 #+BEGIN_EXAMPLE
	/* We set VM_ACCOUNT in a shared mapping's vm_flags, to inform
	 * shmem_zero_setup (perhaps called through /dev/zero's ->mmap)
	 * that memory reservation must be checked; but that reservation
	 * belongs to shared memory object, not to vma: so now clear it.
	 */
	if ((vm_flags & (VM_SHARED|VM_ACCOUNT)) == (VM_SHARED|VM_ACCOUNT))
		vma->vm_flags &= ~VM_ACCOUNT;
 #+END_EXAMPLE 
- 下面一段代码也不知道干什么的
 #+BEGIN_EXAMPLE
	/* Can addr have changed??
	 *
	 * Answer: Yes, several device drivers can do it in their
	 *         f_op->mmap method. -DaveM
	 */
	addr = vma->vm_start;
	pgoff = vma->vm_pgoff;
	vm_flags = vma->vm_flags;

	if (!file || !vma_merge(mm, prev, addr, vma->vm_end,
			vma->vm_flags, NULL, file, pgoff, vma_policy(vma))) {
		file = vma->vm_file;
		vma_link(mm, vma, prev, rb_link, rb_parent);
		if (correct_wcount)
			atomic_inc(&inode->i_writecount);
	} else {
		if (file) {
			if (correct_wcount)
				atomic_inc(&inode->i_writecount);
			fput(file);
		}
		mpol_free(vma_policy(vma));
		kmem_cache_free(vm_area_cachep, vma);
	}
 #+END_EXAMPLE 
- 调用__vm_stat_account()
** void __vm_stat_account(struct mm_struct *mm, unsigned long flags, struct file *file, long pages)
*** mm/mmap.c:
- 若file不为空,就说明这个vma是映射到文件的,对于映射到文件的页要加到mm->shared_mm里去.对于设
  置了VM_GROWUP或VM_GROWSDOWN的页要加到mm->stack_vm里去.对于设置了VM_RESERVED或VM_IO的页要
  加到mm->reserved_vm里去.而mm->locked_vm已经在do_mmap_pgoff()处理了,还在一个mm->exec_vm不
  知道在哪处理了.
- VM_RESERVED:The region is special (for instance, it maps the I/O address space of a
  device), so its pages must not be swapped out.说明这个页不能被交换出去
- VM_IO: The region maps the I/O address space of a device.一个设备的I/O地址映射是不能被交
  换出去的.
** int make_pages_present(unsigned long addr, unsigned long end)
*** mm/memory.c:
- 这个函数是在do_mmap_pgoff()里设置VM_LOCKED时调用的.
- 这个函数主要是调用了get_user_pages()
- 因为get_user_pages()可以实现把所需要的页都在内存里.若只想这样,而不想得到struct page * 结
  构,那么调用get_user_pages()时的第二个参数可以为NULL.
** int get_user_pages(struct task_struct *tsk, struct mm_struct *mm, unsigned long start, int len, int write, int force, struct page **pages, struct vm_area_struct **vmas)
*** mm/memory.c:
- ULK:The get_user_pages( ) function cycles through all starting linear addresses of the
  pages between addr and addr+len ; for each of them, it invokes follow_page( ) to check
  whether there is a mapping to a physical page in the current's Page Tables. If no such
  physical page exists, get_user_pages( ) invokes handle_mm_fault( ) , which, as we'll see
  in the section "Handling a Faulty Address Inside the Address Space," allocates one page
  frame and sets its Page Table entry according to the vm_flags field of the memory region
  descriptor.这个函数还可以把在用户页的内容拷贝到内核.pages ：存放获取的struct page的指针数
  组,vms ： 返回各个页对应的struct vm_area_struct，可以传入NULL表示不获取.start ：要获取其
  页面的起始虚拟地址，它是用户空间使用的一个地址.
- write表示什么意思呢?在从make_pages_present()的调用来看,若vm_area_struct->vm_flags设置了
  VM_WRITE,那么write参数就为1.
- 从以下的代码来看,write参数使得flags设置了VM_WRITE和VM_MAYWRITE或VM_READ和VM_MAYREAD.而
  force参数会把VM_MAY给去掉,这样做有什么用呢?
- get_page()里把page->private赋给参数指针是怎么回事呢？
- 为什么要花这么大的劲来映射一个高端内存页到内核之后，好像又不怎么用呢？应该没那么简单
 #+BEGIN_EXAMPLE
			if (pg > TASK_SIZE)
				pgd = pgd_offset_k(pg);
			else
				pgd = pgd_offset_gate(mm, pg);
			BUG_ON(pgd_none(*pgd));
			pud = pud_offset(pgd, pg);
			BUG_ON(pud_none(*pud));
			pmd = pmd_offset(pud, pg);
			BUG_ON(pmd_none(*pmd));
			pte = pte_offset_map(pmd, pg);
			BUG_ON(pte_none(*pte));
			if (pages) {
				pages[i] = pte_page(*pte);
				get_page(pages[i]);
			}
			pte_unmap(pte)
 #+END_EXAMPLE 
  还是有一点不同的，因为pg这个地址是一个用户空间的地址，这样的话是不是就可以在内核态访问到
  用户态的空间了。一个pgd页表其实可以映射所有的4G内存，所以主内核页表也可以映射到用户态的地
  址，但是在映射用户态的空间时不能用pte_offset_kernel()(里面调用的pmd_offset_kernel()使用
  了__va())因为这个是映射内核空间用的，而pte_offset_map()是通过kmap_atomic()来映射用的,而
  kmap_atomic()返回的是一个pte页表的首地址,也就是说内核若想通过固定内核映射在主内核页表里映
  射一个用户态的空间，那么就要通过pte_offset_map(),在调用pte_offset_map()之前，要用
  pgd_offset(),pud_offset(),pmd_offset()建立不同级的页表，最后调用pte_offset_map().直接调用
  kmap_atomic()返回其实还只是一个pte页表的起始地址而已，pte_offset_map()还要通过一个
  pte_index()找到相应的下标来找出被映射地址的pte项.到这里可以得出，虽然内核的线性地址是物理
  地址加上PAGE_OFFSET,但这不能说明内核的线性地址不使用MMU，使用PAGE_OFFSET是因为内核有时需
  要通过线性地址知道相应的物理地址。注意pte_offset_map()和pte_offset_kernel()实现的区别，前
  者是对参数dir调用pmd_page()返回pte页表地址后再调用kmap_atomic(),再结合pte_index(),后者是
  对参数dir调用pmd_page_kernel()得到pte页表地址再结合pte_index(),所以可以看出kmap_atomic()
  通过了一次转换，但是转换后返回到东西还是一个意思，就是返回的还是pte页表的地址。
  kmap_atomic接收的page的参数是一个pmd页表项所指的包含pte页表的页框的struct page指针。再结
  合kmap_atomic()来重新理解kmap_atomic(),FIX_KMAP_BEGIN到FIX_KMAP_END =
  FIX_KMAP_BEGIN+(KM_TYPE_NR*NR_CPUS)-1这之间没有指定那些模块的地址，只是有一个下标而已，下
  标就在enum km_type里，要想通过这个下标找出该下标所使用的内核虚拟地址，就要通
  过__fix_to_virt()和FIXADDR_TOP这个最顶端的虚拟地址和下标来找，__fix_to_virt()返回的是一个
  页对齐的内核线性地址，且在kmap_atomic()里返回的就是__fix_to_virt()所返回的地址，由些可见
  kmap_atomic()返回的是一个页大小可用的内核线性地址，kmap_atomic()返回的这个页大小的线性地
  址是做什么用的呢？FIXADDR_TOP这个虚拟地址是与enum fixed_addresses里所有的元素相关的，不是
  单与FIX_KMAP_BEGIN相关，其实enum km_type里的所有成员都是可以插到FIX_KMAP_BEGIN和
  FIX_KMAP_END之间的，这个从kmap_atomic()里调用__fix_to_virt()时使用的是FIX_KMAP_BEGIN +
  idx可以看出.所以pte_offset_map()会把一个高端地址的所在的pte页表通过KM_PTE0窗口来映射。要
  注意若地址是高端地址，那么在get_user_pages()里用的pgd是当前进程的pgd,而不是主内核页表,(这
  个说法是错的，用的是主内核页表的，好像也不对，其实用哪个都无所谓，因为所有的pgd的第4G都是
  一样的映射的,应该是用户态的pgd,因为ULK也说过系统初始化完之后就主内核页表是没有使用的，只
  是用来拷贝第4G的页表而已,又有新发现，因为在get_user_pages()函数里有一个通过判断传入的虚拟
  地址来确定是使用主内核页表还是用户态的内核页表)所以在get_user_pages()里通过
  pud_offset(),pmd_offset()这些获得的页表都在都在用户态空间，但是可以通过pte_offset_map()把
  在用户态空间的pmd页表项所指向的整个pte页表映射到KM_PTE0所对应的pte页表项里去再通过
  pte_index()在这个被映射的整个pte页表里找出相应的页表项。所以调用pte_offset_map()之后，对
  于一个用户态空间的pte页表是被两个不同pmd页表项所映射，一个小于PAGE_OFFSET的线性地址的，另
  一个是在PTE0所指向的pte页表，在内核这一边就是通过高端地址固定映射来访问，所以有两个不同线
  性地址映射，且这两个地址互不干扰。所以调用pte_offset_map()得到的pte页表项与之前调用的
  pud_offset(),pmd_offset()得到的页表项没有关联。pmd_offset()里也会使用PAGE_OFFSET,是不是说
  用户态虚拟地址的pmd页表在内核的呢？难道是因为pte页表都在用户态空间所以要用
  pte_offset_map()才可以访问？除了pte_offset_map()就只有pte_offset_kernel()了，而
  pte_offset_kernel()里使用了PAGE_OFFSET，所以要用高端内存固定映射把用户态的pte页表映射成固
  定映射才可以找到相应的页表，接着才可以使用pte_page()找到相应的页描述符地址。
  http://bbs.chinaunix.net/thread-3691966-1-1.html 页表保存在内核态，由内核帮助进程来维护。
  到底pte_offset_map()是如何实现它的功能的呢？再说一次：dir参数要是一个pmd的页表项，且这个
  pmd页表项是映射用户态空间的，address这个地址也是用户态空间的，更重要的是address这个地址要
  在这个pmd页表项所映射的范围内，先用pmd_page(* (dir))获取dir这个pmd页表项所指向的包含pte页
  表的页框的struct page,再调用kmap_atomic(),调用完之后就会把KM_PTE0所对应的pte页表项映射到
  了pmd_page()返回的那个页框上去了，就是说KM_PTE0这个pte页表项映射的那个页其实是用户态空间
  使用的一个pte页表，又因为kmap_atomic()返回的是KM_PTE0相应的pte所对应的4k大小(一页)地址范
  围的起始地址，所以可以把它转成(pte * )的类型，因为这个起始地址就是pte页表的起始地址，这样
  就可以再结合pte_index()找出address这个用户态的虚拟地址所对应的pte页表项。但是为什么对于访
  问用户态空间地址时要这样做呢？
- 获取的vm_area_struct有点奇怪，调用的get_get_vma()总是返回相同的值，那赋给vmas还有什么用呢？
- 第一个if()成立的时候是在或即在找不到也扩展不了vma且虚拟地址不在一个范围内。
- 这下面这一句时vma一定不为空了
 #+BEGIN_EXAMPLE
		if (is_vm_hugetlb_page(vma)) {
 #+END_EXAMPLE
- 如果follow_page()(下面有介绍)失败，且地址空间不为写且有些级的页表没有就把退出循环
 #+BEGIN_EXAMPLE
			while (!(map = follow_page(mm, start, lookup_write))) {
				/*
				 * Shortcut for anonymous pages. We don't want
				 * to force the creation of pages tables for
				 * insanly big anonymously mapped areas that
				 * nobody touched so far. This is important
				 * for doing a core dump for these mappings.
				 */
				if (!lookup_write &&
				    untouched_anonymous_page(mm,vma,start)) {
					map = ZERO_PAGE(start);
					break;
				}
 #+END_EXAMPLE 
 为什么要把map赋予0页呢? 若没有跳出循环,那么以下的情况就是地址对应的pgd,pud,pmd页表项都有
 了,但是可能是pte没有或read/write的权限不对.所以接下来就是调用handle_mm_fault()了.
- 不需要从硬盘拷数据而发生的缺页叫次缺页(tsk->min_flt),需要从硬盘拷数据而发生的缺页叫主缺页
  (tsk->maj_flt)
- get_page_map()这个函数很简单,若struct page *超过了最大值就返回NULL,否则就返回参数.若发现
  有页是不正确的就释放原来已处理好的页
 #+BEGIN_EXAMPLE
			if (pages) {
				pages[i] = get_page_map(map);
				if (!pages[i]) {
					spin_unlock(&mm->page_table_lock);
					while (i--)
						page_cache_release(pages[i]);
					i = -EFAULT;
					goto out;
 #+END_EXAMPLE 
- 在这个函数的第一个if成立时是没有start地址对应的vm_area_struct的,为什么会有这种情况出现呢?在
  这种情况下只是用pte_offset_map()来找到地址对应的页框的struct page,这种情况下不会产生缺页
  异常?
** int handle_mm_fault(struct mm_struct *mm, struct vm_area_struct * vma, unsigned long address, int write_access)
*** mm/memory.c:
- 这个函数先是用pgd_offset(),pud_offset(),pmd_offset(),pte_alloc_map()来找出相应的页表项,若
  没有就建立页表,可见页表是在存在内核里的.
- 被调用的pte_alloc_map是调用pte_alloc_one()来获取一个存放页表的页框的,从pte_alloc_one()实现来看
 #+BEGIN_EXAMPLE
#ifdef CONFIG_HIGHPTE
	pte = alloc_pages(GFP_KERNEL|__GFP_HIGHMEM|__GFP_REPEAT|__GFP_ZERO, 0);
#else
	pte = alloc_pages(GFP_KERNEL|__GFP_REPEAT|__GFP_ZERO, 0);
#endif
 #+END_EXAMPLE
 设置了CONFIG_HIGHPTE才会在就端内存里存放pte页表,不是有高端内存就一定会在高端内存里存放.因
 为可能放在高端内存,所以pte_alloc_map()会调用pte_offset_map()来获取这个地址所在的pte页表项
 的指针.在这个函数里使用了pte_offset_map()得到的pte要在handle_pte_fault()才unmap()掉.

** static inline int handle_pte_fault(struct mm_struct *mm,	struct vm_area_struct * vma, unsigned long address,	int write_access, pte_t *pte, pmd_t *pmd)
*** mm/memory.c:
- 这个函数处理请求调页和写时复制.
- ULK:An addressed page may not be present in main memory either because the page was
  never accessed by the process, or because the corresponding page frame has been
  reclaimed by the kernel.In both cases, the page fault handler must assign a new page
  frame to the process. How this page frame is initialized, however, depends on the kind
  of page and on whether the page was previously accessed by the process. In particular:
  
  1. Either the page was never accessed by the process and it does not map a disk file, or
     the page maps a disk file. The kernel can recognize these cases because the Page
     Table entry is filled with zerosi.e., the pte_none macro returns the value 1.

  2. The page belongs to a non-linear disk file mapping (see the section "Non-Linear
     Memory Mappings" in Chapter 16). The kernel can recognize this case, because the
     Present flag is cleared and the Dirty flag is seti.e., the pte_file macro returns the
     value 1.

  3. The page was already accessed by the process, but its content is temporarily saved on
     disk. The kernel can recognize this case because the Page Table entry is not filled
     with zeros, but the Present and Dirty flags are cleared.
 #+BEGIN_EXAMPLE
	entry = *pte;
	if (!pte_present(entry)) {
		/*
		 * If it truly wasn't present, we know that kswapd
		 * and the PTE updates will not touch it later. So
		 * drop the lock.
		 */
		if (pte_none(entry))
			return do_no_page(mm, vma, address, write_access, pte, pmd);
		if (pte_file(entry))
			return do_file_page(mm, vma, address, write_access, pte, pmd);
		return do_swap_page(mm, vma, address, pte, pmd, entry, write_access);
	}
 #+END_EXAMPLE
   pte_present()返回0时都包含那3种情况,执行do_no_page()是第1种情况,执行do_file_page()是第2
   种情况,执行do_swap_page()是第3种情况.
- 对于写时复制的情况就是调用do_wp_page(),是写的请求,但是pte项表示不能写
 #+BEGIN_EXAMPLE
	if (write_access) {
		if (!pte_write(entry))
			return do_wp_page(mm, vma, address, pte, pmd, entry);
 #+END_EXAMPLE 
- 还有其它的情况就是错误了,所以返回VM_FAULT_MINOR.
- 所以一共有5种情况,而这5种情况分别包含在5个return里.
- 这个函数可以看出,若页在内存里,且是写访问,且pte项表示能写,那么就有问题了,在这种情况下是不
  应该调用这个函数的.在这种情况下还是要把页表项给dirty的,还会把pte项给young了,又dirty但又
  young,那就是不合理的.但是在ULK的Figure 9-5里是没有体现这种情况的.
** static int do_no_page(struct mm_struct *mm, struct vm_area_struct *vma,unsigned long address, int write_access, pte_t *page_table, pmd_t *pmd)
*** mm/memory.c:
- 这个是有两种情况的,若没有是匿名区就调用do_anonymous_page(),否则就是映射到一个文件.
- 这是用了vm_area_struct->vm_page_prot, ulk:vm_page_prot Access permissions for the page
  frames of the region.
- 
** static int do_anonymous_page(struct mm_struct *mm, struct vm_area_struct *vma, pte_t *page_table, pmd_t *pmd, int write_access, unsigned long addr)
*** mm/memory.c:
- 要是写访问才会做事情
- anon_vma_prepare()的作用就是分配一个struct anon_vma结构体
- 调用alloc_zeroed_user_highpage()转而调用alloc_page_vma()转而直接调用alloc_pages()或独立实
  现一个函数来分配一个页框,分配时会使用__GFP_ZERO来初始化页为0
- 在ULK里说了为什么要先pte_unmap()再调用pte_offset_map(),因为alloc_zeroed_user_highpage()可
  能会休眠.
- mm->rss在这里増加的
- 以下的代码最终的作用是,得出一个pte页表项的值,权限是用vma->vm_page_prot,指向的页框是page,
  还把它dirty,还根据VM_WRITE设置write的权限。
 #+BEGIN_EXAMPLE
		entry = maybe_mkwrite(pte_mkdirty(mk_pte(page,
							 vma->vm_page_prot)),
				      vma);
 #+END_EXAMPLE 
- 设置了Page_Referenced
- 因为page_table是一个指向这个虚拟地址所映射的一个pte页表项的指针。所以调用
 #+BEGIN_EXAMPLE
	set_pte_at(mm, addr, page_table, entry);
 #+END_EXAMPLE 
  把entry赋给了page_table所指向的pte页表项.
- 返回VM_FAULT_MINOR表示成功。
- 读的时候是不分配页的。
** static int do_wp_page(struct mm_struct *mm, struct vm_area_struct * vma,	unsigned long address, pte_t *page_table, pmd_t *pmd, pte_t pte)
*** mm/memory.c:
- ULK：If only one process owns the page, Copy On Write does not apply, and the process
  should be free to write the page. Basically, the function reads the _count field of the
  page descriptor: if it is equal to 0 (a single owner), COW must not be done. Actually,
  the check is slightly more complicated, because the _count field is also increased when
  the page is inserted into the swap cache (see the section "The Swap Cache" in
  Chapter 17) and when the PG_private flag in the page descriptor is set. However, when
  COW is not to be done, the page frame is marked as writable, so that it does not cause
  further Page Fault exceptions when writes are attempted
- PG_locked这个标志的作用好像是把页临时锁定在内存不被交换出去的作用所以调用了TestSetPageLocked()
 #+BEGIN_EXAMPLE
	if (!TestSetPageLocked(old_page)) {
 #+END_EXAMPLE 
- 在handle_pte_fault()里调用的do_wp_page()所使用的参数中entry的值是pte所指向的pte页表项的值
- 如果页没有被锁且can_share_swap_page()为真，那么就把这个pte页表项的dirty,young,write标志给
  设置了
 #+BEGIN_EXAMPLE
	if (!TestSetPageLocked(old_page)) {
		int reuse = can_share_swap_page(old_page);
		unlock_page(old_page);
		if (reuse) {
			flush_cache_page(vma, address, pfn);
			entry = maybe_mkwrite(pte_mkyoung(pte_mkdirty(pte)),
					      vma);
			ptep_set_access_flags(vma, address, page_table, entry, 1);
			update_mmu_cache(vma, address, entry);
			lazy_mmu_prot_update(entry);
			pte_unmap(page_table);
			spin_unlock(&mm->page_table_lock);
			return VM_FAULT_MINOR;
		}
	}
 #+END_EXAMPLE
 如果执行上面的代码没有执行，那么就页不能共享了，且页被锁住了。所以就应该拷贝了。
- 为什么PG_reserved没有设置的要増加计数呢？是不是有设置了说明这个页不会被交换，所以就不
  用_count来计数了？ULK这样说:To avoid race conditions,get_page( ) is invoked to increase
  the usage counter of old_page before starting the copy operation
 #+BEGIN_EXAMPLE
	if (!PageReserved(old_page))
		page_cache_get(old_page);
 #+END_EXAMPLE 
- 先分配一个struct anon_vma 结构体再分配一个页框
- 分配页框时要判断是不是ZERO_PAGE页，是不是的区别是分配的页有没有用__GFP_ZERO来分，因为
  alloc_zeroed_user_highpage()就是调用alloc_page_vma()的，但是除了用GFP_HIGHUSER之外还
  用_GFP_ZERO，若不是ZERO_PAGE就要用copy_user_highpage()(下面有介绍)来拷贝页了。
- ULK:Because the allocation of a page frame can block the process, the function checks
  whether the Page Table entry has been modified since the beginning of the function (pte
  and *page_table do not have the same value). In this case, the new page frame is
  released, the usage counter of old_page is decreased (to undo the increment made
  previously), and the function terminates.
 #+BEGIN_EXAMPLE
	page_table = pte_offset_map(pmd, address);
	if (likely(pte_same(*page_table, pte))) {
		if (PageAnon(old_page))
			dec_mm_counter(mm, anon_rss);
		if (PageReserved(old_page))
			inc_mm_counter(mm, rss);
		else
			page_remove_rmap(old_page);
		flush_cache_page(vma, address, pfn);
		break_cow(vma, new_page, address, page_table);
		lru_cache_add_active(new_page);
		page_add_anon_rmap(new_page, vma, address);

		/* Free the old page.. */
		new_page = old_page;
	}
	pte_unmap(page_table);
	page_cache_release(new_page);
	page_cache_release(old_page);
 #+END_EXAMPLE 
  PageAnon()检查页是否为匿名页，低位为1时为匿名页.anao_rss:Number of page frames assigned
  to anonymous memory mappings.为什么PageAnon()为真就要减呢?为什么旧的页若是保留页就增加
  rss呢?若旧的页是保留页那么新的页也是保留的吗?新页的pte页表项是怎么被建立的呢?下面有
  page_remove_rmap()的介绍,就是说若旧页是保留的,那么page->_mapcount是不会变的,这是不是说
  page->_mapcount是对保留页无效的呢?要注意参数page_table是新页所对应的pte页表项,就是
  address所对应的pte页表项的指针,就是发生缺页异常的pte页表项的指针.所以要调用break_cow()(下
  页有介绍)来用新页重建page_table页表项.旧页是减了anon_rss,新页是在page_add_anon_rmap()加了
  anon_rss,但注意新页和旧页不是属于同一个进程的,所以vm_area_struct是不同的.
** static inline void break_cow(struct vm_area_struct * vma, struct page * new_page, unsigned long address, pte_t *page_table)
*** mm/memory.c:
- 重建的页表项是dirty的.写不写要看vm_area_struct
** void page_add_anon_rmap(struct page *page, struct vm_area_struct *vma, unsigned long address)
*** mm/rmap.c:
- 为该匿名页插入反向映射数据结构的内容.
- 增加了anon_rss
- 为什么要把vm_area_struct->anon_vma转成(void*)类型之后再加1呢?
** void page_remove_rmap(struct page *page)
*** mm/rmap.c:
- The _mapcount field stores the number of Page Table entries that refer to the page
  frame. The counter starts from -1: this value means that no Page Table entry references
  the page frame. Thus, if the counter is zero, the page is non-shared, while if it is
  greater than zero the page is shared.
- 这个函数就只是把page->_mapcount减1而已.
** #define page_cache_release(page)	put_page(page)
*** include/linux/pagemap.h:
** void put_page(struct page *page)
*** mm/swap.c:
- 若页不是保留的且页没有引用就调用__page_cache_release()
** void fastcall __page_cache_release(struct page *page)
*** mm/swap.c:
- 若页的计数不为空,那么直接把page设为NULL就是释放了.
- 若页的计数为空,那么就调用free_hot_page()来释放页.
** static inline void copy_user_highpage(struct page *to, struct page *from, unsigned long vaddr)
*** include/linux/highmem.h:
- 在内核态下从一个用户态的页框的数据拷贝到另一个用户态的页框去。
- 是使用内核的高端内存固定映射来拷贝的。使用了KM_USER0和KM_USER1这两个窗口。
- 调用了copy_user_page()，转而调用了copy_page()转而调用mmx_copy_page()或memcpy().
** struct page *follow_page(struct mm_struct *mm, unsigned long address, int write)
*** mm/memory.c:
- 这个函数调用
 #+BEGIN_EXAMPLE
	return __follow_page(mm, address, /*read*/0, write);
 #+END_EXAMPLE 
** static struct page *__follow_page(struct mm_struct *mm, unsigned long address, int read, int write)
*** mm/memory.c:
- follow_page函数是从进程的页表中搜索特定地址对应的页面对象。就是得到用户态虚拟地址
  (0-3G)address所在的页框的struct page指针，若一些权限不符合就不能获取，如要present,要
  write权限要与参数write对应，read也要对应,就这三个权限。
- 若已成功获取struct page *了，那么就要结合参数write和页表项和struct page标志来设置dirty
 #+BEGIN_EXAMPLE
			if (write && !pte_dirty(pte) && !PageDirty(page))
				set_page_dirty(page);
 #+END_EXAMPLE 
  为什么要pte_dirty()和PageDirty()同时假才可以呢？

  还要修改access位
- 所以这个函数不仅仅是获取struct page而己，还修改了东西的。
- 这个函数在某些级的页表没有对应的页表项或是read/write权限不对就会返回NULL
- 这个函数的开头与get_user_pages()第一个if一样是调用
  pgd_offset(),pud_offset(),pmd_offset(),pte_offset_map()来得到一个在用户态的pte页表里
  address所对应的pte页表项。
** struct vm_area_struct *find_extend_vma(struct mm_struct *mm, unsigned long addr)
*** mm/mmap.c:
- 在配置了CONFIG_STACK_GROWSUP时，是用find_vma_prev()来找被extend的vma_area_struct的，而设
  CONFIG_STACK_GROWSDOWN时是用find_vma()来找的。因为如果找到的vma_area_struct若不包含参数
  addr,那么调用expand_stack()时使用的参数就不一样了，CONFIG_STACK_GROWSUP是用前一个区间调用
  expand_stack()的,而CONFIG_STACK_GROWSDOWN无论是否包含addr,都会用find_vma()找到的
  vm_area_struct来调用expand_stack().就如expand_stack()的注释所写的，在
  CONFIG_STACK_GROWSUP时： vma is the first one with address > vma->vm_end.  Have to
  extend vma.否则vma is the first one with address < vma->vm_start.  Have to extend vma.
 #+BEGIN_EXAMPLE
	vma = find_vma_prev(mm, addr, &prev);
	if (vma && (vma->vm_start <= addr))
		return vma;
 #+END_EXAMPLE 
 #+BEGIN_EXAMPLE
	vma = find_vma(mm,addr);
	if (!vma)
		return NULL;
	if (vma->vm_start <= addr)
		return vma;
 #+END_EXAMPLE 
- 这个函数的功能就是先找出一个包含addr地址的区间，若没有区间包含这个地址，那么就把一个附近
  地区间的范围扩展到addr,因为addr附近有两个不同区间，至于选哪个就要看地址是向上还是向下増长
  的了。
** int expand_stack(struct vm_area_struct *vma, unsigned long address)
*** mm/mmap.c:
- 与上一个函数一样也有两个版本。
- 无论是向下还是向上增长，但是vma->vm_end还是大于vma->vm_start的
- 若是向下増长，参数address要这样对齐
 #+BEGIN_EXAMPLE
	address &= PAGE_MASK;
 #+END_EXAMPLE 
  否则是这样,但是为什么要加上一个4呢？
 #+BEGIN_EXAMPLE
	address += 4 + PAGE_SIZE - 1;
	address &= PAGE_MASK;
 #+END_EXAMPLE 
- acct_stack_growth()只是修改了统计变量，是调用之后才修改vm_area_struct->vm_start和
  vm_area_struct->vm_pgoff(向下増长)，vm_area_struct->vm_end(向上増长).
- 所以这个函数最主要是修改了vm_area_struct->vm_start或vm_area_struct->vm_end.
** static int acct_stack_growth(struct vm_area_struct * vma, unsigned long size, unsigned long grow)
*** mm/mmap.c:
- 这个函数也会做与其它一些函数类似的一系列的检查，如调用may_expand_vm()检查RLIMIT_AS是否超
  限，RLIMIT_STACK是否超限，RLIMIT_MEMLOCK是否超限。
- 作完检查之后就可以增加mm->total_vm了，虽然还没有扩展。
- 因为__vm_stat_account()只是对mm->shared_vm, mm->exec_vm, mm->stack_vm, mm->reserved_vm做
  处理。所以这个函数也只是修改了这些统计变量。而没有修改vm_area_struct->vm_start和
  vm_area_struct->vm_end.
** int do_munmap(struct mm_struct *mm, unsigned long start, size_t len)
*** mm/mmap.c:
- ULK:The function goes through two main phases. In the first phase (steps 16), it scans
  the list of memory regions owned by the process and unlinks all regions included in the
  linear address interval from the process address space. In the second phase (steps 712),
  the function updates the process Page Tables and removes the memory regions identified
  in the first phase.
** static void detach_vmas_to_be_unmapped(struct mm_struct *mm, struct vm_area_struct *vma, struct vm_area_struct *prev, unsigned long end)
*** mm/mmap.c:
- 这个函数是以参数vma这个内存区开始,一直到end这个虚拟地址所在的内存区为止,把这个范围内的
  vma都给删掉.但从代码可以看出,无论如何,都要把参数vma这个内存区给删掉.所谓的删掉就是把它从
  红黑树里删掉,并递减map_count,但是没有把内容给清掉,没有把vma的空间清掉.
- map_count Number of memory regions
- 注意要把mmap_cache给清掉
 #+BEGIN_EXAMPLE
	mm->mmap_cache = NULL;		/* Kill the cache. */
 #+END_EXAMPLE 
- 虽然没有已经把从vma开始到end的内存区从红黑树中删除,但是从vma到end的链接是没有删除的.
** unsigned long unmap_vmas(struct mmu_gather **tlbp, struct mm_struct *mm, struct vm_area_struct *vma, unsigned long start_addr, unsigned long end_addr, unsigned long *nr_accounted, struct zap_details *details)
*** mm/memory.c:
- ULK:to clear the Page Table entries covering the linear address interval and to free the
  corresponding page frames
- 这个函数不一定会把参数vma给处理了,若是start_addr大于vma->vm_start.
 #+BEGIN_EXAMPLE
	for ( ; vma && vma->vm_start < end_addr; vma = vma->vm_next) {
		unsigned long end;

		start = max(vma->vm_start, start_addr);
		if (start >= vma->vm_end)
			continue;
 #+END_EXAMPLE 
- 还要注意一种情况就是有可能start_addr和end_addr都在一个vma的里.
- VM_HUGETLB The pages in the region are handled through the extended paging mechanism 
- 
** void unmap_hugepage_range(struct vm_area_struct *vma, unsigned long start, unsigned long end)
*** arch/i386/mm/hugetlbpage.c:
- 这个函数的作用就是把从虚拟地址address到end之间的pte页表项给清掉,把被清掉的页表项所对应的
  页框给释放(put_page()在没有被引用时才会真正被释放掉)
- 这个函数看得比较容易明白,就是先用huge_pte_offset找到要处理的地址的pte页表项的指针,找到之
  后就通过ptep_get_and_clear()获取pte页表项指针的值并把pte页表项清掉,用获取到的pte页表项的
  值通过pte_page()找到对应的页框的struct page,再用找到的struct page通过put_page()把这个页框
  给释放掉.
- 在这个函数会减mm->rss
** static void unmap_page_range(struct mmu_gather *tlb, struct vm_area_struct *vma, unsigned long addr, unsigned long end, struct zap_details *details)
*** mm/memory.c:
- 把所有在addr到end这个范围内的所有pgd页表项都处理一遍，处理一遍就是调用一次zap_pud_range()
** static inline void zap_pud_range(struct mmu_gather *tlb, pgd_t *pgd, unsigned long addr, unsigned long end, struct zap_details *details)
*** mm/memory.c:
- 把所有在addr到end这个范围内的所有pud页表项都处理一遍，处理一遍就是调用一次zap_pmd_range()
** static inline void zap_pmd_range(struct mmu_gather *tlb, pud_t *pud, unsigned long addr, unsigned long end, struct zap_details *details)
*** mm/memory.c:
- 把所有在addr到end这个范围内的所有pte页表项都处理一遍，处理一遍就是调用一次zap_pte_range()
** static void zap_pte_range(struct mmu_gather *tlb, pmd_t *pmd, unsigned long addr, unsigned long end, struct zap_details *details)
*** mm/memory.c:
- 先用pte_offset_map()找到pte页表项的指针。
- 这个函数所调用的free_swap_and_cache()转而调用page_cache_release()转而调用put_page()最后释放页框。
- 如果pte_present()为真，那么释放页的地方在tlb_remove_page()
- 若页是保留页那么这个页是不会被释放的
 #+BEGIN_EXAMPLE
			if (pfn_valid(pfn)) {
				page = pfn_to_page(pfn);
				if (PageReserved(page))
					page = NULL;
			}
 #+END_EXAMPLE 
 但是还是要把pte页表给清掉的
 #+BEGIN_EXAMPLE
			ptent = ptep_get_and_clear(tlb->mm, addr, pte);
 #+END_EXAMPLE 
- 为什么pte为dirty时还要把page给dirty呢？都要回收了还有什么用呢？
 #+BEGIN_EXAMPLE
			if (pte_dirty(ptent))
				set_page_dirty(page);
 #+END_EXAMPLE 
- 若是匿名区就要减anon_rss,否则且pte为young,那么就把页给改成accessed,这是为什么呢？
 #+BEGIN_EXAMPLE
			if (PageAnon(page))
				dec_mm_counter(tlb->mm, anon_rss);
			else if (pte_young(ptent))
				mark_page_accessed(page);
 #+END_EXAMPLE 
- 如果页不present，且pte没有映射到文件那么就调用free_swap_and_cache()
** #define pgoff_to_pte(off)
*** include/asm-i386/pgtable-2level.h:
- 把一个pte页表项的内容设成这个地址所对应的文件的一个偏移量，一个偏移量表示4k大小。
- 这个宏有很多内容
- http://blog.csdn.net/dog250/article/details/5303232

** struct page * find_get_page(struct address_space *mapping, unsigned long offset)
*** mm/filemap.c:
- 这个函数的作用是在radix树里找一个页。
- 先调用radix_tree_lookup()在radix基树里找符合的页
- 再调用page_cache_get()增加页的使用计数。
** void *radix_tree_lookup(struct radix_tree_root *root, unsigned long index)
*** lib/radix-tree.c:
- 从以下的代码可以看出,radix树最多只有5层,因为RADIX_TREE_MAP_MASK是64,就是占6位，而index是
  32位的。
 #+BEGIN_EXAMPLE
			((*slot)->slots +
				((index >> shift) & RADIX_TREE_MAP_MASK));
		shift -= RADIX_TREE_MAP_SHIFT;
 #+END_EXAMPLE
** unsigned find_get_pages(struct address_space *mapping, pgoff_t start, unsigned int nr_pages, struct page **pages)
*** mm/filemap.c:
- 这个函数作用是从radix树里找一堆连续的页。start是指要找的页的第一个索引，所找的所有页的索
  引是连续的。
- 先调用radix_tree_gang_lookup()找页。
** unsigned int radix_tree_gang_lookup(struct radix_tree_root *root, void **results, unsigned long first_index, unsigned int max_items)
*** lib/radix-tree.c:
- 主要是调用__lookup()来找出连续的页,但是因为调用__lookup()一次只能找出在一个叶子结点上的连
  续的页, 所以若想要找的连续的页有跨过一个叶子节点,那么就要多次调用__lookup().
** struct page *find_lock_page(struct address_space *mapping, unsigned long offset)
*** mm/filemap.c:
- 这个函数与find_get_page()的不同就是获得页之后就把它锁住(PG_locked),锁住的作用就是只有这个
  进程可以访问该页,与PG_reserved不一样.就只是加多了以下的代码:
 #+BEGIN_EXAMPLE
		if (TestSetPageLocked(page)) {
			read_unlock_irq(&mapping->tree_lock);
			lock_page(page);
			read_lock_irq(&mapping->tree_lock);

			/* Has the page been truncated while we slept? */
			if (page->mapping != mapping || page->index != offset) {
				unlock_page(page);
				page_cache_release(page);
				goto repeat;
			}
		}
 #+END_EXAMPLE 
- 若发现页以被锁住就调用lock_page()阻塞等待把页锁住.
- 若发现页已不映射文件或不映到原来的文件或映射到原来文件但不是之前的偏移量就释放页.但不是简
  单地返回,而是重新执行radix_tree_lookup()来找页.
 #+BEGIN_EXAMPLE
			if (page->mapping != mapping || page->index != offset) {
 #+END_EXAMPLE 
** struct page *find_trylock_page(struct address_space *mapping, unsigned long offset)
*** mm/filemap.c:
- 与find_lock_page()的不同就是以下的代码:
 #+BEGIN_EXAMPLE
	if (page && TestSetPageLocked(page))
		page = NULL;
 #+END_EXAMPLE 
** struct page *find_or_create_page(struct address_space *mapping, unsigned long index, unsigned int gfp_mask)
*** mm/filemap.c:
- 这个函数的设计有点意思,为什么同时需要page和cached_page两个相同的变量呢?
** int radix_tree_preload(int gfp_mask)
*** lib/radix-tree.c:
- 这个函数的作用是把一些radix的节点先分配放到struct radix_tree_preload->nodes数组里作为缓
  冲,使得需要时不因为缺内存而分不了.
- radix_tree_preloads这个变量是PER-cpu变量
- 实现很简单,就是不断地调用kmem_cache_alloc()
** int add_to_page_cache(struct page *page, struct address_space *mapping, pgoff_t offset, int gfp_mask)
*** mm/filemap.c:
- 先调用radix_tree_preload()预先分配好radix结点
- 再调用radix_tree_insert()把页放到radix结点去.
- 也要把页锁在内存里
- 最后就是修改page->mapping,page->index,mapping->nrpages
** void remove_from_page_cache(struct page *page)
*** mm/filemap.c:
- 这个函数从radix树里删除一个页,但是不会释放这个页
- 调用__remove_from_page_cache()
** void __remove_from_page_cache(struct page *page)
*** mm/filemap.c:
- 这个函数调用radix_tree_delete()把有关page的路径的结点按需删掉
- 只是把page->mapping设为0,page->index不修改,也不把page给释放掉
** static inline struct page *__read_cache_page(struct address_space *mapping,unsigned long index,int (*filler)(void *,struct page*),void *data)
*** mm/filemap.c:
- 与find_or_create_page()基本相同,只是多了以下的代码
 #+BEGIN_EXAMPLE
		err = filler(data, page);
		if (err < 0) {
			page_cache_release(page);
			page = ERR_PTR(err);
		}
 #+END_EXAMPLE 
** void fastcall mark_page_accessed(struct page *page)
*** mm/swap.c:
- 有这样的注释
 #_BEGIN_EXAMPLE
/*
 * Mark a page as having seen activity.
 *
 * inactive,unreferenced	->	inactive,referenced
 * inactive,referenced		->	active,unreferenced
 * active,unreferenced		->	active,referenced
 */
 #+END_EXAMPLE 
 从这段注释来看,active标志和reference标志是一个二进制进位的关系,就是若是unreferenced,那么就
 先设为referenced,若是referenced,就要把referenced改为unreferenced并进一位来修改
 active.referenced标志总是取反.
** struct page *read_cache_page(struct address_space *mapping, unsigned long index, int (*filler)(void *,struct page*),void *data)
*** mm/swap.c:
- 先调用__read_cache_page(),再调用mark_page_accessed()
- 如果不是最新的说要调用filler()函数了,但是不是在__read_cache_page()有调用filler()了吗?但是
  要调用filler()就要先把页给锁住,但是锁页的过程中可能被其它的进程把页给更新了.
** static inline int grow_buffers(struct block_device *bdev, sector_t block, int size)
*** fs/buffer.c:
- In order to add a block device buffer page to the page cache, the kernel invokes the
  grow_buffers() function,
- 函数先算好块号,和在radix树的偏移量(index)
** static struct page *grow_dev_page(struct block_device *bdev, sector_t block, pgoff_t index, int size)
*** fs/buffer.c:
- 先调用find_or_create_page()来找到可以放这个buffer的页
- page_has_buffers()这个函数就是检查PG_private有没有设置,若有设置就说明指向buffer_head的链
  表
- 会调用init_page_buffers(),来处理所有的struct buffer_head,但是无论在什么时候,一个buffer
  page都会被填满吗?既使是有一些struct buffer_head的数据没有被访问?
- 
** static void init_page_buffers(struct page *page, struct block_device *bdev,sector_t block, int size)
*** fs/buffer.c:
- 结合grow_dev_page()和init_page_buffers()来看,进入init_page_buffers()之前在page里的buffer
  链表的struct buffer_head已经分配好了,但是不知道在哪分配的,在grow_dev_page()里调用的
  find_or_create_page()是没有分配struct buffer_head,所以在分配buffer page时没有分配struct
  buffer_head,这也就是为什么在grow_dev_page()里要调用page_has_buffers()一样.因为这个函数是
  用循环来处理所有的struct buffer_head的,且因为在grow_dev_page()里调用这个函数来看,一个页
  里的所有的struct buffer_head的b_size是一样的.原来分配struct buffer_head是在grow_dev_page
  里做的,调用了alloc_page_buffers()来分配.
- 
** struct buffer_head *alloc_page_buffers(struct page *page, unsigned long size,int retry)
*** fs/buffer.c:
- 为一个buffer page分配所有的struct buffer_head结构体
- 函数一共初始化了struct buffer_head的b_this_page,b_size,b_page,b_data
- 从这个函数可以看出,一个buffer page的struct buffer_head是一下子全部分配的.
- 可以看出这个struct buffer_head链表还不是一个循环链表.这个要在link_dev_buffers
 #+BEGIN_EXAMPLE
		bh->b_this_page = head;
		bh->b_blocknr = -1;
		head = bh;
 #+END_EXAMPLE
- 调用set_bh_page()来初始化b_page和b_data
- 因为分配给struct buffer_head是从高地址开始，所以若是PAGE_SIZE不能整除size,那么就会在低地
  址区有一块不被使用的空间。
** void set_bh_page(struct buffer_head *bh,struct page *page, unsigned long offset)
*** fs/buffer.c:
- 初始化b_page和b_data
- b_page就是page
- 若是高端内存的时候就把b_data修改成该struct buffer_head所管理的块在页内的偏移量
 #+BEGIN_EXAMPLE
		bh->b_data = (char *)(0 + offset);
 #+END_EXAMPLE
 因为是指针，所以要转换为指针类型。因为不能像低端内存那样找到页框所对应的线性地址，所以高
 端内存就不能像低端内存那样在b_data那里保存struct buffer_head所管理的块的线性地址.
 #+BEGIN_EXAMPLE
		bh->b_data = page_address(page) + offset;
 #+END_EXAMPLE 
** static inline void link_dev_buffers(struct page *page, struct buffer_head *head)
*** fs/buffer.c:
- 在grow_dev_page()里调用的alloc_page_buffers()已经把分配的struct buffer_head都给链起来了，
  但还没有把它们链到page里去了.
- 把struct buffer_head弄成循环的
- 再调用attach_page_buffers()把这个链表赋给页的private.
** int try_to_release_page(struct page *page, int gfp_mask)
*** fs/buffer.c:
- 这个函数的作用是单单释放buffer page的吗?还是释放cache page(包含有buffer_page)，看函数名还
  真的是看不出来的
- 调用address_space->a_ops->releasepage()时释放的应该不是buffer page,而调用
  try_to_free_buffers()这个函数才是释放buffer page的.
** int try_to_free_buffers(struct page *page)
*** fs/buffer.c:
- 这个函数就是释放buffer page的
- 若PG_writeback设置了就说明这个页正在被页回disk,所以不能释放.
